{
    "docs": [
        {
            "location": "/",
            "text": "An Introduction to Statistical Learning: with Applications in R... with Python!\n\n\nThis page contains the solutions to the exercises proposed in\n\n\n\n\n'An Introduction to Statistical Learning with Applications in R' (ISLR) by James, Witten, Hastie and Tibshirani\n [1].\n\n\n\n\nBoth conceptual and applied exercises were solved.\nAn effort was made to detail all the answers and to provide a set of bibliographical references that we found useful.\nThe exercises were solved using Python instead of R.\nYou are welcome to \ncollaborate\n.\n\n\nNote [03.October.2017]: we will release each chapter's solutions on a monthly basis (at least).\n\n\nSolutions\n\n\n\n\n\n\n\n\nChapter 2\n\n\nChapter 3\n\n\nChapter 4\n\n\nChapter 5\n\n\nChapter 6\n\n\nChapter 7\n\n\nChapter 8\n\n\nChapter 9\n\n\nChapter 10\n\n\n\n\n\n\n\n\n\n\n2.1\n\n\n3.1\n\n\n4.1\n\n\n5.1\n\n\n6.1\n\n\n7.1\n\n\n8.1\n\n\n9.1\n\n\n10.1\n\n\n\n\n\n\n2.2\n\n\n3.2\n\n\n4.2\n\n\n5.2\n\n\n6.2\n\n\n7.2\n\n\n8.2\n\n\n9.2\n\n\n10.2\n\n\n\n\n\n\n2.3\n\n\n3.3\n\n\n4.3\n\n\n5.3\n\n\n6.3\n\n\n7.3\n\n\n8.3\n\n\n9.3\n\n\n10.3\n\n\n\n\n\n\n2.4\n\n\n3.4\n\n\n4.4\n\n\n5.4\n\n\n6.4\n\n\n7.4\n\n\n8.4\n\n\n9.4\n\n\n10.4\n\n\n\n\n\n\n2.5\n\n\n3.5\n\n\n4.5\n\n\n5.5\n\n\n6.5\n\n\n7.5\n\n\n8.5\n\n\n9.5\n\n\n10.5\n\n\n\n\n\n\n2.6\n\n\n3.6\n\n\n4.6\n\n\n5.6\n\n\n6.6\n\n\n7.6\n\n\n8.6\n\n\n9.6\n\n\n10.6\n\n\n\n\n\n\n2.7\n\n\n3.7\n\n\n4.7\n\n\n5.7\n\n\n6.7\n\n\n7.7\n\n\n8.7\n\n\n9.7\n\n\n10.7\n\n\n\n\n\n\n2.8\n\n\n3.8\n\n\n4.8\n\n\n5.8\n\n\n6.8\n\n\n7.8\n\n\n8.8\n\n\n9.8\n\n\n10.8\n\n\n\n\n\n\n2.9\n\n\n3.9\n\n\n4.9\n\n\n5.9\n\n\n6.9\n\n\n7.9\n\n\n8.9\n\n\n\n\n10.9\n\n\n\n\n\n\n2.10\n\n\n3.10\n\n\n4.10\n\n\n\n\n6.10\n\n\n7.10\n\n\n8.10\n\n\n\n\n10.10\n\n\n\n\n\n\n\n\n3.11\n\n\n4.11\n\n\n\n\n6.11\n\n\n7.11\n\n\n8.11\n\n\n\n\n10.11\n\n\n\n\n\n\n\n\n3.12\n\n\n4.12\n\n\n\n\n\n\n7.12\n\n\n8.12\n\n\n\n\n\n\n\n\n\n\n\n\n3.13\n\n\n4.13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotivation\n\n\nThe main motivation of this project was learning.\nToday there are several good books and other resources from which to learn the material we covered, and we spent some time choosing a good learning project.\nWe chose \nISLR\n because it is an excellent, clear introduction to statistical learning, that  keeps a nice balance between theory, intuition, mathematical rigour and programming.\n\nOur main goal was to use the exercises as an excuse to improve our proficiency using Python's data science stack.\n\nWe had done other data science projects with Python, but, as we imagined, we still had a bit more to learn (and still do!).\nSince the book was written with R in mind, it made the use of Python a cool additional challenge.\nWe are strong advocates of the \nactive learning\n principles, and this project, once more, reinforced them in our minds.\nIf you're starting out in machine learning with Python (or R!), we recommend you try it!\n\n\nTechnical requirements, and How to Install\n\n\nThis project was developed using Python 3.5 on Jupyter notebooks (Jupyter Lab, in fact).\nWe tried to stay within the standard Python data science stack as much as possible.\nAccordingly, our main Python packages were numpy, matplotlib, pandas, seaborn, statsmodels and scikit-learn.\nYou should be able to run this with the standard Python setup, and the additional libraries we list below.\n\n\nIf you're just starting out with Python, here's a more complete 'how-to'. We recommend using \nAnaconda\n whether you are using Linux, Mac or Windows. Anaconda allows you to easily manage several Python environments.\nAn environment is a collection of installed Python packages.\nImagine that you have two projects with different requirements: a recent one with, say, Python 3.5 and matplotlib 4.0, and a legacy project with Python 2.7 and matplotlib 3.5.\nA good environment manager helps you install libraries and allows you to switch between both environments easily, avoiding dependencies migraines.\nYou can even work on both at the same time.\nYou don't want to know what the alternative is, to not using an environment manager.\nSo after installing Anaconda, the easiest way is to create a new environment and just install the libraries we list below one by one.\nAfter this is done, just make sure the desired environment is active (for example, on Linux and Mac, type 'source activate \n', and you're good to go). \n\n\nHere's the list of packages we installed:\n\n\n\n\njupyterlab (but this should run just as well on regular ipython notebooks)\n\n\nnumpy\n\n\npandas\n\n\nmatplotlib\n\n\nsklearn\n\n\nseaborn\n\n\nipywidgets (so that a seaborn import warning goes away)\n\n\nstatsmodels\n\n\nmlxtend \n\n\n\n\nIn addition, we chose mkdocs to present these solutions in a website format, for a better presentation. We might change to a different scheme in the future (any suggestions), but meanwhile we used these libraries:\n\n\n\n\nmkdocs\n\n\nmkdocs-cinder\n\n\npymdown-extensions #for latex \n\n\n\n\nHow to colaborate\n\n\nIf you want to collaborate, you can open an issue in our GitHub project and give us your suggestions on how to improve these solutions.\nOn GitHub, you can also fork this project and send a pull request to fix any mistakes that you have found.\nAlternatively, you can also go for the classical way of collaboration and send us an \ne-mail\n.\nAny effort to improve the quality of these solutions will be appreciated.\n\n\nMain references\n\n\nIn addition to thinking hard about them, to solve the exercises we followed several references.\nBesides ISLR [1], which is \navailable for free\n and explains almost everything you need to know to solve the exercises, \nwe also read some other books that provide a self-contained introduction to the field of statistical learning [2, 3, 4]. \nWe also spent some quality time on \nCrossValidate\n.\nFor the Python data science stack we think Wes McKinney's book [5] is a good choice, as well as Jake VanderPlas' [6].\nAdditional references for some of the exercises are scattered througout the solutions.\n\n\n\n\n[1] \nJames, G., Witten, D., Hastie, T., Tibshirani, R., 2015, An Introduction to Statistical Learning with Applications in R, Springer.\n \n[available for free]\n\n\n[2] \nHastie, T., Tibshirani, R. 2006, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.\n \n[available for free]\n\n\n[3] \nBishop, C.M., 2006, Pattern Recognition and Machine Learning, Springer.\n\n\n[4] \nMurphy, K.P., 2012, Machine Learning: A Probabilistic Perspective, MIT Press.\n\n\n[5] \nMcKinney, W., 2012, Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython\n\n\n[6] \nJake VanderPlas, 2016, Python Data Science Handbook: Essential Tools for Working with Data, O'Reilly Media.\n\n\n\n\nLearning resources\n\n\nFortunately, online resources are becoming more and more an essential tool for self-learning strategies.\nDuring the course of this project, we found several resources that can help you on your learning path. \nHere are the best ones we found. \n\n\n\n\nStanford's online course by the authors of ISLR\n\n\nAndrew Ng's Machine Learning course\n\n\n\n\nOther solutions to ISLR\n\n\nThere are other solutions to ISLR, though most of them do not use Python.\nBelow you can find a link to the solutions we found that were reasonably complete.\nWe did a occasional check with some of these, and they might be a good complementary resource.\nA special mention to \nJWarmenhoven's github repo\n, which uses Python to reproduce figures, tables and calculations of the main text of the chapters and labs.\nPlease let us know if you find any other significant solutions. \n\n\n\n\nJWarmenhoven: Python for the chapters text and labs\n\n\nJames Crouser's Exercises and Python Labs\n\n\nAsadoughi's\n\n\nJohn Weatherwax's solutions to the conceptual and practical exercises in R\n\n\nPierre Pacquay's\n\n\nyahwes'\n\n\n\n\nMIT License\n\n\nCopyright (c) [2017] [ISLP]\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "Home"
        },
        {
            "location": "/#an-introduction-to-statistical-learning-with-applications-in-r-with-python",
            "text": "This page contains the solutions to the exercises proposed in   'An Introduction to Statistical Learning with Applications in R' (ISLR) by James, Witten, Hastie and Tibshirani  [1].   Both conceptual and applied exercises were solved.\nAn effort was made to detail all the answers and to provide a set of bibliographical references that we found useful.\nThe exercises were solved using Python instead of R.\nYou are welcome to  collaborate .  Note [03.October.2017]: we will release each chapter's solutions on a monthly basis (at least).",
            "title": "An Introduction to Statistical Learning: with Applications in R... with Python!"
        },
        {
            "location": "/#solutions",
            "text": "Chapter 2  Chapter 3  Chapter 4  Chapter 5  Chapter 6  Chapter 7  Chapter 8  Chapter 9  Chapter 10      2.1  3.1  4.1  5.1  6.1  7.1  8.1  9.1  10.1    2.2  3.2  4.2  5.2  6.2  7.2  8.2  9.2  10.2    2.3  3.3  4.3  5.3  6.3  7.3  8.3  9.3  10.3    2.4  3.4  4.4  5.4  6.4  7.4  8.4  9.4  10.4    2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5  10.5    2.6  3.6  4.6  5.6  6.6  7.6  8.6  9.6  10.6    2.7  3.7  4.7  5.7  6.7  7.7  8.7  9.7  10.7    2.8  3.8  4.8  5.8  6.8  7.8  8.8  9.8  10.8    2.9  3.9  4.9  5.9  6.9  7.9  8.9   10.9    2.10  3.10  4.10   6.10  7.10  8.10   10.10     3.11  4.11   6.11  7.11  8.11   10.11     3.12  4.12    7.12  8.12       3.13  4.13           3.14            3.15",
            "title": "Solutions"
        },
        {
            "location": "/#motivation",
            "text": "The main motivation of this project was learning.\nToday there are several good books and other resources from which to learn the material we covered, and we spent some time choosing a good learning project.\nWe chose  ISLR  because it is an excellent, clear introduction to statistical learning, that  keeps a nice balance between theory, intuition, mathematical rigour and programming. Our main goal was to use the exercises as an excuse to improve our proficiency using Python's data science stack. \nWe had done other data science projects with Python, but, as we imagined, we still had a bit more to learn (and still do!).\nSince the book was written with R in mind, it made the use of Python a cool additional challenge.\nWe are strong advocates of the  active learning  principles, and this project, once more, reinforced them in our minds.\nIf you're starting out in machine learning with Python (or R!), we recommend you try it!",
            "title": "Motivation"
        },
        {
            "location": "/#technical-requirements-and-how-to-install",
            "text": "This project was developed using Python 3.5 on Jupyter notebooks (Jupyter Lab, in fact).\nWe tried to stay within the standard Python data science stack as much as possible.\nAccordingly, our main Python packages were numpy, matplotlib, pandas, seaborn, statsmodels and scikit-learn.\nYou should be able to run this with the standard Python setup, and the additional libraries we list below.  If you're just starting out with Python, here's a more complete 'how-to'. We recommend using  Anaconda  whether you are using Linux, Mac or Windows. Anaconda allows you to easily manage several Python environments.\nAn environment is a collection of installed Python packages.\nImagine that you have two projects with different requirements: a recent one with, say, Python 3.5 and matplotlib 4.0, and a legacy project with Python 2.7 and matplotlib 3.5.\nA good environment manager helps you install libraries and allows you to switch between both environments easily, avoiding dependencies migraines.\nYou can even work on both at the same time.\nYou don't want to know what the alternative is, to not using an environment manager.\nSo after installing Anaconda, the easiest way is to create a new environment and just install the libraries we list below one by one.\nAfter this is done, just make sure the desired environment is active (for example, on Linux and Mac, type 'source activate  ', and you're good to go).   Here's the list of packages we installed:   jupyterlab (but this should run just as well on regular ipython notebooks)  numpy  pandas  matplotlib  sklearn  seaborn  ipywidgets (so that a seaborn import warning goes away)  statsmodels  mlxtend    In addition, we chose mkdocs to present these solutions in a website format, for a better presentation. We might change to a different scheme in the future (any suggestions), but meanwhile we used these libraries:   mkdocs  mkdocs-cinder  pymdown-extensions #for latex",
            "title": "Technical requirements, and How to Install"
        },
        {
            "location": "/#how-to-colaborate",
            "text": "If you want to collaborate, you can open an issue in our GitHub project and give us your suggestions on how to improve these solutions.\nOn GitHub, you can also fork this project and send a pull request to fix any mistakes that you have found.\nAlternatively, you can also go for the classical way of collaboration and send us an  e-mail .\nAny effort to improve the quality of these solutions will be appreciated.",
            "title": "How to colaborate"
        },
        {
            "location": "/#main-references",
            "text": "In addition to thinking hard about them, to solve the exercises we followed several references.\nBesides ISLR [1], which is  available for free  and explains almost everything you need to know to solve the exercises, \nwe also read some other books that provide a self-contained introduction to the field of statistical learning [2, 3, 4]. \nWe also spent some quality time on  CrossValidate .\nFor the Python data science stack we think Wes McKinney's book [5] is a good choice, as well as Jake VanderPlas' [6].\nAdditional references for some of the exercises are scattered througout the solutions.   [1]  James, G., Witten, D., Hastie, T., Tibshirani, R., 2015, An Introduction to Statistical Learning with Applications in R, Springer.   [available for free]  [2]  Hastie, T., Tibshirani, R. 2006, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.   [available for free]  [3]  Bishop, C.M., 2006, Pattern Recognition and Machine Learning, Springer.  [4]  Murphy, K.P., 2012, Machine Learning: A Probabilistic Perspective, MIT Press.  [5]  McKinney, W., 2012, Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython  [6]  Jake VanderPlas, 2016, Python Data Science Handbook: Essential Tools for Working with Data, O'Reilly Media.",
            "title": "Main references"
        },
        {
            "location": "/#learning-resources",
            "text": "Fortunately, online resources are becoming more and more an essential tool for self-learning strategies.\nDuring the course of this project, we found several resources that can help you on your learning path. \nHere are the best ones we found.    Stanford's online course by the authors of ISLR  Andrew Ng's Machine Learning course",
            "title": "Learning resources"
        },
        {
            "location": "/#other-solutions-to-islr",
            "text": "There are other solutions to ISLR, though most of them do not use Python.\nBelow you can find a link to the solutions we found that were reasonably complete.\nWe did a occasional check with some of these, and they might be a good complementary resource.\nA special mention to  JWarmenhoven's github repo , which uses Python to reproduce figures, tables and calculations of the main text of the chapters and labs.\nPlease let us know if you find any other significant solutions.    JWarmenhoven: Python for the chapters text and labs  James Crouser's Exercises and Python Labs  Asadoughi's  John Weatherwax's solutions to the conceptual and practical exercises in R  Pierre Pacquay's  yahwes'",
            "title": "Other solutions to ISLR"
        },
        {
            "location": "/#mit-license",
            "text": "Copyright (c) [2017] [ISLP]  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "MIT License"
        },
        {
            "location": "/sols/chapter2/exercise1/",
            "text": "Exercise 2.1\n\n\nTo answer to this exercise, we need to understand the \nsources of error\n in a statistical learning method. For regression, assuming \n\\(Y = f(X) + \\varepsilon\\)\n, where \n\\(E[\\varepsilon]=0\\)\n and \n\\(Var[\\varepsilon]=\\sigma_\\varepsilon^2\\)\n, we can always obtain a decomposition of the test mean squared error, \n\\(E[(Y - \\hat{f}(x_0))^2\\)\n, into the sum of the irreducible error, the squared bias and the variance [1, page 223]:\n\n\n\\begin{align}\n\\mathrm{E}\\Big[\\big(Y - \\hat{f}(x)\\big)^2 \\Big|\\, X=x_0 \\Big]\n & =  \\sigma_\\varepsilon^2 + \\mathrm{Bias}^2\\big[\\hat{f}(x_0)\\big] + \\mathrm{Var}\\left[ \\hat{f}(x_0) \\right] , \\\n\\end{align}\nwhere \n\\(\\sigma_\\varepsilon^2\\)\n is the noise or irreducible error, \n\n\n\\[\\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}\\]\nand\n\n\n\\[\\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}\\]\nSince the irreducible error corresponds to the lowest achievable error, a good test set performance of a statistical learning method requires low variance as well as low squared bias.\n\n\nWhen we approximate a problem (possibly very complex), by a simpler model we introduce an error known as \nbias\n.\nThe simplest non-trivial example might be approximating a non-linear relationship (for example, a quadratic one) by a linear function of parameters and predictors. In this case, will have a always non-zero test error, regardless of how well we fit the model parameters, how large the training set is, or even how small the noise is (even zero). The more the true model deviates from a linear one, the larger this error will be.\n\n\nOn the other hand, \nvariance\n refers to the amount by which the estimation function would change if it was estimated using a different training set.\nThe training set is used to estimate the model parameters, which means that we obtain different estimates from different training sets. We hope however that this difference is small, and we say that between estimates from different training sets is small, in which case we say that the learning method has low variance.\nOn the other hand, a method for which small changes in the training set might lead to large changes in the estimated model parameters is referred as a method with high variance.\n\n\nIn general, more flexible methods have less bias and have higher variance. This is referred to as the  \nbias-variance trade-off\n since a low test mean squared error requires both low bias and low variance.\n\n\n(a) Extremely large sample, few predictors\n\n\nA flexible method is expected to be \nbetter\n. \n\n\nSince the sample size is extremely large and the number of predictors is small, a more flexible method would be able to better fit the data, while not fitting the noise due to the very large sample size. In other words a more flexible model would have the upside of a less bias, without much risk of \noverfitting\n.\n\n\n(b) Awful lot of predictors, small sample\n\n\nA flexible method is expected to be  \nworse\n.\n\n\nIt is very likely that when the number of predictors is extremely large and the number of observations is small a flexible model would fit the noise, meaning that, given another random data set of the same distribuition, the fit would likely be significantly different. Therefore one would be better off using a less flexible method, which will have more bias, but will be less likely to overfit.\n\n\n(c) Highly non-linear relationship\n\n\nA flexible method is expected to be  \nbetter\n.\n\n\nA more flexible method will likely be necessary to model a highly non-linear relationship, otherwise the model will be too biased and not capture the non-linearities of the model. No matter how large the sample size, a less flexible model would always be limited.\n\n\n(d) Extremely high variance\n\n\nA flexible method is expected to be  \nworse\n.\n\n\nSince the variance is extremely high, a more flexible model will fit the noise more and thus very likely overfit. A less flexible model will be more likely to still capture the essential 'features' of the model without picking up extraneous ones induced by the noise.\n\n\nFurther reading\n\n\nThe bias-variance decomposition is a fundamental aspect of machine learning which is present not only in regression. This decomposition has been generalized to more general loss functions and to classification learning methods. See, for example, [2].\n\n\n\n\n[1] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. \nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n. Springer, 2009.\n\n\n[2] James, Gareth M. \n\"Variance and bias for general loss functions.\"\n  Machine learning 51.2 (2003): 115-135.",
            "title": "2.1"
        },
        {
            "location": "/sols/chapter2/exercise1/#exercise-21",
            "text": "To answer to this exercise, we need to understand the  sources of error  in a statistical learning method. For regression, assuming  \\(Y = f(X) + \\varepsilon\\) , where  \\(E[\\varepsilon]=0\\)  and  \\(Var[\\varepsilon]=\\sigma_\\varepsilon^2\\) , we can always obtain a decomposition of the test mean squared error,  \\(E[(Y - \\hat{f}(x_0))^2\\) , into the sum of the irreducible error, the squared bias and the variance [1, page 223]:  \\begin{align}\n\\mathrm{E}\\Big[\\big(Y - \\hat{f}(x)\\big)^2 \\Big|\\, X=x_0 \\Big]\n & =  \\sigma_\\varepsilon^2 + \\mathrm{Bias}^2\\big[\\hat{f}(x_0)\\big] + \\mathrm{Var}\\left[ \\hat{f}(x_0) \\right] , \\\n\\end{align}\nwhere  \\(\\sigma_\\varepsilon^2\\)  is the noise or irreducible error,   \\[\\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}\\] and  \\[\\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}\\] Since the irreducible error corresponds to the lowest achievable error, a good test set performance of a statistical learning method requires low variance as well as low squared bias.  When we approximate a problem (possibly very complex), by a simpler model we introduce an error known as  bias .\nThe simplest non-trivial example might be approximating a non-linear relationship (for example, a quadratic one) by a linear function of parameters and predictors. In this case, will have a always non-zero test error, regardless of how well we fit the model parameters, how large the training set is, or even how small the noise is (even zero). The more the true model deviates from a linear one, the larger this error will be.  On the other hand,  variance  refers to the amount by which the estimation function would change if it was estimated using a different training set.\nThe training set is used to estimate the model parameters, which means that we obtain different estimates from different training sets. We hope however that this difference is small, and we say that between estimates from different training sets is small, in which case we say that the learning method has low variance.\nOn the other hand, a method for which small changes in the training set might lead to large changes in the estimated model parameters is referred as a method with high variance.  In general, more flexible methods have less bias and have higher variance. This is referred to as the   bias-variance trade-off  since a low test mean squared error requires both low bias and low variance.",
            "title": "Exercise 2.1"
        },
        {
            "location": "/sols/chapter2/exercise1/#a-extremely-large-sample-few-predictors",
            "text": "A flexible method is expected to be  better .   Since the sample size is extremely large and the number of predictors is small, a more flexible method would be able to better fit the data, while not fitting the noise due to the very large sample size. In other words a more flexible model would have the upside of a less bias, without much risk of  overfitting .",
            "title": "(a) Extremely large sample, few predictors"
        },
        {
            "location": "/sols/chapter2/exercise1/#b-awful-lot-of-predictors-small-sample",
            "text": "A flexible method is expected to be   worse .  It is very likely that when the number of predictors is extremely large and the number of observations is small a flexible model would fit the noise, meaning that, given another random data set of the same distribuition, the fit would likely be significantly different. Therefore one would be better off using a less flexible method, which will have more bias, but will be less likely to overfit.",
            "title": "(b) Awful lot of predictors, small sample"
        },
        {
            "location": "/sols/chapter2/exercise1/#c-highly-non-linear-relationship",
            "text": "A flexible method is expected to be   better .  A more flexible method will likely be necessary to model a highly non-linear relationship, otherwise the model will be too biased and not capture the non-linearities of the model. No matter how large the sample size, a less flexible model would always be limited.",
            "title": "(c) Highly non-linear relationship"
        },
        {
            "location": "/sols/chapter2/exercise1/#d-extremely-high-variance",
            "text": "A flexible method is expected to be   worse .  Since the variance is extremely high, a more flexible model will fit the noise more and thus very likely overfit. A less flexible model will be more likely to still capture the essential 'features' of the model without picking up extraneous ones induced by the noise.",
            "title": "(d) Extremely high variance"
        },
        {
            "location": "/sols/chapter2/exercise1/#further-reading",
            "text": "The bias-variance decomposition is a fundamental aspect of machine learning which is present not only in regression. This decomposition has been generalized to more general loss functions and to classification learning methods. See, for example, [2].   [1] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman.  The Elements of Statistical Learning: Data Mining, Inference, and Prediction . Springer, 2009.  [2] James, Gareth M.  \"Variance and bias for general loss functions.\"   Machine learning 51.2 (2003): 115-135.",
            "title": "Further reading"
        },
        {
            "location": "/sols/chapter2/exercise2/",
            "text": "Exercise 2.2\n\n\n(a) CEO salary\n\n\nRegression\n, since the variable we are interested in, CEO salary, is quantitative.\nSince we want to understand the factors affecting CEO salary we are most interested in \ninference\n.\nThe number of predictors is 3, and the sample size is 500 \n(p=3, n=500)\n.\n\n\n(b) New product launch: success or failure?\n\n\nClassification\n, because we are studying the outcome of the launch of a new product, which we are representing in one of two categories: success or failure.\nIn this case we wish to know whether a new launch will be a success or a failure more than understand why, so we are more interested in \nprediction\n.\nThe number of predictors is 13, and the sample size is 20 \n(p=13, n=20)\n.\n\n\n(c) Change in the US dollar\n\n\nRegression\n, since the variable we want to predict is a real number, and we are more interested in \nprediction\n.\nThe number of predictors is 3, and the sample size is 52 \n(p=3, n=52)\n.",
            "title": "2.2"
        },
        {
            "location": "/sols/chapter2/exercise2/#exercise-22",
            "text": "",
            "title": "Exercise 2.2"
        },
        {
            "location": "/sols/chapter2/exercise2/#a-ceo-salary",
            "text": "Regression , since the variable we are interested in, CEO salary, is quantitative.\nSince we want to understand the factors affecting CEO salary we are most interested in  inference .\nThe number of predictors is 3, and the sample size is 500  (p=3, n=500) .",
            "title": "(a) CEO salary"
        },
        {
            "location": "/sols/chapter2/exercise2/#b-new-product-launch-success-or-failure",
            "text": "Classification , because we are studying the outcome of the launch of a new product, which we are representing in one of two categories: success or failure.\nIn this case we wish to know whether a new launch will be a success or a failure more than understand why, so we are more interested in  prediction .\nThe number of predictors is 13, and the sample size is 20  (p=13, n=20) .",
            "title": "(b) New product launch: success or failure?"
        },
        {
            "location": "/sols/chapter2/exercise2/#c-change-in-the-us-dollar",
            "text": "Regression , since the variable we want to predict is a real number, and we are more interested in  prediction .\nThe number of predictors is 3, and the sample size is 52  (p=3, n=52) .",
            "title": "(c) Change in the US dollar"
        },
        {
            "location": "/sols/chapter2/exercise4/",
            "text": "Exercise 2.4\n\n\n(a) Classification\n\n\nSpam filters\n. \nAny self-respecting email service has a good spam filter: ideally it keeps any undesired email away from your inbox, and let's through every email you're interested in.\nThe response has just two categories: 'spam' or 'not spam', and the predictors can include several different variables: whether you have corresponded with the sender, whether you have their email address in some of your non-spam emails, the words and sequences of words of the emails (for example, if the email includes any reference to a 'Nigerian prince'), whether similar emails have already been tagged as spam by other users, etc.\nThe main goal is \nprediction\n because the most important task is to accurately be able to predict whether a future email message is spam or not-spam. \nAn important aspect in this case is the rate of false positives and false negatives.\nIn the case of email it is usually much more acceptable to have false negatives (spam that got through to your inbox) than false positives (important email that was classified as spam and you have never noticed). \n\n\nDigit recognition. (or text or speech or facial recognition)\n \nAn important task required everyday around the world is the recognition of handwritten digits and addresses for postal service or document scanning and OCR (Optical Character Recognition).\nDepending on how we define the learning task, the predictors can be the original digital photographs or scans of the documents, or just a cropped, grayscale image of a single digit (that is, a N by M matrix of real numbers, each describing the gray scale value of a single pixel).\nThe response might be one of ten digits (0 to 9) or it might include commas and the full numbers (not just the digits), depending on how we decide to define the task.\nAgain here the main goal is \nprediction\n, since the most important thing is to recognize a correct address or bank account number from a letter or document. \n\n\nSocial analysis\n. Classify people into supporters of \nSporting\n, supporters of \nBenfica\n, neither or both (so the response is one of these four categories).\nAs predictors, include factors such as age, nationality, address, income, education, gender, whether they appreciate classical music, their criminal record, etc.\nHere the main goal is \ninference\n, since more than predicting whether someone supports a specific club we are interested in understanding and studying the different factors and discovering interesting relationships between them.\n\n\nOther examples\n: fraud detection, medical diagnosis, stock prediction (buy, sell, hold), astronomical objects classification, choosing \nstrawberries\n or \ncucumbers\n.\n\n\n(b) Regression\n\n\nGalton's original 'regression' to the mean\n. \nStudy the height of children and its relationship to the height of their parents.\nHere the response is the height of the child and the predictor is the height of both parents (or the mean of both).\nThe main goal would be \ninference\n, since we are trying to better understand, from a scientific perspective, how much genetics influences an individual's height.\nUnless, of course, you are an overly zealous future parent trying to predict the chances of your child playing in the NBA given your own height and your partner's - in which case your main goal might be prediction.\nThis problem is \nwhere the name 'regression' comes from\n and, perhaps surprisingly, \nGalton's statistical method\n has stood the test of time quite well.\n\n\nHouse prices\n. Buying a house is probably the biggest investment that many people do during their life time.\nAccordingly, it is natural that people want to know the value of a house in order to do the best deal possible.\nThe main goal in this cases is \nprediction\n, since we want to predict house's price considering a set of predictors. \nThere are several factors used to predict house's price. \nFor example, a \nKaggle competition\n in which we participated, suggests a total of 79 predictors to predict the final price of each house in Boston.\n\n\nWeather\n.\nThe response can be temperature, wind velocity or precipitation amount in different locations, and the predictors can be the same variables for previous times.\nThe goal is both \nprediction\n (will it rain too much this month and ruin the plantations; will there be enough snow for the snowtrip?) and \ninference\n (what causes hurricanes, or the current climate change?). \n\n\nOther examples\n:\nreturns of investment in stocks, predicting school grades from amount and type of study, predicting the popularity of a book, film, article or tweet.\n\n\n(c) Cluster analysis\n\n\nOutlier detection\n.\nIf you can define a distance between the data points or a density function in the predictors space, you can use cluster analysis to identify candidate outliers such as the data points farther from the majority of the points, or those in sparser regions.\n\n\nMarket research\n. \nUsing cluster analysis, market researchers can group consumers into market segments according to their similarities.\nThis can provide a better understanding on group characteristics, as well as it can provide useful insights about how different groups of consumers/potential customer behave and relate with different groups.\n\n\nRecommender systems\n.\nWill you think watching 'Pulp Fiction' is time well spent?\nIf enough is known about you, including some of your film preferences (and perhaps how many times you have watched 'Stalker' or 'Pulp Fiction') and some similar data about other people, we can have an indication of whether will you think rewatching 'Pulp Fiction' is worth your time...\nAre you 'closer' to the people who think the answer is 'No' or to the ones that think 'Yes'?\n\n\nOther examples\n:\ngene sequencing, data compression, social network analysis, representation learning, topography classification.",
            "title": "2.4"
        },
        {
            "location": "/sols/chapter2/exercise4/#exercise-24",
            "text": "",
            "title": "Exercise 2.4"
        },
        {
            "location": "/sols/chapter2/exercise4/#a-classification",
            "text": "Spam filters . \nAny self-respecting email service has a good spam filter: ideally it keeps any undesired email away from your inbox, and let's through every email you're interested in.\nThe response has just two categories: 'spam' or 'not spam', and the predictors can include several different variables: whether you have corresponded with the sender, whether you have their email address in some of your non-spam emails, the words and sequences of words of the emails (for example, if the email includes any reference to a 'Nigerian prince'), whether similar emails have already been tagged as spam by other users, etc.\nThe main goal is  prediction  because the most important task is to accurately be able to predict whether a future email message is spam or not-spam. \nAn important aspect in this case is the rate of false positives and false negatives.\nIn the case of email it is usually much more acceptable to have false negatives (spam that got through to your inbox) than false positives (important email that was classified as spam and you have never noticed).   Digit recognition. (or text or speech or facial recognition)  \nAn important task required everyday around the world is the recognition of handwritten digits and addresses for postal service or document scanning and OCR (Optical Character Recognition).\nDepending on how we define the learning task, the predictors can be the original digital photographs or scans of the documents, or just a cropped, grayscale image of a single digit (that is, a N by M matrix of real numbers, each describing the gray scale value of a single pixel).\nThe response might be one of ten digits (0 to 9) or it might include commas and the full numbers (not just the digits), depending on how we decide to define the task.\nAgain here the main goal is  prediction , since the most important thing is to recognize a correct address or bank account number from a letter or document.   Social analysis . Classify people into supporters of  Sporting , supporters of  Benfica , neither or both (so the response is one of these four categories).\nAs predictors, include factors such as age, nationality, address, income, education, gender, whether they appreciate classical music, their criminal record, etc.\nHere the main goal is  inference , since more than predicting whether someone supports a specific club we are interested in understanding and studying the different factors and discovering interesting relationships between them.  Other examples : fraud detection, medical diagnosis, stock prediction (buy, sell, hold), astronomical objects classification, choosing  strawberries  or  cucumbers .",
            "title": "(a) Classification"
        },
        {
            "location": "/sols/chapter2/exercise4/#b-regression",
            "text": "Galton's original 'regression' to the mean . \nStudy the height of children and its relationship to the height of their parents.\nHere the response is the height of the child and the predictor is the height of both parents (or the mean of both).\nThe main goal would be  inference , since we are trying to better understand, from a scientific perspective, how much genetics influences an individual's height.\nUnless, of course, you are an overly zealous future parent trying to predict the chances of your child playing in the NBA given your own height and your partner's - in which case your main goal might be prediction.\nThis problem is  where the name 'regression' comes from  and, perhaps surprisingly,  Galton's statistical method  has stood the test of time quite well.  House prices . Buying a house is probably the biggest investment that many people do during their life time.\nAccordingly, it is natural that people want to know the value of a house in order to do the best deal possible.\nThe main goal in this cases is  prediction , since we want to predict house's price considering a set of predictors. \nThere are several factors used to predict house's price. \nFor example, a  Kaggle competition  in which we participated, suggests a total of 79 predictors to predict the final price of each house in Boston.  Weather .\nThe response can be temperature, wind velocity or precipitation amount in different locations, and the predictors can be the same variables for previous times.\nThe goal is both  prediction  (will it rain too much this month and ruin the plantations; will there be enough snow for the snowtrip?) and  inference  (what causes hurricanes, or the current climate change?).   Other examples :\nreturns of investment in stocks, predicting school grades from amount and type of study, predicting the popularity of a book, film, article or tweet.",
            "title": "(b) Regression"
        },
        {
            "location": "/sols/chapter2/exercise4/#c-cluster-analysis",
            "text": "Outlier detection .\nIf you can define a distance between the data points or a density function in the predictors space, you can use cluster analysis to identify candidate outliers such as the data points farther from the majority of the points, or those in sparser regions.  Market research . \nUsing cluster analysis, market researchers can group consumers into market segments according to their similarities.\nThis can provide a better understanding on group characteristics, as well as it can provide useful insights about how different groups of consumers/potential customer behave and relate with different groups.  Recommender systems .\nWill you think watching 'Pulp Fiction' is time well spent?\nIf enough is known about you, including some of your film preferences (and perhaps how many times you have watched 'Stalker' or 'Pulp Fiction') and some similar data about other people, we can have an indication of whether will you think rewatching 'Pulp Fiction' is worth your time...\nAre you 'closer' to the people who think the answer is 'No' or to the ones that think 'Yes'?  Other examples :\ngene sequencing, data compression, social network analysis, representation learning, topography classification.",
            "title": "(c) Cluster analysis"
        },
        {
            "location": "/sols/chapter2/exercise5/",
            "text": "Exercise 2.5\n\n\nA very flexible approach (versus a less flexible)\n\n\nAdvantages\n\n\n\n\nLess bias.\n\n\nGiven enough data, better results.\n\n\n\n\nDisadvantages\n\n\n\n\nMore risk of overfitting.  \n\n\nHarder to train.\n\n\nLonger to train.\n\n\nComputational more demanding.\n\n\nLess clear interpretability.\n\n\n\n\nWhen is one approach preferable?\n\n\nMore flexible\n\n\n\n\nLarge sample size and small number of predictors.\n\n\nNon-linear relationship between the predictors and response.\n\n\n\n\nLess flexible\n\n\n\n\nSmall sample size and large number of predictors.\n\n\nHigh variance of the error terms.",
            "title": "2.5"
        },
        {
            "location": "/sols/chapter2/exercise5/#exercise-25",
            "text": "",
            "title": "Exercise 2.5"
        },
        {
            "location": "/sols/chapter2/exercise5/#a-very-flexible-approach-versus-a-less-flexible",
            "text": "",
            "title": "A very flexible approach (versus a less flexible)"
        },
        {
            "location": "/sols/chapter2/exercise5/#advantages",
            "text": "Less bias.  Given enough data, better results.",
            "title": "Advantages"
        },
        {
            "location": "/sols/chapter2/exercise5/#disadvantages",
            "text": "More risk of overfitting.    Harder to train.  Longer to train.  Computational more demanding.  Less clear interpretability.",
            "title": "Disadvantages"
        },
        {
            "location": "/sols/chapter2/exercise5/#when-is-one-approach-preferable",
            "text": "",
            "title": "When is one approach preferable?"
        },
        {
            "location": "/sols/chapter2/exercise5/#more-flexible",
            "text": "Large sample size and small number of predictors.  Non-linear relationship between the predictors and response.",
            "title": "More flexible"
        },
        {
            "location": "/sols/chapter2/exercise5/#less-flexible",
            "text": "Small sample size and large number of predictors.  High variance of the error terms.",
            "title": "Less flexible"
        },
        {
            "location": "/sols/chapter2/exercise6/",
            "text": "Exercise 2.6\n\n\nThat this exercise does not ask for a precise definition of parametric and non-parametric methods and that the main text does also not provide one (section 2.1.2) is a sign that this definition is not consensual, and in fact it depends on the context in which it is used in statistics [1].\nFor our purposes we follow both the main text and Murphy [2] (and a few others [3-6]), for a definition of a parametric and non-parametric model (and by model, this we mean a probability distribution or density, or a regression function).\nSo, for our purposes, a parametric model means two thin\ngs: (1) strong, explicit assumptions about the form of the model, (2) which is parametrized by a finite, fixed number of parameters that does not change with the amount of data.\n(Yes, the first part of this definition is vague.)\nConversely, a non-parametric model means that, in general, the number of parameters depends on the amount of data and that we try to keep assumptions as weak as possible.\n'Non-parametric' is thus an unfortunate name since it does not mean 'no parameters'.\nOn the contrary, they do contain parameters (often many more) but these tend to determine the model complexity rather than the form.\nThe typical examples of a parametric and non-parametric models are linear regression and KNN, respectively.\nAnother good example, is the one from the text and figures 2.4, 2.5 and 2.6.\n\n\nThe advantages of a parametric approach are, in general, that it requires less observations, is faster and more computationally tractable, and more robust (less misguided by noise).\nThe disadvantages are the stronger assumptions that make the model less flexible, perhaps unable to capture the underlying adequately regardless of the amount of training data.\n\n\nThere are also other types of models such as semi-parametric models [6], which have two components: a parametric and a non-parametric one.\nOne can also have a non-parametric model by using a parametric model but aditionally adapting the number of parameters as needed (say the degree of the polynomial in a linear regression).\n\n\nReferences\n\n\n\n\n[1] https://en.wikipedia.org/wiki/Nonparametric_statistics#Definitions\n\n\n[1] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.\n\n\n[2] Domingos, Pedro. \"A few useful things to know about machine learning.\" Communications of the ACM 55.10 (2012): 78-87.\n\n\n[3] Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013.\n\n\n[4] Bishop, C.M.: Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA (2006)\n\n\n[5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n\n\n[6] Wellner, Jon A., Chris AJ Klaassen, and Ya'acov Ritov. \"Semiparametric models: a review of progress since BKRW (1993).\" (2006): 25-44.",
            "title": "2.6"
        },
        {
            "location": "/sols/chapter2/exercise6/#exercise-26",
            "text": "That this exercise does not ask for a precise definition of parametric and non-parametric methods and that the main text does also not provide one (section 2.1.2) is a sign that this definition is not consensual, and in fact it depends on the context in which it is used in statistics [1].\nFor our purposes we follow both the main text and Murphy [2] (and a few others [3-6]), for a definition of a parametric and non-parametric model (and by model, this we mean a probability distribution or density, or a regression function).\nSo, for our purposes, a parametric model means two thin\ngs: (1) strong, explicit assumptions about the form of the model, (2) which is parametrized by a finite, fixed number of parameters that does not change with the amount of data.\n(Yes, the first part of this definition is vague.)\nConversely, a non-parametric model means that, in general, the number of parameters depends on the amount of data and that we try to keep assumptions as weak as possible.\n'Non-parametric' is thus an unfortunate name since it does not mean 'no parameters'.\nOn the contrary, they do contain parameters (often many more) but these tend to determine the model complexity rather than the form.\nThe typical examples of a parametric and non-parametric models are linear regression and KNN, respectively.\nAnother good example, is the one from the text and figures 2.4, 2.5 and 2.6.  The advantages of a parametric approach are, in general, that it requires less observations, is faster and more computationally tractable, and more robust (less misguided by noise).\nThe disadvantages are the stronger assumptions that make the model less flexible, perhaps unable to capture the underlying adequately regardless of the amount of training data.  There are also other types of models such as semi-parametric models [6], which have two components: a parametric and a non-parametric one.\nOne can also have a non-parametric model by using a parametric model but aditionally adapting the number of parameters as needed (say the degree of the polynomial in a linear regression).",
            "title": "Exercise 2.6"
        },
        {
            "location": "/sols/chapter2/exercise6/#references",
            "text": "[1] https://en.wikipedia.org/wiki/Nonparametric_statistics#Definitions  [1] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.  [2] Domingos, Pedro. \"A few useful things to know about machine learning.\" Communications of the ACM 55.10 (2012): 78-87.  [3] Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013.  [4] Bishop, C.M.: Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA (2006)  [5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.  [6] Wellner, Jon A., Chris AJ Klaassen, and Ya'acov Ritov. \"Semiparametric models: a review of progress since BKRW (1993).\" (2006): 25-44.",
            "title": "References"
        },
        {
            "location": "/about/",
            "text": "This is a project by \nbotlnec\n, a group of friends working in different areas of data science. \n\n\nThis project was developed by two members of the Botlnec team and is included in a set of activities that this group of friends has been developping.\n\n\nPedro Marcelino\n\n\nPedro works as a researcher at \nLNEC\n and he is also a PhD candidate at \nIST\n.\nCurrently, he is developing a new approach to the maintenance management of transport infrastructures using machine learning.\nIn his spare time, he likes to work on TreeTree2, read books and play sports. \n\n\nJo\u00e3o Rico\n\n\nJo\u00e3o is a researcher at LNEC interested in machine and human learning.",
            "title": "About"
        },
        {
            "location": "/about/#pedro-marcelino",
            "text": "Pedro works as a researcher at  LNEC  and he is also a PhD candidate at  IST .\nCurrently, he is developing a new approach to the maintenance management of transport infrastructures using machine learning.\nIn his spare time, he likes to work on TreeTree2, read books and play sports.",
            "title": "Pedro Marcelino"
        },
        {
            "location": "/about/#joao-rico",
            "text": "Jo\u00e3o is a researcher at LNEC interested in machine and human learning.",
            "title": "Jo\u00e3o Rico"
        }
    ]
}