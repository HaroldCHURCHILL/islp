{
    "docs": [
        {
            "location": "/",
            "text": "An Introduction to Statistical Learning: with Applications in R... with Python!\n\n\nThis page contains the solutions to the exercises proposed in\n\n\n\n\n'An Introduction to Statistical Learning with Applications in R' (ISLR) by James, Witten, Hastie and Tibshirani\n [1].\n\n\n\n\nBoth conceptual and applied exercises were solved.\nAn effort was made to detail all the answers and to provide a set of bibliographical references that we found useful.\nThe exercises were solved using Python instead of R.\nYou are welcome to \ncollaborate\n.\n\n\nNote [03.October.2017]: we will release each chapter's solutions on a monthly basis (at least).\n\n\nSolutions\n\n\n\n\n\n\n\n\nChapter 2\n\n\nChapter 3\n\n\nChapter 4\n\n\nChapter 5\n\n\nChapter 6\n\n\nChapter 7\n\n\nChapter 8\n\n\nChapter 9\n\n\nChapter 10\n\n\n\n\n\n\n\n\n\n\n2.1\n\n\n3.1\n\n\n4.1\n\n\n5.1\n\n\n6.1\n\n\n7.1\n\n\n8.1\n\n\n9.1\n\n\n10.1\n\n\n\n\n\n\n2.2\n\n\n3.2\n\n\n4.2\n\n\n5.2\n\n\n6.2\n\n\n7.2\n\n\n8.2\n\n\n9.2\n\n\n10.2\n\n\n\n\n\n\n2.3\n\n\n3.3\n\n\n4.3\n\n\n5.3\n\n\n6.3\n\n\n7.3\n\n\n8.3\n\n\n9.3\n\n\n10.3\n\n\n\n\n\n\n2.4\n\n\n3.4\n\n\n4.4\n\n\n5.4\n\n\n6.4\n\n\n7.4\n\n\n8.4\n\n\n9.4\n\n\n10.4\n\n\n\n\n\n\n2.5\n\n\n3.5\n\n\n4.5\n\n\n5.5\n\n\n6.5\n\n\n7.5\n\n\n8.5\n\n\n9.5\n\n\n10.5\n\n\n\n\n\n\n2.6\n\n\n3.6\n\n\n4.6\n\n\n5.6\n\n\n6.6\n\n\n7.6\n\n\n8.6\n\n\n9.6\n\n\n10.6\n\n\n\n\n\n\n2.7\n\n\n3.7\n\n\n4.7\n\n\n5.7\n\n\n6.7\n\n\n7.7\n\n\n8.7\n\n\n9.7\n\n\n10.7\n\n\n\n\n\n\n2.8\n\n\n3.8\n\n\n4.8\n\n\n5.8\n\n\n6.8\n\n\n7.8\n\n\n8.8\n\n\n9.8\n\n\n10.8\n\n\n\n\n\n\n2.9\n\n\n3.9\n\n\n4.9\n\n\n5.9\n\n\n6.9\n\n\n7.9\n\n\n8.9\n\n\n\n\n10.9\n\n\n\n\n\n\n2.10\n\n\n3.10\n\n\n4.10\n\n\n\n\n6.10\n\n\n7.10\n\n\n8.10\n\n\n\n\n10.10\n\n\n\n\n\n\n\n\n3.11\n\n\n4.11\n\n\n\n\n6.11\n\n\n7.11\n\n\n8.11\n\n\n\n\n10.11\n\n\n\n\n\n\n\n\n3.12\n\n\n4.12\n\n\n\n\n\n\n7.12\n\n\n8.12\n\n\n\n\n\n\n\n\n\n\n\n\n3.13\n\n\n4.13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotivation\n\n\nThe main motivation of this project was learning.\nToday there are several good books and other resources from which to learn the material we covered, and we spent some time choosing a good learning project.\nWe chose \nISLR\n because it is an excellent, clear introduction to statistical learning, that  keeps a nice balance between theory, intuition, mathematical rigour and programming.\n\nOur main goal was to use the exercises as an excuse to improve our proficiency using Python's data science stack.\n\nWe had done other data science projects with Python, but, as we imagined, we still had a bit more to learn (and still do!).\nSince the book was written with R in mind, it made the use of Python a cool additional challenge.\nWe are strong advocates of the \nactive learning\n principles, and this project, once more, reinforced them in our minds.\nIf you're starting out in machine learning with Python (or R!), we recommend you try it!\n\n\nTechnical requirements, and How to Install\n\n\nThis project was developed using Python 3.5 on Jupyter notebooks (Jupyter Lab, in fact).\nWe tried to stay within the standard Python data science stack as much as possible.\nAccordingly, our main Python packages were numpy, matplotlib, pandas, seaborn, statsmodels and scikit-learn.\nYou should be able to run this with the standard Python setup, and the additional libraries we list below.\n\n\nIf you're just starting out with Python, here's a more complete 'how-to'. We recommend using \nAnaconda\n whether you are using Linux, Mac or Windows. Anaconda allows you to easily manage several Python environments.\nAn environment is a collection of installed Python packages.\nImagine that you have two projects with different requirements: a recent one with, say, Python 3.5 and matplotlib 4.0, and a legacy project with Python 2.7 and matplotlib 3.5.\nA good environment manager helps you install libraries and allows you to switch between both environments easily, avoiding dependencies migraines.\nYou can even work on both at the same time.\nYou don't want to know what the alternative is, to not using an environment manager.\nSo after installing Anaconda, the easiest way is to create a new environment and just install the libraries we list below one by one.\nAfter this is done, just make sure the desired environment is active (for example, on Linux and Mac, type 'source activate \n', and you're good to go). \n\n\nHere's the list of packages we installed:\n\n\n\n\njupyterlab (but this should run just as well on regular ipython notebooks)\n\n\nnumpy\n\n\npandas\n\n\nmatplotlib\n\n\nsklearn\n\n\nseaborn\n\n\nipywidgets (so that a seaborn import warning goes away)\n\n\nstatsmodels\n\n\nmlxtend \n\n\n\n\nIn addition, we chose mkdocs to present these solutions in a website format, for a better presentation. We might change to a different scheme in the future (any suggestions), but meanwhile we used these libraries:\n\n\n\n\nmkdocs\n\n\nmkdocs-cinder\n\n\npymdown-extensions #for latex \n\n\n\n\nHow to colaborate\n\n\nIf you want to collaborate, you can open an issue in our GitHub project and give us your suggestions on how to improve these solutions.\nOn GitHub, you can also fork this project and send a pull request to fix any mistakes that you have found.\nAlternatively, you can also go for the classical way of collaboration and send us an \ne-mail\n.\nAny effort to improve the quality of these solutions will be appreciated.\n\n\nMain references\n\n\nIn addition to thinking hard about them, to solve the exercises we followed several references.\nBesides ISLR [1], which is \navailable for free\n and explains almost everything you need to know to solve the exercises, \nwe also read some other books that provide a self-contained introduction to the field of statistical learning [2, 3, 4]. \nWe also spent some quality time on \nCrossValidate\n.\nFor the Python data science stack we think Wes McKinney's book [5] is a good choice, as well as Jake VanderPlas' [6].\nAdditional references for some of the exercises are scattered througout the solutions.\n\n\n\n\n[1] \nJames, G., Witten, D., Hastie, T., Tibshirani, R., 2015, An Introduction to Statistical Learning with Applications in R, Springer.\n \n[available for free]\n\n\n[2] \nHastie, T., Tibshirani, R. 2006, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.\n \n[available for free]\n\n\n[3] \nBishop, C.M., 2006, Pattern Recognition and Machine Learning, Springer.\n\n\n[4] \nMurphy, K.P., 2012, Machine Learning: A Probabilistic Perspective, MIT Press.\n\n\n[5] \nMcKinney, W., 2012, Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython\n\n\n[6] \nJake VanderPlas, 2016, Python Data Science Handbook: Essential Tools for Working with Data, O'Reilly Media.\n\n\n\n\nLearning resources\n\n\nFortunately, online resources are becoming more and more an essential tool for self-learning strategies.\nDuring the course of this project, we found several resources that can help you on your learning path. \nHere are the best ones we found. \n\n\n\n\nStanford's online course by the authors of ISLR\n\n\nAndrew Ng's Machine Learning course\n\n\n\n\nOther solutions to ISLR\n\n\nThere are other solutions to ISLR, though most of them do not use Python.\nBelow you can find a link to the solutions we found that were reasonably complete.\nWe did a occasional check with some of these, and they might be a good complementary resource.\nA special mention to \nJWarmenhoven's github repo\n, which uses Python to reproduce figures, tables and calculations of the main text of the chapters and labs.\nPlease let us know if you find any other significant solutions. \n\n\n\n\nJWarmenhoven: Python for the chapters text and labs\n\n\nJames Crouser's Exercises and Python Labs\n\n\nAsadoughi's\n\n\nJohn Weatherwax's solutions to the conceptual and practical exercises in R\n\n\nPierre Pacquay's\n\n\nyahwes'\n\n\n\n\nMIT License\n\n\nCopyright (c) [2017] [ISLP]\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "Home"
        },
        {
            "location": "/#an-introduction-to-statistical-learning-with-applications-in-r-with-python",
            "text": "This page contains the solutions to the exercises proposed in   'An Introduction to Statistical Learning with Applications in R' (ISLR) by James, Witten, Hastie and Tibshirani  [1].   Both conceptual and applied exercises were solved.\nAn effort was made to detail all the answers and to provide a set of bibliographical references that we found useful.\nThe exercises were solved using Python instead of R.\nYou are welcome to  collaborate .  Note [03.October.2017]: we will release each chapter's solutions on a monthly basis (at least).",
            "title": "An Introduction to Statistical Learning: with Applications in R... with Python!"
        },
        {
            "location": "/#solutions",
            "text": "Chapter 2  Chapter 3  Chapter 4  Chapter 5  Chapter 6  Chapter 7  Chapter 8  Chapter 9  Chapter 10      2.1  3.1  4.1  5.1  6.1  7.1  8.1  9.1  10.1    2.2  3.2  4.2  5.2  6.2  7.2  8.2  9.2  10.2    2.3  3.3  4.3  5.3  6.3  7.3  8.3  9.3  10.3    2.4  3.4  4.4  5.4  6.4  7.4  8.4  9.4  10.4    2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5  10.5    2.6  3.6  4.6  5.6  6.6  7.6  8.6  9.6  10.6    2.7  3.7  4.7  5.7  6.7  7.7  8.7  9.7  10.7    2.8  3.8  4.8  5.8  6.8  7.8  8.8  9.8  10.8    2.9  3.9  4.9  5.9  6.9  7.9  8.9   10.9    2.10  3.10  4.10   6.10  7.10  8.10   10.10     3.11  4.11   6.11  7.11  8.11   10.11     3.12  4.12    7.12  8.12       3.13  4.13           3.14            3.15",
            "title": "Solutions"
        },
        {
            "location": "/#motivation",
            "text": "The main motivation of this project was learning.\nToday there are several good books and other resources from which to learn the material we covered, and we spent some time choosing a good learning project.\nWe chose  ISLR  because it is an excellent, clear introduction to statistical learning, that  keeps a nice balance between theory, intuition, mathematical rigour and programming. Our main goal was to use the exercises as an excuse to improve our proficiency using Python's data science stack. \nWe had done other data science projects with Python, but, as we imagined, we still had a bit more to learn (and still do!).\nSince the book was written with R in mind, it made the use of Python a cool additional challenge.\nWe are strong advocates of the  active learning  principles, and this project, once more, reinforced them in our minds.\nIf you're starting out in machine learning with Python (or R!), we recommend you try it!",
            "title": "Motivation"
        },
        {
            "location": "/#technical-requirements-and-how-to-install",
            "text": "This project was developed using Python 3.5 on Jupyter notebooks (Jupyter Lab, in fact).\nWe tried to stay within the standard Python data science stack as much as possible.\nAccordingly, our main Python packages were numpy, matplotlib, pandas, seaborn, statsmodels and scikit-learn.\nYou should be able to run this with the standard Python setup, and the additional libraries we list below.  If you're just starting out with Python, here's a more complete 'how-to'. We recommend using  Anaconda  whether you are using Linux, Mac or Windows. Anaconda allows you to easily manage several Python environments.\nAn environment is a collection of installed Python packages.\nImagine that you have two projects with different requirements: a recent one with, say, Python 3.5 and matplotlib 4.0, and a legacy project with Python 2.7 and matplotlib 3.5.\nA good environment manager helps you install libraries and allows you to switch between both environments easily, avoiding dependencies migraines.\nYou can even work on both at the same time.\nYou don't want to know what the alternative is, to not using an environment manager.\nSo after installing Anaconda, the easiest way is to create a new environment and just install the libraries we list below one by one.\nAfter this is done, just make sure the desired environment is active (for example, on Linux and Mac, type 'source activate  ', and you're good to go).   Here's the list of packages we installed:   jupyterlab (but this should run just as well on regular ipython notebooks)  numpy  pandas  matplotlib  sklearn  seaborn  ipywidgets (so that a seaborn import warning goes away)  statsmodels  mlxtend    In addition, we chose mkdocs to present these solutions in a website format, for a better presentation. We might change to a different scheme in the future (any suggestions), but meanwhile we used these libraries:   mkdocs  mkdocs-cinder  pymdown-extensions #for latex",
            "title": "Technical requirements, and How to Install"
        },
        {
            "location": "/#how-to-colaborate",
            "text": "If you want to collaborate, you can open an issue in our GitHub project and give us your suggestions on how to improve these solutions.\nOn GitHub, you can also fork this project and send a pull request to fix any mistakes that you have found.\nAlternatively, you can also go for the classical way of collaboration and send us an  e-mail .\nAny effort to improve the quality of these solutions will be appreciated.",
            "title": "How to colaborate"
        },
        {
            "location": "/#main-references",
            "text": "In addition to thinking hard about them, to solve the exercises we followed several references.\nBesides ISLR [1], which is  available for free  and explains almost everything you need to know to solve the exercises, \nwe also read some other books that provide a self-contained introduction to the field of statistical learning [2, 3, 4]. \nWe also spent some quality time on  CrossValidate .\nFor the Python data science stack we think Wes McKinney's book [5] is a good choice, as well as Jake VanderPlas' [6].\nAdditional references for some of the exercises are scattered througout the solutions.   [1]  James, G., Witten, D., Hastie, T., Tibshirani, R., 2015, An Introduction to Statistical Learning with Applications in R, Springer.   [available for free]  [2]  Hastie, T., Tibshirani, R. 2006, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.   [available for free]  [3]  Bishop, C.M., 2006, Pattern Recognition and Machine Learning, Springer.  [4]  Murphy, K.P., 2012, Machine Learning: A Probabilistic Perspective, MIT Press.  [5]  McKinney, W., 2012, Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython  [6]  Jake VanderPlas, 2016, Python Data Science Handbook: Essential Tools for Working with Data, O'Reilly Media.",
            "title": "Main references"
        },
        {
            "location": "/#learning-resources",
            "text": "Fortunately, online resources are becoming more and more an essential tool for self-learning strategies.\nDuring the course of this project, we found several resources that can help you on your learning path. \nHere are the best ones we found.    Stanford's online course by the authors of ISLR  Andrew Ng's Machine Learning course",
            "title": "Learning resources"
        },
        {
            "location": "/#other-solutions-to-islr",
            "text": "There are other solutions to ISLR, though most of them do not use Python.\nBelow you can find a link to the solutions we found that were reasonably complete.\nWe did a occasional check with some of these, and they might be a good complementary resource.\nA special mention to  JWarmenhoven's github repo , which uses Python to reproduce figures, tables and calculations of the main text of the chapters and labs.\nPlease let us know if you find any other significant solutions.    JWarmenhoven: Python for the chapters text and labs  James Crouser's Exercises and Python Labs  Asadoughi's  John Weatherwax's solutions to the conceptual and practical exercises in R  Pierre Pacquay's  yahwes'",
            "title": "Other solutions to ISLR"
        },
        {
            "location": "/#mit-license",
            "text": "Copyright (c) [2017] [ISLP]  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "MIT License"
        },
        {
            "location": "/sols/chapter2/exercise1/",
            "text": "Exercise 2.1\n\n\nTo answer to this exercise, we need to understand the \nsources of error\n in a statistical learning method. For regression, assuming \n\\(Y = f(X) + \\varepsilon\\)\n, where \n\\(E[\\varepsilon]=0\\)\n and \n\\(Var[\\varepsilon]=\\sigma_\\varepsilon^2\\)\n, we can always obtain a decomposition of the test mean squared error, \n\\(E[(Y - \\hat{f}(x_0))^2\\)\n, into the sum of the irreducible error, the squared bias and the variance [1, page 223]:\n\n\n\\begin{align}\n\\mathrm{E}\\Big[\\big(Y - \\hat{f}(x)\\big)^2 \\Big|\\, X=x_0 \\Big]\n & =  \\sigma_\\varepsilon^2 + \\mathrm{Bias}^2\\big[\\hat{f}(x_0)\\big] + \\mathrm{Var}\\left[ \\hat{f}(x_0) \\right] , \\\n\\end{align}\nwhere \n\\(\\sigma_\\varepsilon^2\\)\n is the noise or irreducible error, \n\n\n\\[\\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}\\]\nand\n\n\n\\[\\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}\\]\nSince the irreducible error corresponds to the lowest achievable error, a good test set performance of a statistical learning method requires low variance as well as low squared bias.\n\n\nWhen we approximate a problem (possibly very complex), by a simpler model we introduce an error known as \nbias\n.\nThe simplest non-trivial example might be approximating a non-linear relationship (for example, a quadratic one) by a linear function of parameters and predictors. In this case, will have a always non-zero test error, regardless of how well we fit the model parameters, how large the training set is, or even how small the noise is (even zero). The more the true model deviates from a linear one, the larger this error will be.\n\n\nOn the other hand, \nvariance\n refers to the amount by which the estimation function would change if it was estimated using a different training set.\nThe training set is used to estimate the model parameters, which means that we obtain different estimates from different training sets. We hope however that this difference is small, and we say that between estimates from different training sets is small, in which case we say that the learning method has low variance.\nOn the other hand, a method for which small changes in the training set might lead to large changes in the estimated model parameters is referred as a method with high variance.\n\n\nIn general, more flexible methods have less bias and have higher variance. This is referred to as the  \nbias-variance trade-off\n since a low test mean squared error requires both low bias and low variance.\n\n\n(a) Extremely large sample, few predictors\n\n\nA flexible method is expected to be \nbetter\n. \n\n\nSince the sample size is extremely large and the number of predictors is small, a more flexible method would be able to better fit the data, while not fitting the noise due to the very large sample size. In other words a more flexible model would have the upside of a less bias, without much risk of \noverfitting\n.\n\n\n(b) Awful lot of predictors, small sample\n\n\nA flexible method is expected to be  \nworse\n.\n\n\nIt is very likely that when the number of predictors is extremely large and the number of observations is small a flexible model would fit the noise, meaning that, given another random data set of the same distribuition, the fit would likely be significantly different. Therefore one would be better off using a less flexible method, which will have more bias, but will be less likely to overfit.\n\n\n(c) Highly non-linear relationship\n\n\nA flexible method is expected to be  \nbetter\n.\n\n\nA more flexible method will likely be necessary to model a highly non-linear relationship, otherwise the model will be too biased and not capture the non-linearities of the model. No matter how large the sample size, a less flexible model would always be limited.\n\n\n(d) Extremely high variance\n\n\nA flexible method is expected to be  \nworse\n.\n\n\nSince the variance is extremely high, a more flexible model will fit the noise more and thus very likely overfit. A less flexible model will be more likely to still capture the essential 'features' of the model without picking up extraneous ones induced by the noise.\n\n\nFurther reading\n\n\nThe bias-variance decomposition is a fundamental aspect of machine learning which is present not only in regression. This decomposition has been generalized to more general loss functions and to classification learning methods. See, for example, [2].\n\n\n\n\n[1] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. \nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n. Springer, 2009.\n\n\n[2] James, Gareth M. \n\"Variance and bias for general loss functions.\"\n  Machine learning 51.2 (2003): 115-135.",
            "title": "2.1"
        },
        {
            "location": "/sols/chapter2/exercise1/#exercise-21",
            "text": "To answer to this exercise, we need to understand the  sources of error  in a statistical learning method. For regression, assuming  \\(Y = f(X) + \\varepsilon\\) , where  \\(E[\\varepsilon]=0\\)  and  \\(Var[\\varepsilon]=\\sigma_\\varepsilon^2\\) , we can always obtain a decomposition of the test mean squared error,  \\(E[(Y - \\hat{f}(x_0))^2\\) , into the sum of the irreducible error, the squared bias and the variance [1, page 223]:  \\begin{align}\n\\mathrm{E}\\Big[\\big(Y - \\hat{f}(x)\\big)^2 \\Big|\\, X=x_0 \\Big]\n & =  \\sigma_\\varepsilon^2 + \\mathrm{Bias}^2\\big[\\hat{f}(x_0)\\big] + \\mathrm{Var}\\left[ \\hat{f}(x_0) \\right] , \\\n\\end{align}\nwhere  \\(\\sigma_\\varepsilon^2\\)  is the noise or irreducible error,   \\[\\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}\\] and  \\[\\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}\\] Since the irreducible error corresponds to the lowest achievable error, a good test set performance of a statistical learning method requires low variance as well as low squared bias.  When we approximate a problem (possibly very complex), by a simpler model we introduce an error known as  bias .\nThe simplest non-trivial example might be approximating a non-linear relationship (for example, a quadratic one) by a linear function of parameters and predictors. In this case, will have a always non-zero test error, regardless of how well we fit the model parameters, how large the training set is, or even how small the noise is (even zero). The more the true model deviates from a linear one, the larger this error will be.  On the other hand,  variance  refers to the amount by which the estimation function would change if it was estimated using a different training set.\nThe training set is used to estimate the model parameters, which means that we obtain different estimates from different training sets. We hope however that this difference is small, and we say that between estimates from different training sets is small, in which case we say that the learning method has low variance.\nOn the other hand, a method for which small changes in the training set might lead to large changes in the estimated model parameters is referred as a method with high variance.  In general, more flexible methods have less bias and have higher variance. This is referred to as the   bias-variance trade-off  since a low test mean squared error requires both low bias and low variance.",
            "title": "Exercise 2.1"
        },
        {
            "location": "/sols/chapter2/exercise1/#a-extremely-large-sample-few-predictors",
            "text": "A flexible method is expected to be  better .   Since the sample size is extremely large and the number of predictors is small, a more flexible method would be able to better fit the data, while not fitting the noise due to the very large sample size. In other words a more flexible model would have the upside of a less bias, without much risk of  overfitting .",
            "title": "(a) Extremely large sample, few predictors"
        },
        {
            "location": "/sols/chapter2/exercise1/#b-awful-lot-of-predictors-small-sample",
            "text": "A flexible method is expected to be   worse .  It is very likely that when the number of predictors is extremely large and the number of observations is small a flexible model would fit the noise, meaning that, given another random data set of the same distribuition, the fit would likely be significantly different. Therefore one would be better off using a less flexible method, which will have more bias, but will be less likely to overfit.",
            "title": "(b) Awful lot of predictors, small sample"
        },
        {
            "location": "/sols/chapter2/exercise1/#c-highly-non-linear-relationship",
            "text": "A flexible method is expected to be   better .  A more flexible method will likely be necessary to model a highly non-linear relationship, otherwise the model will be too biased and not capture the non-linearities of the model. No matter how large the sample size, a less flexible model would always be limited.",
            "title": "(c) Highly non-linear relationship"
        },
        {
            "location": "/sols/chapter2/exercise1/#d-extremely-high-variance",
            "text": "A flexible method is expected to be   worse .  Since the variance is extremely high, a more flexible model will fit the noise more and thus very likely overfit. A less flexible model will be more likely to still capture the essential 'features' of the model without picking up extraneous ones induced by the noise.",
            "title": "(d) Extremely high variance"
        },
        {
            "location": "/sols/chapter2/exercise1/#further-reading",
            "text": "The bias-variance decomposition is a fundamental aspect of machine learning which is present not only in regression. This decomposition has been generalized to more general loss functions and to classification learning methods. See, for example, [2].   [1] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman.  The Elements of Statistical Learning: Data Mining, Inference, and Prediction . Springer, 2009.  [2] James, Gareth M.  \"Variance and bias for general loss functions.\"   Machine learning 51.2 (2003): 115-135.",
            "title": "Further reading"
        },
        {
            "location": "/sols/chapter2/exercise2/",
            "text": "Exercise 2.2\n\n\n(a) CEO salary\n\n\nRegression\n, since the variable we are interested in, CEO salary, is quantitative.\nSince we want to understand the factors affecting CEO salary we are most interested in \ninference\n.\nThe number of predictors is 3, and the sample size is 500 \n(p=3, n=500)\n.\n\n\n(b) New product launch: success or failure?\n\n\nClassification\n, because we are studying the outcome of the launch of a new product, which we are representing in one of two categories: success or failure.\nIn this case we wish to know whether a new launch will be a success or a failure more than understand why, so we are more interested in \nprediction\n.\nThe number of predictors is 13, and the sample size is 20 \n(p=13, n=20)\n.\n\n\n(c) Change in the US dollar\n\n\nRegression\n, since the variable we want to predict is a real number, and we are more interested in \nprediction\n.\nThe number of predictors is 3, and the sample size is 52 \n(p=3, n=52)\n.",
            "title": "2.2"
        },
        {
            "location": "/sols/chapter2/exercise2/#exercise-22",
            "text": "",
            "title": "Exercise 2.2"
        },
        {
            "location": "/sols/chapter2/exercise2/#a-ceo-salary",
            "text": "Regression , since the variable we are interested in, CEO salary, is quantitative.\nSince we want to understand the factors affecting CEO salary we are most interested in  inference .\nThe number of predictors is 3, and the sample size is 500  (p=3, n=500) .",
            "title": "(a) CEO salary"
        },
        {
            "location": "/sols/chapter2/exercise2/#b-new-product-launch-success-or-failure",
            "text": "Classification , because we are studying the outcome of the launch of a new product, which we are representing in one of two categories: success or failure.\nIn this case we wish to know whether a new launch will be a success or a failure more than understand why, so we are more interested in  prediction .\nThe number of predictors is 13, and the sample size is 20  (p=13, n=20) .",
            "title": "(b) New product launch: success or failure?"
        },
        {
            "location": "/sols/chapter2/exercise2/#c-change-in-the-us-dollar",
            "text": "Regression , since the variable we want to predict is a real number, and we are more interested in  prediction .\nThe number of predictors is 3, and the sample size is 52  (p=3, n=52) .",
            "title": "(c) Change in the US dollar"
        },
        {
            "location": "/sols/chapter2/exercise3/",
            "text": "Exercise 2.3\n\n\nIn (a) we provide a rough sketch of the general case, which is merely illustrative of the general behaviour of each quantity as described in (b). They are similar to the left panel of  figure 2.9 of the text and the left panel of figure 2.12. \n\n\n(a) Sketch of bias, variance, training, test and Bayes errors\n\n\n# the functions chosen here were chosen just as a rough, quick way to sketch the functions in a plot\n# they do not represent in any way an analytical formula for these quantities or anything of the sort\n# these formulas would depend on the model and fitting procedure in any case\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0.0, 10.0, 0.02)\n\ndef squared_bias(x):\n    return .002*(-x+10)**3\ndef variance(x):\n    return .002*x**3 \ndef training_error(x):\n    return 2.38936 - 0.825077*x + 0.176655*x**2 - 0.0182319*x**3 + 0.00067091*x**4\ndef test_error(x):\n    return 3 - 0.6*x + .06*x**2\ndef bayes_error(x):\n    return x + 1 - x\n\nplt.xkcd()\n#frame = plt.gca()\n#frame.axes.xaxis.set_ticklabels([])\nplt.figure(figsize=(10, 8))\nplt.plot(x,squared_bias(x), label='squared bias')\nplt.plot(x, variance(x), label='variance')\nplt.plot(x, training_error(x), label='training error')\nplt.plot(x, test_error(x), label='test error')\nplt.plot(x, bayes_error(x), label='Bayes error')\nplt.legend(loc='upper center')\nplt.xlabel('model flexibility')\nplt.show()\n\n#arbitrary units\n\n\n\n\n\n\n(b) Why these shapes?\n\n\nSquared bias.\n This is the error in our model introduced by the difference of our approximation and the true underlying function. A more flexible model will be increasingly similar,  and the squared bias therefore dimishes as the flexibility increases. (It might even reach zero if for example the underlying function is a polynomial and by increasing the flexibility at a certain point we include the polynomials of this degree in our hypothesis space.)\n\n\nVariance.\n In the limit of a model with no flexibility the variance will be zero, since the model fit will be independent of the data. As the flexibility increases the variance will increase as well since the noise in a particular training set will correspondingly captured by the model. The curve described by the variance is an monotonically increasing function of the flexibility of the model.\n\n\nTraining error.\n The training error is given by the average (squared) difference between the predictions of the model and the observations. If a model if very unflexible this can be quite high, but as the flexibility increases this difference will decrease. If we consider polynomials for example increasing the flexibility of the model might mean increasing the degree of the polynomial to be fitted. The additional degrees of freedom will decrease the average difference and reduce the training error.\n\n\nTest error.\n Ths expected test error is given by the formula: Variance + Bias + Bayes error, all of which are non-negative. The Bayes error is constant and a lower bound for the test error. The test error has a minimum at an intermediate level of flexibility: not too flexible, so that the variance does not dominate, and not too unflexible, so that the squared bias is not too high. The plot of the test error thus resembles sort of an upward (deformed) parabola: high for unflexible models, decreasing as flexibility increases until it reaches a minimum. Then the variance starts to dominate and the test error starts increasing. The distance between this minimum and the Bayes irreducible error gives us an idea of how well the best function in the hypothesis space will fit.\n\n\nBayes error.\n This term is constant since by definition it does not depend on X and therefore on the flexibility of the model.",
            "title": "2.3"
        },
        {
            "location": "/sols/chapter2/exercise3/#exercise-23",
            "text": "In (a) we provide a rough sketch of the general case, which is merely illustrative of the general behaviour of each quantity as described in (b). They are similar to the left panel of  figure 2.9 of the text and the left panel of figure 2.12.",
            "title": "Exercise 2.3"
        },
        {
            "location": "/sols/chapter2/exercise3/#a-sketch-of-bias-variance-training-test-and-bayes-errors",
            "text": "# the functions chosen here were chosen just as a rough, quick way to sketch the functions in a plot\n# they do not represent in any way an analytical formula for these quantities or anything of the sort\n# these formulas would depend on the model and fitting procedure in any case\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0.0, 10.0, 0.02)\n\ndef squared_bias(x):\n    return .002*(-x+10)**3\ndef variance(x):\n    return .002*x**3 \ndef training_error(x):\n    return 2.38936 - 0.825077*x + 0.176655*x**2 - 0.0182319*x**3 + 0.00067091*x**4\ndef test_error(x):\n    return 3 - 0.6*x + .06*x**2\ndef bayes_error(x):\n    return x + 1 - x\n\nplt.xkcd()\n#frame = plt.gca()\n#frame.axes.xaxis.set_ticklabels([])\nplt.figure(figsize=(10, 8))\nplt.plot(x,squared_bias(x), label='squared bias')\nplt.plot(x, variance(x), label='variance')\nplt.plot(x, training_error(x), label='training error')\nplt.plot(x, test_error(x), label='test error')\nplt.plot(x, bayes_error(x), label='Bayes error')\nplt.legend(loc='upper center')\nplt.xlabel('model flexibility')\nplt.show()\n\n#arbitrary units",
            "title": "(a) Sketch of bias, variance, training, test and Bayes errors"
        },
        {
            "location": "/sols/chapter2/exercise3/#b-why-these-shapes",
            "text": "Squared bias.  This is the error in our model introduced by the difference of our approximation and the true underlying function. A more flexible model will be increasingly similar,  and the squared bias therefore dimishes as the flexibility increases. (It might even reach zero if for example the underlying function is a polynomial and by increasing the flexibility at a certain point we include the polynomials of this degree in our hypothesis space.)  Variance.  In the limit of a model with no flexibility the variance will be zero, since the model fit will be independent of the data. As the flexibility increases the variance will increase as well since the noise in a particular training set will correspondingly captured by the model. The curve described by the variance is an monotonically increasing function of the flexibility of the model.  Training error.  The training error is given by the average (squared) difference between the predictions of the model and the observations. If a model if very unflexible this can be quite high, but as the flexibility increases this difference will decrease. If we consider polynomials for example increasing the flexibility of the model might mean increasing the degree of the polynomial to be fitted. The additional degrees of freedom will decrease the average difference and reduce the training error.  Test error.  Ths expected test error is given by the formula: Variance + Bias + Bayes error, all of which are non-negative. The Bayes error is constant and a lower bound for the test error. The test error has a minimum at an intermediate level of flexibility: not too flexible, so that the variance does not dominate, and not too unflexible, so that the squared bias is not too high. The plot of the test error thus resembles sort of an upward (deformed) parabola: high for unflexible models, decreasing as flexibility increases until it reaches a minimum. Then the variance starts to dominate and the test error starts increasing. The distance between this minimum and the Bayes irreducible error gives us an idea of how well the best function in the hypothesis space will fit.  Bayes error.  This term is constant since by definition it does not depend on X and therefore on the flexibility of the model.",
            "title": "(b) Why these shapes?"
        },
        {
            "location": "/sols/chapter2/exercise4/",
            "text": "Exercise 2.4\n\n\n(a) Classification\n\n\nSpam filters\n. \nAny self-respecting email service has a good spam filter: ideally it keeps any undesired email away from your inbox, and let's through every email you're interested in.\nThe response has just two categories: 'spam' or 'not spam', and the predictors can include several different variables: whether you have corresponded with the sender, whether you have their email address in some of your non-spam emails, the words and sequences of words of the emails (for example, if the email includes any reference to a 'Nigerian prince'), whether similar emails have already been tagged as spam by other users, etc.\nThe main goal is \nprediction\n because the most important task is to accurately be able to predict whether a future email message is spam or not-spam. \nAn important aspect in this case is the rate of false positives and false negatives.\nIn the case of email it is usually much more acceptable to have false negatives (spam that got through to your inbox) than false positives (important email that was classified as spam and you have never noticed). \n\n\nDigit recognition. (or text or speech or facial recognition)\n \nAn important task required everyday around the world is the recognition of handwritten digits and addresses for postal service or document scanning and OCR (Optical Character Recognition).\nDepending on how we define the learning task, the predictors can be the original digital photographs or scans of the documents, or just a cropped, grayscale image of a single digit (that is, a N by M matrix of real numbers, each describing the gray scale value of a single pixel).\nThe response might be one of ten digits (0 to 9) or it might include commas and the full numbers (not just the digits), depending on how we decide to define the task.\nAgain here the main goal is \nprediction\n, since the most important thing is to recognize a correct address or bank account number from a letter or document. \n\n\nSocial analysis\n. Classify people into supporters of \nSporting\n, supporters of \nBenfica\n, neither or both (so the response is one of these four categories).\nAs predictors, include factors such as age, nationality, address, income, education, gender, whether they appreciate classical music, their criminal record, etc.\nHere the main goal is \ninference\n, since more than predicting whether someone supports a specific club we are interested in understanding and studying the different factors and discovering interesting relationships between them.\n\n\nOther examples\n: fraud detection, medical diagnosis, stock prediction (buy, sell, hold), astronomical objects classification, choosing \nstrawberries\n or \ncucumbers\n.\n\n\n(b) Regression\n\n\nGalton's original 'regression' to the mean\n. \nStudy the height of children and its relationship to the height of their parents.\nHere the response is the height of the child and the predictor is the height of both parents (or the mean of both).\nThe main goal would be \ninference\n, since we are trying to better understand, from a scientific perspective, how much genetics influences an individual's height.\nUnless, of course, you are an overly zealous future parent trying to predict the chances of your child playing in the NBA given your own height and your partner's - in which case your main goal might be prediction.\nThis problem is \nwhere the name 'regression' comes from\n and, perhaps surprisingly, \nGalton's statistical method\n has stood the test of time quite well.\n\n\nHouse prices\n. Buying a house is probably the biggest investment that many people do during their life time.\nAccordingly, it is natural that people want to know the value of a house in order to do the best deal possible.\nThe main goal in this cases is \nprediction\n, since we want to predict house's price considering a set of predictors. \nThere are several factors used to predict house's price. \nFor example, a \nKaggle competition\n in which we participated, suggests a total of 79 predictors to predict the final price of each house in Boston.\n\n\nWeather\n.\nThe response can be temperature, wind velocity or precipitation amount in different locations, and the predictors can be the same variables for previous times.\nThe goal is both \nprediction\n (will it rain too much this month and ruin the plantations; will there be enough snow for the snowtrip?) and \ninference\n (what causes hurricanes, or the current climate change?). \n\n\nOther examples\n:\nreturns of investment in stocks, predicting school grades from amount and type of study, predicting the popularity of a book, film, article or tweet.\n\n\n(c) Cluster analysis\n\n\nOutlier detection\n.\nIf you can define a distance between the data points or a density function in the predictors space, you can use cluster analysis to identify candidate outliers such as the data points farther from the majority of the points, or those in sparser regions.\n\n\nMarket research\n. \nUsing cluster analysis, market researchers can group consumers into market segments according to their similarities.\nThis can provide a better understanding on group characteristics, as well as it can provide useful insights about how different groups of consumers/potential customer behave and relate with different groups.\n\n\nRecommender systems\n.\nWill you think watching 'Pulp Fiction' is time well spent?\nIf enough is known about you, including some of your film preferences (and perhaps how many times you have watched 'Stalker' or 'Pulp Fiction') and some similar data about other people, we can have an indication of whether will you think rewatching 'Pulp Fiction' is worth your time...\nAre you 'closer' to the people who think the answer is 'No' or to the ones that think 'Yes'?\n\n\nOther examples\n:\ngene sequencing, data compression, social network analysis, representation learning, topography classification.",
            "title": "2.4"
        },
        {
            "location": "/sols/chapter2/exercise4/#exercise-24",
            "text": "",
            "title": "Exercise 2.4"
        },
        {
            "location": "/sols/chapter2/exercise4/#a-classification",
            "text": "Spam filters . \nAny self-respecting email service has a good spam filter: ideally it keeps any undesired email away from your inbox, and let's through every email you're interested in.\nThe response has just two categories: 'spam' or 'not spam', and the predictors can include several different variables: whether you have corresponded with the sender, whether you have their email address in some of your non-spam emails, the words and sequences of words of the emails (for example, if the email includes any reference to a 'Nigerian prince'), whether similar emails have already been tagged as spam by other users, etc.\nThe main goal is  prediction  because the most important task is to accurately be able to predict whether a future email message is spam or not-spam. \nAn important aspect in this case is the rate of false positives and false negatives.\nIn the case of email it is usually much more acceptable to have false negatives (spam that got through to your inbox) than false positives (important email that was classified as spam and you have never noticed).   Digit recognition. (or text or speech or facial recognition)  \nAn important task required everyday around the world is the recognition of handwritten digits and addresses for postal service or document scanning and OCR (Optical Character Recognition).\nDepending on how we define the learning task, the predictors can be the original digital photographs or scans of the documents, or just a cropped, grayscale image of a single digit (that is, a N by M matrix of real numbers, each describing the gray scale value of a single pixel).\nThe response might be one of ten digits (0 to 9) or it might include commas and the full numbers (not just the digits), depending on how we decide to define the task.\nAgain here the main goal is  prediction , since the most important thing is to recognize a correct address or bank account number from a letter or document.   Social analysis . Classify people into supporters of  Sporting , supporters of  Benfica , neither or both (so the response is one of these four categories).\nAs predictors, include factors such as age, nationality, address, income, education, gender, whether they appreciate classical music, their criminal record, etc.\nHere the main goal is  inference , since more than predicting whether someone supports a specific club we are interested in understanding and studying the different factors and discovering interesting relationships between them.  Other examples : fraud detection, medical diagnosis, stock prediction (buy, sell, hold), astronomical objects classification, choosing  strawberries  or  cucumbers .",
            "title": "(a) Classification"
        },
        {
            "location": "/sols/chapter2/exercise4/#b-regression",
            "text": "Galton's original 'regression' to the mean . \nStudy the height of children and its relationship to the height of their parents.\nHere the response is the height of the child and the predictor is the height of both parents (or the mean of both).\nThe main goal would be  inference , since we are trying to better understand, from a scientific perspective, how much genetics influences an individual's height.\nUnless, of course, you are an overly zealous future parent trying to predict the chances of your child playing in the NBA given your own height and your partner's - in which case your main goal might be prediction.\nThis problem is  where the name 'regression' comes from  and, perhaps surprisingly,  Galton's statistical method  has stood the test of time quite well.  House prices . Buying a house is probably the biggest investment that many people do during their life time.\nAccordingly, it is natural that people want to know the value of a house in order to do the best deal possible.\nThe main goal in this cases is  prediction , since we want to predict house's price considering a set of predictors. \nThere are several factors used to predict house's price. \nFor example, a  Kaggle competition  in which we participated, suggests a total of 79 predictors to predict the final price of each house in Boston.  Weather .\nThe response can be temperature, wind velocity or precipitation amount in different locations, and the predictors can be the same variables for previous times.\nThe goal is both  prediction  (will it rain too much this month and ruin the plantations; will there be enough snow for the snowtrip?) and  inference  (what causes hurricanes, or the current climate change?).   Other examples :\nreturns of investment in stocks, predicting school grades from amount and type of study, predicting the popularity of a book, film, article or tweet.",
            "title": "(b) Regression"
        },
        {
            "location": "/sols/chapter2/exercise4/#c-cluster-analysis",
            "text": "Outlier detection .\nIf you can define a distance between the data points or a density function in the predictors space, you can use cluster analysis to identify candidate outliers such as the data points farther from the majority of the points, or those in sparser regions.  Market research . \nUsing cluster analysis, market researchers can group consumers into market segments according to their similarities.\nThis can provide a better understanding on group characteristics, as well as it can provide useful insights about how different groups of consumers/potential customer behave and relate with different groups.  Recommender systems .\nWill you think watching 'Pulp Fiction' is time well spent?\nIf enough is known about you, including some of your film preferences (and perhaps how many times you have watched 'Stalker' or 'Pulp Fiction') and some similar data about other people, we can have an indication of whether will you think rewatching 'Pulp Fiction' is worth your time...\nAre you 'closer' to the people who think the answer is 'No' or to the ones that think 'Yes'?  Other examples :\ngene sequencing, data compression, social network analysis, representation learning, topography classification.",
            "title": "(c) Cluster analysis"
        },
        {
            "location": "/sols/chapter2/exercise5/",
            "text": "Exercise 2.5\n\n\nA very flexible approach (versus a less flexible)\n\n\nAdvantages\n\n\n\n\nLess bias.\n\n\nGiven enough data, better results.\n\n\n\n\nDisadvantages\n\n\n\n\nMore risk of overfitting.  \n\n\nHarder to train.\n\n\nLonger to train.\n\n\nComputational more demanding.\n\n\nLess clear interpretability.\n\n\n\n\nWhen is one approach preferable?\n\n\nMore flexible\n\n\n\n\nLarge sample size and small number of predictors.\n\n\nNon-linear relationship between the predictors and response.\n\n\n\n\nLess flexible\n\n\n\n\nSmall sample size and large number of predictors.\n\n\nHigh variance of the error terms.",
            "title": "2.5"
        },
        {
            "location": "/sols/chapter2/exercise5/#exercise-25",
            "text": "",
            "title": "Exercise 2.5"
        },
        {
            "location": "/sols/chapter2/exercise5/#a-very-flexible-approach-versus-a-less-flexible",
            "text": "",
            "title": "A very flexible approach (versus a less flexible)"
        },
        {
            "location": "/sols/chapter2/exercise5/#advantages",
            "text": "Less bias.  Given enough data, better results.",
            "title": "Advantages"
        },
        {
            "location": "/sols/chapter2/exercise5/#disadvantages",
            "text": "More risk of overfitting.    Harder to train.  Longer to train.  Computational more demanding.  Less clear interpretability.",
            "title": "Disadvantages"
        },
        {
            "location": "/sols/chapter2/exercise5/#when-is-one-approach-preferable",
            "text": "",
            "title": "When is one approach preferable?"
        },
        {
            "location": "/sols/chapter2/exercise5/#more-flexible",
            "text": "Large sample size and small number of predictors.  Non-linear relationship between the predictors and response.",
            "title": "More flexible"
        },
        {
            "location": "/sols/chapter2/exercise5/#less-flexible",
            "text": "Small sample size and large number of predictors.  High variance of the error terms.",
            "title": "Less flexible"
        },
        {
            "location": "/sols/chapter2/exercise6/",
            "text": "Exercise 2.6\n\n\nThat this exercise does not ask for a precise definition of parametric and non-parametric methods and that the main text does also not provide one (section 2.1.2) is a sign that this definition is not consensual, and in fact it depends on the context in which it is used in statistics [1].\nFor our purposes we follow both the main text and Murphy [2] (and a few others [3-6]), for a definition of a parametric and non-parametric model (and by model, this we mean a probability distribution or density, or a regression function).\nSo, for our purposes, a parametric model means two thin\ngs: (1) strong, explicit assumptions about the form of the model, (2) which is parametrized by a finite, fixed number of parameters that does not change with the amount of data.\n(Yes, the first part of this definition is vague.)\nConversely, a non-parametric model means that, in general, the number of parameters depends on the amount of data and that we try to keep assumptions as weak as possible.\n'Non-parametric' is thus an unfortunate name since it does not mean 'no parameters'.\nOn the contrary, they do contain parameters (often many more) but these tend to determine the model complexity rather than the form.\nThe typical examples of a parametric and non-parametric models are linear regression and KNN, respectively.\nAnother good example, is the one from the text and figures 2.4, 2.5 and 2.6.\n\n\nThe advantages of a parametric approach are, in general, that it requires less observations, is faster and more computationally tractable, and more robust (less misguided by noise).\nThe disadvantages are the stronger assumptions that make the model less flexible, perhaps unable to capture the underlying adequately regardless of the amount of training data.\n\n\nThere are also other types of models such as semi-parametric models [6], which have two components: a parametric and a non-parametric one.\nOne can also have a non-parametric model by using a parametric model but aditionally adapting the number of parameters as needed (say the degree of the polynomial in a linear regression).\n\n\nReferences\n\n\n\n\n[1] https://en.wikipedia.org/wiki/Nonparametric_statistics#Definitions\n\n\n[1] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.\n\n\n[2] Domingos, Pedro. \"A few useful things to know about machine learning.\" Communications of the ACM 55.10 (2012): 78-87.\n\n\n[3] Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013.\n\n\n[4] Bishop, C.M.: Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA (2006)\n\n\n[5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n\n\n[6] Wellner, Jon A., Chris AJ Klaassen, and Ya'acov Ritov. \"Semiparametric models: a review of progress since BKRW (1993).\" (2006): 25-44.",
            "title": "2.6"
        },
        {
            "location": "/sols/chapter2/exercise6/#exercise-26",
            "text": "That this exercise does not ask for a precise definition of parametric and non-parametric methods and that the main text does also not provide one (section 2.1.2) is a sign that this definition is not consensual, and in fact it depends on the context in which it is used in statistics [1].\nFor our purposes we follow both the main text and Murphy [2] (and a few others [3-6]), for a definition of a parametric and non-parametric model (and by model, this we mean a probability distribution or density, or a regression function).\nSo, for our purposes, a parametric model means two thin\ngs: (1) strong, explicit assumptions about the form of the model, (2) which is parametrized by a finite, fixed number of parameters that does not change with the amount of data.\n(Yes, the first part of this definition is vague.)\nConversely, a non-parametric model means that, in general, the number of parameters depends on the amount of data and that we try to keep assumptions as weak as possible.\n'Non-parametric' is thus an unfortunate name since it does not mean 'no parameters'.\nOn the contrary, they do contain parameters (often many more) but these tend to determine the model complexity rather than the form.\nThe typical examples of a parametric and non-parametric models are linear regression and KNN, respectively.\nAnother good example, is the one from the text and figures 2.4, 2.5 and 2.6.  The advantages of a parametric approach are, in general, that it requires less observations, is faster and more computationally tractable, and more robust (less misguided by noise).\nThe disadvantages are the stronger assumptions that make the model less flexible, perhaps unable to capture the underlying adequately regardless of the amount of training data.  There are also other types of models such as semi-parametric models [6], which have two components: a parametric and a non-parametric one.\nOne can also have a non-parametric model by using a parametric model but aditionally adapting the number of parameters as needed (say the degree of the polynomial in a linear regression).",
            "title": "Exercise 2.6"
        },
        {
            "location": "/sols/chapter2/exercise6/#references",
            "text": "[1] https://en.wikipedia.org/wiki/Nonparametric_statistics#Definitions  [1] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.  [2] Domingos, Pedro. \"A few useful things to know about machine learning.\" Communications of the ACM 55.10 (2012): 78-87.  [3] Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013.  [4] Bishop, C.M.: Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA (2006)  [5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.  [6] Wellner, Jon A., Chris AJ Klaassen, and Ya'acov Ritov. \"Semiparametric models: a review of progress since BKRW (1993).\" (2006): 25-44.",
            "title": "References"
        },
        {
            "location": "/sols/chapter2/exercise7/",
            "text": "Exercise 2.7\n\n\nimport numpy as np\nimport pandas as pd\n\nd = {'X1': pd.Series([0,2,0,0,-1,1]),\n     'X2': pd.Series([3,0,1,1,0,1]),\n     'X3': pd.Series([0,0,3,2,1,1]),\n     'Y': pd.Series(['Red','Red','Red','Green','Green','Red'])}\ndf = pd.DataFrame(d)\ndf.index = np.arange(1, len(df) + 1)\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nY\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n0\n\n      \n3\n\n      \n0\n\n      \nRed\n\n    \n\n    \n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \nRed\n\n    \n\n    \n\n      \n3\n\n      \n0\n\n      \n1\n\n      \n3\n\n      \nRed\n\n    \n\n    \n\n      \n4\n\n      \n0\n\n      \n1\n\n      \n2\n\n      \nGreen\n\n    \n\n    \n\n      \n5\n\n      \n-1\n\n      \n0\n\n      \n1\n\n      \nGreen\n\n    \n\n    \n\n      \n6\n\n      \n1\n\n      \n1\n\n      \n1\n\n      \nRed\n\n    \n\n  \n\n\n\n\n\n\n\n(a) Euclidian distance\n\n\nfrom math import sqrt\ndf['distance']=np.sqrt(df['X1']**2+df['X2']**2+df['X3']**2)\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nY\n\n      \ndistance\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n0\n\n      \n3\n\n      \n0\n\n      \nRed\n\n      \n3.000000\n\n    \n\n    \n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \nRed\n\n      \n2.000000\n\n    \n\n    \n\n      \n3\n\n      \n0\n\n      \n1\n\n      \n3\n\n      \nRed\n\n      \n3.162278\n\n    \n\n    \n\n      \n4\n\n      \n0\n\n      \n1\n\n      \n2\n\n      \nGreen\n\n      \n2.236068\n\n    \n\n    \n\n      \n5\n\n      \n-1\n\n      \n0\n\n      \n1\n\n      \nGreen\n\n      \n1.414214\n\n    \n\n    \n\n      \n6\n\n      \n1\n\n      \n1\n\n      \n1\n\n      \nRed\n\n      \n1.732051\n\n    \n\n  \n\n\n\n\n\n\n\n(b) K=1\n\n\ndf.sort_values(['distance'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nY\n\n      \ndistance\n\n    \n\n  \n\n  \n\n    \n\n      \n5\n\n      \n-1\n\n      \n0\n\n      \n1\n\n      \nGreen\n\n      \n1.414214\n\n    \n\n    \n\n      \n6\n\n      \n1\n\n      \n1\n\n      \n1\n\n      \nRed\n\n      \n1.732051\n\n    \n\n    \n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \nRed\n\n      \n2.000000\n\n    \n\n    \n\n      \n4\n\n      \n0\n\n      \n1\n\n      \n2\n\n      \nGreen\n\n      \n2.236068\n\n    \n\n    \n\n      \n1\n\n      \n0\n\n      \n3\n\n      \n0\n\n      \nRed\n\n      \n3.000000\n\n    \n\n    \n\n      \n3\n\n      \n0\n\n      \n1\n\n      \n3\n\n      \nRed\n\n      \n3.162278\n\n    \n\n  \n\n\n\n\n\n\n\nAs we can see by sorting the data by distance to the origin, for K=1, our prediction is \nGreen\n, since that's the value of the nearest neighbor (point 5 at distance 1.41). \n\n\n(c) K=3\n\n\nOn the other hand, for K=3 our prediction is \nRed\n, because that's the mode of the 3 nearest neigbours: Green, Red and Red (points 5, 6 and 2, respectively).\n\n\n(d) Highly non-linear Bayes decision boundary\n\n\nA large value of K leads to a smoother decision boundary, as if the non-linearities where averaged out. This happens because KNN uses majority voting and this means less emphasis on individual points. For a large value of K, we will likely have a decision boundary which varies very little from point to point, since the result of this majority voting would have to change while for most points this will be a large majority. That is, one of its nearest neighbors changing from one class to the other would still leave the majority voting the same. By contrast, when K is very small, the decision boundary will be better able to capture local non-linearities, because it the majority of neighbors can vary significantly from point to point since the are so few neighbors. Accordingly, we would expect \nthe best value of K to be small\n. \n\n\nImagine this simple example: a true linear boundary that splits the plane in two semi-planes (classes A and B), but with an additional enclave in one of the regions. That is, the true model has a small region of, say, class A inside the class B semi-plane. Would we more likely capture this region for small or large K? Say we have 3 neighboring data points in the class A enclave inside the large class B region with say 50 points. If K > 4, each of the 3 points in the enclave will be classified as B, since they always lose by majority voting (unless K is so large, say 100, that many of the points from A semi-plane region enter this voting). If, on the other hand, K < 4, each of the 3 class A points inside the enclave will be classified as class A, and we will capture this non-linearity of the regions.",
            "title": "2.7"
        },
        {
            "location": "/sols/chapter2/exercise7/#exercise-27",
            "text": "import numpy as np\nimport pandas as pd\n\nd = {'X1': pd.Series([0,2,0,0,-1,1]),\n     'X2': pd.Series([3,0,1,1,0,1]),\n     'X3': pd.Series([0,0,3,2,1,1]),\n     'Y': pd.Series(['Red','Red','Red','Green','Green','Red'])}\ndf = pd.DataFrame(d)\ndf.index = np.arange(1, len(df) + 1)\ndf   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       Y \n     \n   \n   \n     \n       1 \n       0 \n       3 \n       0 \n       Red \n     \n     \n       2 \n       2 \n       0 \n       0 \n       Red \n     \n     \n       3 \n       0 \n       1 \n       3 \n       Red \n     \n     \n       4 \n       0 \n       1 \n       2 \n       Green \n     \n     \n       5 \n       -1 \n       0 \n       1 \n       Green \n     \n     \n       6 \n       1 \n       1 \n       1 \n       Red",
            "title": "Exercise 2.7"
        },
        {
            "location": "/sols/chapter2/exercise7/#a-euclidian-distance",
            "text": "from math import sqrt\ndf['distance']=np.sqrt(df['X1']**2+df['X2']**2+df['X3']**2)\ndf   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       Y \n       distance \n     \n   \n   \n     \n       1 \n       0 \n       3 \n       0 \n       Red \n       3.000000 \n     \n     \n       2 \n       2 \n       0 \n       0 \n       Red \n       2.000000 \n     \n     \n       3 \n       0 \n       1 \n       3 \n       Red \n       3.162278 \n     \n     \n       4 \n       0 \n       1 \n       2 \n       Green \n       2.236068 \n     \n     \n       5 \n       -1 \n       0 \n       1 \n       Green \n       1.414214 \n     \n     \n       6 \n       1 \n       1 \n       1 \n       Red \n       1.732051",
            "title": "(a) Euclidian distance"
        },
        {
            "location": "/sols/chapter2/exercise7/#b-k1",
            "text": "df.sort_values(['distance'])   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       Y \n       distance \n     \n   \n   \n     \n       5 \n       -1 \n       0 \n       1 \n       Green \n       1.414214 \n     \n     \n       6 \n       1 \n       1 \n       1 \n       Red \n       1.732051 \n     \n     \n       2 \n       2 \n       0 \n       0 \n       Red \n       2.000000 \n     \n     \n       4 \n       0 \n       1 \n       2 \n       Green \n       2.236068 \n     \n     \n       1 \n       0 \n       3 \n       0 \n       Red \n       3.000000 \n     \n     \n       3 \n       0 \n       1 \n       3 \n       Red \n       3.162278 \n     \n      As we can see by sorting the data by distance to the origin, for K=1, our prediction is  Green , since that's the value of the nearest neighbor (point 5 at distance 1.41).",
            "title": "(b) K=1"
        },
        {
            "location": "/sols/chapter2/exercise7/#c-k3",
            "text": "On the other hand, for K=3 our prediction is  Red , because that's the mode of the 3 nearest neigbours: Green, Red and Red (points 5, 6 and 2, respectively).",
            "title": "(c) K=3"
        },
        {
            "location": "/sols/chapter2/exercise7/#d-highly-non-linear-bayes-decision-boundary",
            "text": "A large value of K leads to a smoother decision boundary, as if the non-linearities where averaged out. This happens because KNN uses majority voting and this means less emphasis on individual points. For a large value of K, we will likely have a decision boundary which varies very little from point to point, since the result of this majority voting would have to change while for most points this will be a large majority. That is, one of its nearest neighbors changing from one class to the other would still leave the majority voting the same. By contrast, when K is very small, the decision boundary will be better able to capture local non-linearities, because it the majority of neighbors can vary significantly from point to point since the are so few neighbors. Accordingly, we would expect  the best value of K to be small .   Imagine this simple example: a true linear boundary that splits the plane in two semi-planes (classes A and B), but with an additional enclave in one of the regions. That is, the true model has a small region of, say, class A inside the class B semi-plane. Would we more likely capture this region for small or large K? Say we have 3 neighboring data points in the class A enclave inside the large class B region with say 50 points. If K > 4, each of the 3 points in the enclave will be classified as B, since they always lose by majority voting (unless K is so large, say 100, that many of the points from A semi-plane region enter this voting). If, on the other hand, K < 4, each of the 3 class A points inside the enclave will be classified as class A, and we will capture this non-linearity of the regions.",
            "title": "(d) Highly non-linear Bayes decision boundary"
        },
        {
            "location": "/sols/chapter2/exercise8/",
            "text": "Exercise 2.8\n\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\npd.options.display.float_format = '{:,.2f}'.format # Print only 2 decimal cases.\n\n\n\n\n(a) Read csv\n\n\ncollege = pd.read_csv(\"../data/College.csv\") # Portable import, works on Windows as well.\ncollege\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nUnnamed: 0\n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nAbilene Christian University\n\n      \nYes\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.10\n\n      \n12\n\n      \n7041\n\n      \n60\n\n    \n\n    \n\n      \n1\n\n      \nAdelphi University\n\n      \nYes\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.20\n\n      \n16\n\n      \n10527\n\n      \n56\n\n    \n\n    \n\n      \n2\n\n      \nAdrian College\n\n      \nYes\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.90\n\n      \n30\n\n      \n8735\n\n      \n54\n\n    \n\n    \n\n      \n3\n\n      \nAgnes Scott College\n\n      \nYes\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.70\n\n      \n37\n\n      \n19016\n\n      \n59\n\n    \n\n    \n\n      \n4\n\n      \nAlaska Pacific University\n\n      \nYes\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.90\n\n      \n2\n\n      \n10922\n\n      \n15\n\n    \n\n    \n\n      \n5\n\n      \nAlbertson College\n\n      \nYes\n\n      \n587\n\n      \n479\n\n      \n158\n\n      \n38\n\n      \n62\n\n      \n678\n\n      \n41\n\n      \n13500\n\n      \n3335\n\n      \n500\n\n      \n675\n\n      \n67\n\n      \n73\n\n      \n9.40\n\n      \n11\n\n      \n9727\n\n      \n55\n\n    \n\n    \n\n      \n6\n\n      \nAlbertus Magnus College\n\n      \nYes\n\n      \n353\n\n      \n340\n\n      \n103\n\n      \n17\n\n      \n45\n\n      \n416\n\n      \n230\n\n      \n13290\n\n      \n5720\n\n      \n500\n\n      \n1500\n\n      \n90\n\n      \n93\n\n      \n11.50\n\n      \n26\n\n      \n8861\n\n      \n63\n\n    \n\n    \n\n      \n7\n\n      \nAlbion College\n\n      \nYes\n\n      \n1899\n\n      \n1720\n\n      \n489\n\n      \n37\n\n      \n68\n\n      \n1594\n\n      \n32\n\n      \n13868\n\n      \n4826\n\n      \n450\n\n      \n850\n\n      \n89\n\n      \n100\n\n      \n13.70\n\n      \n37\n\n      \n11487\n\n      \n73\n\n    \n\n    \n\n      \n8\n\n      \nAlbright College\n\n      \nYes\n\n      \n1038\n\n      \n839\n\n      \n227\n\n      \n30\n\n      \n63\n\n      \n973\n\n      \n306\n\n      \n15595\n\n      \n4400\n\n      \n300\n\n      \n500\n\n      \n79\n\n      \n84\n\n      \n11.30\n\n      \n23\n\n      \n11644\n\n      \n80\n\n    \n\n    \n\n      \n9\n\n      \nAlderson-Broaddus College\n\n      \nYes\n\n      \n582\n\n      \n498\n\n      \n172\n\n      \n21\n\n      \n44\n\n      \n799\n\n      \n78\n\n      \n10468\n\n      \n3380\n\n      \n660\n\n      \n1800\n\n      \n40\n\n      \n41\n\n      \n11.50\n\n      \n15\n\n      \n8991\n\n      \n52\n\n    \n\n    \n\n      \n10\n\n      \nAlfred University\n\n      \nYes\n\n      \n1732\n\n      \n1425\n\n      \n472\n\n      \n37\n\n      \n75\n\n      \n1830\n\n      \n110\n\n      \n16548\n\n      \n5406\n\n      \n500\n\n      \n600\n\n      \n82\n\n      \n88\n\n      \n11.30\n\n      \n31\n\n      \n10932\n\n      \n73\n\n    \n\n    \n\n      \n11\n\n      \nAllegheny College\n\n      \nYes\n\n      \n2652\n\n      \n1900\n\n      \n484\n\n      \n44\n\n      \n77\n\n      \n1707\n\n      \n44\n\n      \n17080\n\n      \n4440\n\n      \n400\n\n      \n600\n\n      \n73\n\n      \n91\n\n      \n9.90\n\n      \n41\n\n      \n11711\n\n      \n76\n\n    \n\n    \n\n      \n12\n\n      \nAllentown Coll. of St. Francis de Sales\n\n      \nYes\n\n      \n1179\n\n      \n780\n\n      \n290\n\n      \n38\n\n      \n64\n\n      \n1130\n\n      \n638\n\n      \n9690\n\n      \n4785\n\n      \n600\n\n      \n1000\n\n      \n60\n\n      \n84\n\n      \n13.30\n\n      \n21\n\n      \n7940\n\n      \n74\n\n    \n\n    \n\n      \n13\n\n      \nAlma College\n\n      \nYes\n\n      \n1267\n\n      \n1080\n\n      \n385\n\n      \n44\n\n      \n73\n\n      \n1306\n\n      \n28\n\n      \n12572\n\n      \n4552\n\n      \n400\n\n      \n400\n\n      \n79\n\n      \n87\n\n      \n15.30\n\n      \n32\n\n      \n9305\n\n      \n68\n\n    \n\n    \n\n      \n14\n\n      \nAlverno College\n\n      \nYes\n\n      \n494\n\n      \n313\n\n      \n157\n\n      \n23\n\n      \n46\n\n      \n1317\n\n      \n1235\n\n      \n8352\n\n      \n3640\n\n      \n650\n\n      \n2449\n\n      \n36\n\n      \n69\n\n      \n11.10\n\n      \n26\n\n      \n8127\n\n      \n55\n\n    \n\n    \n\n      \n15\n\n      \nAmerican International College\n\n      \nYes\n\n      \n1420\n\n      \n1093\n\n      \n220\n\n      \n9\n\n      \n22\n\n      \n1018\n\n      \n287\n\n      \n8700\n\n      \n4780\n\n      \n450\n\n      \n1400\n\n      \n78\n\n      \n84\n\n      \n14.70\n\n      \n19\n\n      \n7355\n\n      \n69\n\n    \n\n    \n\n      \n16\n\n      \nAmherst College\n\n      \nYes\n\n      \n4302\n\n      \n992\n\n      \n418\n\n      \n83\n\n      \n96\n\n      \n1593\n\n      \n5\n\n      \n19760\n\n      \n5300\n\n      \n660\n\n      \n1598\n\n      \n93\n\n      \n98\n\n      \n8.40\n\n      \n63\n\n      \n21424\n\n      \n100\n\n    \n\n    \n\n      \n17\n\n      \nAnderson University\n\n      \nYes\n\n      \n1216\n\n      \n908\n\n      \n423\n\n      \n19\n\n      \n40\n\n      \n1819\n\n      \n281\n\n      \n10100\n\n      \n3520\n\n      \n550\n\n      \n1100\n\n      \n48\n\n      \n61\n\n      \n12.10\n\n      \n14\n\n      \n7994\n\n      \n59\n\n    \n\n    \n\n      \n18\n\n      \nAndrews University\n\n      \nYes\n\n      \n1130\n\n      \n704\n\n      \n322\n\n      \n14\n\n      \n23\n\n      \n1586\n\n      \n326\n\n      \n9996\n\n      \n3090\n\n      \n900\n\n      \n1320\n\n      \n62\n\n      \n66\n\n      \n11.50\n\n      \n18\n\n      \n10908\n\n      \n46\n\n    \n\n    \n\n      \n19\n\n      \nAngelo State University\n\n      \nNo\n\n      \n3540\n\n      \n2001\n\n      \n1016\n\n      \n24\n\n      \n54\n\n      \n4190\n\n      \n1512\n\n      \n5130\n\n      \n3592\n\n      \n500\n\n      \n2000\n\n      \n60\n\n      \n62\n\n      \n23.10\n\n      \n5\n\n      \n4010\n\n      \n34\n\n    \n\n    \n\n      \n20\n\n      \nAntioch University\n\n      \nYes\n\n      \n713\n\n      \n661\n\n      \n252\n\n      \n25\n\n      \n44\n\n      \n712\n\n      \n23\n\n      \n15476\n\n      \n3336\n\n      \n400\n\n      \n1100\n\n      \n69\n\n      \n82\n\n      \n11.30\n\n      \n35\n\n      \n42926\n\n      \n48\n\n    \n\n    \n\n      \n21\n\n      \nAppalachian State University\n\n      \nNo\n\n      \n7313\n\n      \n4664\n\n      \n1910\n\n      \n20\n\n      \n63\n\n      \n9940\n\n      \n1035\n\n      \n6806\n\n      \n2540\n\n      \n96\n\n      \n2000\n\n      \n83\n\n      \n96\n\n      \n18.30\n\n      \n14\n\n      \n5854\n\n      \n70\n\n    \n\n    \n\n      \n22\n\n      \nAquinas College\n\n      \nYes\n\n      \n619\n\n      \n516\n\n      \n219\n\n      \n20\n\n      \n51\n\n      \n1251\n\n      \n767\n\n      \n11208\n\n      \n4124\n\n      \n350\n\n      \n1615\n\n      \n55\n\n      \n65\n\n      \n12.70\n\n      \n25\n\n      \n6584\n\n      \n65\n\n    \n\n    \n\n      \n23\n\n      \nArizona State University Main campus\n\n      \nNo\n\n      \n12809\n\n      \n10308\n\n      \n3761\n\n      \n24\n\n      \n49\n\n      \n22593\n\n      \n7585\n\n      \n7434\n\n      \n4850\n\n      \n700\n\n      \n2100\n\n      \n88\n\n      \n93\n\n      \n18.90\n\n      \n5\n\n      \n4602\n\n      \n48\n\n    \n\n    \n\n      \n24\n\n      \nArkansas College (Lyon College)\n\n      \nYes\n\n      \n708\n\n      \n334\n\n      \n166\n\n      \n46\n\n      \n74\n\n      \n530\n\n      \n182\n\n      \n8644\n\n      \n3922\n\n      \n500\n\n      \n800\n\n      \n79\n\n      \n88\n\n      \n12.60\n\n      \n24\n\n      \n14579\n\n      \n54\n\n    \n\n    \n\n      \n25\n\n      \nArkansas Tech University\n\n      \nNo\n\n      \n1734\n\n      \n1729\n\n      \n951\n\n      \n12\n\n      \n52\n\n      \n3602\n\n      \n939\n\n      \n3460\n\n      \n2650\n\n      \n450\n\n      \n1000\n\n      \n57\n\n      \n60\n\n      \n19.60\n\n      \n5\n\n      \n4739\n\n      \n48\n\n    \n\n    \n\n      \n26\n\n      \nAssumption College\n\n      \nYes\n\n      \n2135\n\n      \n1700\n\n      \n491\n\n      \n23\n\n      \n59\n\n      \n1708\n\n      \n689\n\n      \n12000\n\n      \n5920\n\n      \n500\n\n      \n500\n\n      \n93\n\n      \n93\n\n      \n13.80\n\n      \n30\n\n      \n7100\n\n      \n88\n\n    \n\n    \n\n      \n27\n\n      \nAuburn University-Main Campus\n\n      \nNo\n\n      \n7548\n\n      \n6791\n\n      \n3070\n\n      \n25\n\n      \n57\n\n      \n16262\n\n      \n1716\n\n      \n6300\n\n      \n3933\n\n      \n600\n\n      \n1908\n\n      \n85\n\n      \n91\n\n      \n16.70\n\n      \n18\n\n      \n6642\n\n      \n69\n\n    \n\n    \n\n      \n28\n\n      \nAugsburg College\n\n      \nYes\n\n      \n662\n\n      \n513\n\n      \n257\n\n      \n12\n\n      \n30\n\n      \n2074\n\n      \n726\n\n      \n11902\n\n      \n4372\n\n      \n540\n\n      \n950\n\n      \n65\n\n      \n65\n\n      \n12.80\n\n      \n31\n\n      \n7836\n\n      \n58\n\n    \n\n    \n\n      \n29\n\n      \nAugustana College IL\n\n      \nYes\n\n      \n1879\n\n      \n1658\n\n      \n497\n\n      \n36\n\n      \n69\n\n      \n1950\n\n      \n38\n\n      \n13353\n\n      \n4173\n\n      \n540\n\n      \n821\n\n      \n78\n\n      \n83\n\n      \n12.70\n\n      \n40\n\n      \n9220\n\n      \n71\n\n    \n\n    \n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n    \n\n    \n\n      \n747\n\n      \nWestfield State College\n\n      \nNo\n\n      \n3100\n\n      \n2150\n\n      \n825\n\n      \n3\n\n      \n20\n\n      \n3234\n\n      \n941\n\n      \n5542\n\n      \n3788\n\n      \n500\n\n      \n1300\n\n      \n75\n\n      \n79\n\n      \n15.70\n\n      \n20\n\n      \n4222\n\n      \n65\n\n    \n\n    \n\n      \n748\n\n      \nWestminster College MO\n\n      \nYes\n\n      \n662\n\n      \n553\n\n      \n184\n\n      \n20\n\n      \n43\n\n      \n665\n\n      \n37\n\n      \n10720\n\n      \n4050\n\n      \n600\n\n      \n1650\n\n      \n66\n\n      \n70\n\n      \n12.50\n\n      \n20\n\n      \n7925\n\n      \n62\n\n    \n\n    \n\n      \n749\n\n      \nWestminster College\n\n      \nYes\n\n      \n996\n\n      \n866\n\n      \n377\n\n      \n29\n\n      \n58\n\n      \n1411\n\n      \n72\n\n      \n12065\n\n      \n3615\n\n      \n430\n\n      \n685\n\n      \n62\n\n      \n78\n\n      \n12.50\n\n      \n41\n\n      \n8596\n\n      \n80\n\n    \n\n    \n\n      \n750\n\n      \nWestminster College of Salt Lake City\n\n      \nYes\n\n      \n917\n\n      \n720\n\n      \n213\n\n      \n21\n\n      \n60\n\n      \n979\n\n      \n743\n\n      \n8820\n\n      \n4050\n\n      \n600\n\n      \n2025\n\n      \n68\n\n      \n83\n\n      \n10.50\n\n      \n34\n\n      \n7170\n\n      \n50\n\n    \n\n    \n\n      \n751\n\n      \nWestmont College\n\n      \nNo\n\n      \n950\n\n      \n713\n\n      \n351\n\n      \n42\n\n      \n72\n\n      \n1276\n\n      \n9\n\n      \n14320\n\n      \n5304\n\n      \n490\n\n      \n1410\n\n      \n77\n\n      \n77\n\n      \n14.90\n\n      \n17\n\n      \n8837\n\n      \n87\n\n    \n\n    \n\n      \n752\n\n      \nWheaton College IL\n\n      \nYes\n\n      \n1432\n\n      \n920\n\n      \n548\n\n      \n56\n\n      \n84\n\n      \n2200\n\n      \n56\n\n      \n11480\n\n      \n4200\n\n      \n530\n\n      \n1400\n\n      \n81\n\n      \n83\n\n      \n12.70\n\n      \n40\n\n      \n11916\n\n      \n85\n\n    \n\n    \n\n      \n753\n\n      \nWestminster College PA\n\n      \nYes\n\n      \n1738\n\n      \n1373\n\n      \n417\n\n      \n21\n\n      \n55\n\n      \n1335\n\n      \n30\n\n      \n18460\n\n      \n5970\n\n      \n700\n\n      \n850\n\n      \n92\n\n      \n96\n\n      \n13.20\n\n      \n41\n\n      \n22704\n\n      \n71\n\n    \n\n    \n\n      \n754\n\n      \nWheeling Jesuit College\n\n      \nYes\n\n      \n903\n\n      \n755\n\n      \n213\n\n      \n15\n\n      \n49\n\n      \n971\n\n      \n305\n\n      \n10500\n\n      \n4545\n\n      \n600\n\n      \n600\n\n      \n66\n\n      \n71\n\n      \n14.10\n\n      \n27\n\n      \n7494\n\n      \n72\n\n    \n\n    \n\n      \n755\n\n      \nWhitman College\n\n      \nYes\n\n      \n1861\n\n      \n998\n\n      \n359\n\n      \n45\n\n      \n77\n\n      \n1220\n\n      \n46\n\n      \n16670\n\n      \n4900\n\n      \n750\n\n      \n800\n\n      \n80\n\n      \n83\n\n      \n10.50\n\n      \n51\n\n      \n13198\n\n      \n72\n\n    \n\n    \n\n      \n756\n\n      \nWhittier College\n\n      \nYes\n\n      \n1681\n\n      \n1069\n\n      \n344\n\n      \n35\n\n      \n63\n\n      \n1235\n\n      \n30\n\n      \n16249\n\n      \n5699\n\n      \n500\n\n      \n1998\n\n      \n84\n\n      \n92\n\n      \n13.60\n\n      \n29\n\n      \n11778\n\n      \n52\n\n    \n\n    \n\n      \n757\n\n      \nWhitworth College\n\n      \nYes\n\n      \n1121\n\n      \n926\n\n      \n372\n\n      \n43\n\n      \n70\n\n      \n1270\n\n      \n160\n\n      \n12660\n\n      \n4500\n\n      \n678\n\n      \n2424\n\n      \n80\n\n      \n80\n\n      \n16.90\n\n      \n20\n\n      \n8328\n\n      \n80\n\n    \n\n    \n\n      \n758\n\n      \nWidener University\n\n      \nYes\n\n      \n2139\n\n      \n1492\n\n      \n502\n\n      \n24\n\n      \n64\n\n      \n2186\n\n      \n2171\n\n      \n12350\n\n      \n5370\n\n      \n500\n\n      \n1350\n\n      \n88\n\n      \n86\n\n      \n12.60\n\n      \n19\n\n      \n9603\n\n      \n63\n\n    \n\n    \n\n      \n759\n\n      \nWilkes University\n\n      \nYes\n\n      \n1631\n\n      \n1431\n\n      \n434\n\n      \n15\n\n      \n36\n\n      \n1803\n\n      \n603\n\n      \n11150\n\n      \n5130\n\n      \n550\n\n      \n1260\n\n      \n78\n\n      \n92\n\n      \n13.30\n\n      \n24\n\n      \n8543\n\n      \n67\n\n    \n\n    \n\n      \n760\n\n      \nWillamette University\n\n      \nYes\n\n      \n1658\n\n      \n1327\n\n      \n395\n\n      \n49\n\n      \n80\n\n      \n1595\n\n      \n159\n\n      \n14800\n\n      \n4620\n\n      \n400\n\n      \n790\n\n      \n91\n\n      \n94\n\n      \n13.30\n\n      \n37\n\n      \n10779\n\n      \n68\n\n    \n\n    \n\n      \n761\n\n      \nWilliam Jewell College\n\n      \nYes\n\n      \n663\n\n      \n547\n\n      \n315\n\n      \n32\n\n      \n67\n\n      \n1279\n\n      \n75\n\n      \n10060\n\n      \n2970\n\n      \n500\n\n      \n2600\n\n      \n74\n\n      \n80\n\n      \n11.20\n\n      \n19\n\n      \n7885\n\n      \n59\n\n    \n\n    \n\n      \n762\n\n      \nWilliam Woods University\n\n      \nYes\n\n      \n469\n\n      \n435\n\n      \n227\n\n      \n17\n\n      \n39\n\n      \n851\n\n      \n120\n\n      \n10535\n\n      \n4365\n\n      \n550\n\n      \n3700\n\n      \n39\n\n      \n66\n\n      \n12.90\n\n      \n16\n\n      \n7438\n\n      \n52\n\n    \n\n    \n\n      \n763\n\n      \nWilliams College\n\n      \nYes\n\n      \n4186\n\n      \n1245\n\n      \n526\n\n      \n81\n\n      \n96\n\n      \n1988\n\n      \n29\n\n      \n19629\n\n      \n5790\n\n      \n500\n\n      \n1200\n\n      \n94\n\n      \n99\n\n      \n9.00\n\n      \n64\n\n      \n22014\n\n      \n99\n\n    \n\n    \n\n      \n764\n\n      \nWilson College\n\n      \nYes\n\n      \n167\n\n      \n130\n\n      \n46\n\n      \n16\n\n      \n50\n\n      \n199\n\n      \n676\n\n      \n11428\n\n      \n5084\n\n      \n450\n\n      \n475\n\n      \n67\n\n      \n76\n\n      \n8.30\n\n      \n43\n\n      \n10291\n\n      \n67\n\n    \n\n    \n\n      \n765\n\n      \nWingate College\n\n      \nYes\n\n      \n1239\n\n      \n1017\n\n      \n383\n\n      \n10\n\n      \n34\n\n      \n1207\n\n      \n157\n\n      \n7820\n\n      \n3400\n\n      \n550\n\n      \n1550\n\n      \n69\n\n      \n81\n\n      \n13.90\n\n      \n8\n\n      \n7264\n\n      \n91\n\n    \n\n    \n\n      \n766\n\n      \nWinona State University\n\n      \nNo\n\n      \n3325\n\n      \n2047\n\n      \n1301\n\n      \n20\n\n      \n45\n\n      \n5800\n\n      \n872\n\n      \n4200\n\n      \n2700\n\n      \n300\n\n      \n1200\n\n      \n53\n\n      \n60\n\n      \n20.20\n\n      \n18\n\n      \n5318\n\n      \n58\n\n    \n\n    \n\n      \n767\n\n      \nWinthrop University\n\n      \nNo\n\n      \n2320\n\n      \n1805\n\n      \n769\n\n      \n24\n\n      \n61\n\n      \n3395\n\n      \n670\n\n      \n6400\n\n      \n3392\n\n      \n580\n\n      \n2150\n\n      \n71\n\n      \n80\n\n      \n12.80\n\n      \n26\n\n      \n6729\n\n      \n59\n\n    \n\n    \n\n      \n768\n\n      \nWisconsin Lutheran College\n\n      \nYes\n\n      \n152\n\n      \n128\n\n      \n75\n\n      \n17\n\n      \n41\n\n      \n282\n\n      \n22\n\n      \n9100\n\n      \n3700\n\n      \n500\n\n      \n1400\n\n      \n48\n\n      \n48\n\n      \n8.50\n\n      \n26\n\n      \n8960\n\n      \n50\n\n    \n\n    \n\n      \n769\n\n      \nWittenberg University\n\n      \nYes\n\n      \n1979\n\n      \n1739\n\n      \n575\n\n      \n42\n\n      \n68\n\n      \n1980\n\n      \n144\n\n      \n15948\n\n      \n4404\n\n      \n400\n\n      \n800\n\n      \n82\n\n      \n95\n\n      \n12.80\n\n      \n29\n\n      \n10414\n\n      \n78\n\n    \n\n    \n\n      \n770\n\n      \nWofford College\n\n      \nYes\n\n      \n1501\n\n      \n935\n\n      \n273\n\n      \n51\n\n      \n83\n\n      \n1059\n\n      \n34\n\n      \n12680\n\n      \n4150\n\n      \n605\n\n      \n1440\n\n      \n91\n\n      \n92\n\n      \n15.30\n\n      \n42\n\n      \n7875\n\n      \n75\n\n    \n\n    \n\n      \n771\n\n      \nWorcester Polytechnic Institute\n\n      \nYes\n\n      \n2768\n\n      \n2314\n\n      \n682\n\n      \n49\n\n      \n86\n\n      \n2802\n\n      \n86\n\n      \n15884\n\n      \n5370\n\n      \n530\n\n      \n730\n\n      \n92\n\n      \n94\n\n      \n15.20\n\n      \n34\n\n      \n10774\n\n      \n82\n\n    \n\n    \n\n      \n772\n\n      \nWorcester State College\n\n      \nNo\n\n      \n2197\n\n      \n1515\n\n      \n543\n\n      \n4\n\n      \n26\n\n      \n3089\n\n      \n2029\n\n      \n6797\n\n      \n3900\n\n      \n500\n\n      \n1200\n\n      \n60\n\n      \n60\n\n      \n21.00\n\n      \n14\n\n      \n4469\n\n      \n40\n\n    \n\n    \n\n      \n773\n\n      \nXavier University\n\n      \nYes\n\n      \n1959\n\n      \n1805\n\n      \n695\n\n      \n24\n\n      \n47\n\n      \n2849\n\n      \n1107\n\n      \n11520\n\n      \n4960\n\n      \n600\n\n      \n1250\n\n      \n73\n\n      \n75\n\n      \n13.30\n\n      \n31\n\n      \n9189\n\n      \n83\n\n    \n\n    \n\n      \n774\n\n      \nXavier University of Louisiana\n\n      \nYes\n\n      \n2097\n\n      \n1915\n\n      \n695\n\n      \n34\n\n      \n61\n\n      \n2793\n\n      \n166\n\n      \n6900\n\n      \n4200\n\n      \n617\n\n      \n781\n\n      \n67\n\n      \n75\n\n      \n14.40\n\n      \n20\n\n      \n8323\n\n      \n49\n\n    \n\n    \n\n      \n775\n\n      \nYale University\n\n      \nYes\n\n      \n10705\n\n      \n2453\n\n      \n1317\n\n      \n95\n\n      \n99\n\n      \n5217\n\n      \n83\n\n      \n19840\n\n      \n6510\n\n      \n630\n\n      \n2115\n\n      \n96\n\n      \n96\n\n      \n5.80\n\n      \n49\n\n      \n40386\n\n      \n99\n\n    \n\n    \n\n      \n776\n\n      \nYork College of Pennsylvania\n\n      \nYes\n\n      \n2989\n\n      \n1855\n\n      \n691\n\n      \n28\n\n      \n63\n\n      \n2988\n\n      \n1726\n\n      \n4990\n\n      \n3560\n\n      \n500\n\n      \n1250\n\n      \n75\n\n      \n75\n\n      \n18.10\n\n      \n28\n\n      \n4509\n\n      \n99\n\n    \n\n  \n\n\n\n\n777 rows \u00d7 19 columns\n\n\n\n\n\n(b) University names as index\n\n\nThe fix() function in R (similar to edit()) allows on-the-fly edit to the dataframe by invoking an editor.\nFurther details can be found \nhere\n and \nhere\n.\n\n\n# [1]\ncollege = college.set_index(\"Unnamed: 0\") # The default option 'drop=True', deletes the column\ncollege.index.name = 'Names'\ncollege.head()\n# The empty row below the columns names (e.g. Private, Apps, etc.) is there because the index has a name and that creates an additional row.\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n    \n\n      \nNames\n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \nAbilene Christian University\n\n      \nYes\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.10\n\n      \n12\n\n      \n7041\n\n      \n60\n\n    \n\n    \n\n      \nAdelphi University\n\n      \nYes\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.20\n\n      \n16\n\n      \n10527\n\n      \n56\n\n    \n\n    \n\n      \nAdrian College\n\n      \nYes\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.90\n\n      \n30\n\n      \n8735\n\n      \n54\n\n    \n\n    \n\n      \nAgnes Scott College\n\n      \nYes\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.70\n\n      \n37\n\n      \n19016\n\n      \n59\n\n    \n\n    \n\n      \nAlaska Pacific University\n\n      \nYes\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.90\n\n      \n2\n\n      \n10922\n\n      \n15\n\n    \n\n  \n\n\n\n\n\n\n\n[1] \nhttps://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/advanced-indexing?ex=1\n \n\n\n# Alternative solution: We could have done this all in one less line with:\ncollege = pd.read_csv('../data/College.csv', index_col='Unnamed: 0')\ncollege.index.name = 'Names'\ncollege.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n    \n\n      \nNames\n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \nAbilene Christian University\n\n      \nYes\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.10\n\n      \n12\n\n      \n7041\n\n      \n60\n\n    \n\n    \n\n      \nAdelphi University\n\n      \nYes\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.20\n\n      \n16\n\n      \n10527\n\n      \n56\n\n    \n\n    \n\n      \nAdrian College\n\n      \nYes\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.90\n\n      \n30\n\n      \n8735\n\n      \n54\n\n    \n\n    \n\n      \nAgnes Scott College\n\n      \nYes\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.70\n\n      \n37\n\n      \n19016\n\n      \n59\n\n    \n\n    \n\n      \nAlaska Pacific University\n\n      \nYes\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.90\n\n      \n2\n\n      \n10922\n\n      \n15\n\n    \n\n  \n\n\n\n\n\n\n\n(c)\n\n\ni. Summary\n\n\ncollege.describe(include='all')\n# [2, 3, 4] Without the 'all' option, the column 'Private' is not shown because it is categorical\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n777\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n    \n\n    \n\n      \nunique\n\n      \n2\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n    \n\n    \n\n      \ntop\n\n      \nYes\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n    \n\n    \n\n      \nfreq\n\n      \n565\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n    \n\n    \n\n      \nmean\n\n      \nNaN\n\n      \n3,001.64\n\n      \n2,018.80\n\n      \n779.97\n\n      \n27.56\n\n      \n55.80\n\n      \n3,699.91\n\n      \n855.30\n\n      \n10,440.67\n\n      \n4,357.53\n\n      \n549.38\n\n      \n1,340.64\n\n      \n72.66\n\n      \n79.70\n\n      \n14.09\n\n      \n22.74\n\n      \n9,660.17\n\n      \n65.46\n\n    \n\n    \n\n      \nstd\n\n      \nNaN\n\n      \n3,870.20\n\n      \n2,451.11\n\n      \n929.18\n\n      \n17.64\n\n      \n19.80\n\n      \n4,850.42\n\n      \n1,522.43\n\n      \n4,023.02\n\n      \n1,096.70\n\n      \n165.11\n\n      \n677.07\n\n      \n16.33\n\n      \n14.72\n\n      \n3.96\n\n      \n12.39\n\n      \n5,221.77\n\n      \n17.18\n\n    \n\n    \n\n      \nmin\n\n      \nNaN\n\n      \n81.00\n\n      \n72.00\n\n      \n35.00\n\n      \n1.00\n\n      \n9.00\n\n      \n139.00\n\n      \n1.00\n\n      \n2,340.00\n\n      \n1,780.00\n\n      \n96.00\n\n      \n250.00\n\n      \n8.00\n\n      \n24.00\n\n      \n2.50\n\n      \n0.00\n\n      \n3,186.00\n\n      \n10.00\n\n    \n\n    \n\n      \n25%\n\n      \nNaN\n\n      \n776.00\n\n      \n604.00\n\n      \n242.00\n\n      \n15.00\n\n      \n41.00\n\n      \n992.00\n\n      \n95.00\n\n      \n7,320.00\n\n      \n3,597.00\n\n      \n470.00\n\n      \n850.00\n\n      \n62.00\n\n      \n71.00\n\n      \n11.50\n\n      \n13.00\n\n      \n6,751.00\n\n      \n53.00\n\n    \n\n    \n\n      \n50%\n\n      \nNaN\n\n      \n1,558.00\n\n      \n1,110.00\n\n      \n434.00\n\n      \n23.00\n\n      \n54.00\n\n      \n1,707.00\n\n      \n353.00\n\n      \n9,990.00\n\n      \n4,200.00\n\n      \n500.00\n\n      \n1,200.00\n\n      \n75.00\n\n      \n82.00\n\n      \n13.60\n\n      \n21.00\n\n      \n8,377.00\n\n      \n65.00\n\n    \n\n    \n\n      \n75%\n\n      \nNaN\n\n      \n3,624.00\n\n      \n2,424.00\n\n      \n902.00\n\n      \n35.00\n\n      \n69.00\n\n      \n4,005.00\n\n      \n967.00\n\n      \n12,925.00\n\n      \n5,050.00\n\n      \n600.00\n\n      \n1,700.00\n\n      \n85.00\n\n      \n92.00\n\n      \n16.50\n\n      \n31.00\n\n      \n10,830.00\n\n      \n78.00\n\n    \n\n    \n\n      \nmax\n\n      \nNaN\n\n      \n48,094.00\n\n      \n26,330.00\n\n      \n6,392.00\n\n      \n96.00\n\n      \n100.00\n\n      \n31,643.00\n\n      \n21,836.00\n\n      \n21,700.00\n\n      \n8,124.00\n\n      \n2,340.00\n\n      \n6,800.00\n\n      \n103.00\n\n      \n100.00\n\n      \n39.80\n\n      \n64.00\n\n      \n56,233.00\n\n      \n118.00\n\n    \n\n  \n\n\n\n\n\n\n\n# Alternative solution: call describe twice. One on number, and another on object.\ncollege.describe(include=['number'])\n# or college.describe(include=[np.number])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n    \n\n    \n\n      \nmean\n\n      \n3,001.64\n\n      \n2,018.80\n\n      \n779.97\n\n      \n27.56\n\n      \n55.80\n\n      \n3,699.91\n\n      \n855.30\n\n      \n10,440.67\n\n      \n4,357.53\n\n      \n549.38\n\n      \n1,340.64\n\n      \n72.66\n\n      \n79.70\n\n      \n14.09\n\n      \n22.74\n\n      \n9,660.17\n\n      \n65.46\n\n    \n\n    \n\n      \nstd\n\n      \n3,870.20\n\n      \n2,451.11\n\n      \n929.18\n\n      \n17.64\n\n      \n19.80\n\n      \n4,850.42\n\n      \n1,522.43\n\n      \n4,023.02\n\n      \n1,096.70\n\n      \n165.11\n\n      \n677.07\n\n      \n16.33\n\n      \n14.72\n\n      \n3.96\n\n      \n12.39\n\n      \n5,221.77\n\n      \n17.18\n\n    \n\n    \n\n      \nmin\n\n      \n81.00\n\n      \n72.00\n\n      \n35.00\n\n      \n1.00\n\n      \n9.00\n\n      \n139.00\n\n      \n1.00\n\n      \n2,340.00\n\n      \n1,780.00\n\n      \n96.00\n\n      \n250.00\n\n      \n8.00\n\n      \n24.00\n\n      \n2.50\n\n      \n0.00\n\n      \n3,186.00\n\n      \n10.00\n\n    \n\n    \n\n      \n25%\n\n      \n776.00\n\n      \n604.00\n\n      \n242.00\n\n      \n15.00\n\n      \n41.00\n\n      \n992.00\n\n      \n95.00\n\n      \n7,320.00\n\n      \n3,597.00\n\n      \n470.00\n\n      \n850.00\n\n      \n62.00\n\n      \n71.00\n\n      \n11.50\n\n      \n13.00\n\n      \n6,751.00\n\n      \n53.00\n\n    \n\n    \n\n      \n50%\n\n      \n1,558.00\n\n      \n1,110.00\n\n      \n434.00\n\n      \n23.00\n\n      \n54.00\n\n      \n1,707.00\n\n      \n353.00\n\n      \n9,990.00\n\n      \n4,200.00\n\n      \n500.00\n\n      \n1,200.00\n\n      \n75.00\n\n      \n82.00\n\n      \n13.60\n\n      \n21.00\n\n      \n8,377.00\n\n      \n65.00\n\n    \n\n    \n\n      \n75%\n\n      \n3,624.00\n\n      \n2,424.00\n\n      \n902.00\n\n      \n35.00\n\n      \n69.00\n\n      \n4,005.00\n\n      \n967.00\n\n      \n12,925.00\n\n      \n5,050.00\n\n      \n600.00\n\n      \n1,700.00\n\n      \n85.00\n\n      \n92.00\n\n      \n16.50\n\n      \n31.00\n\n      \n10,830.00\n\n      \n78.00\n\n    \n\n    \n\n      \nmax\n\n      \n48,094.00\n\n      \n26,330.00\n\n      \n6,392.00\n\n      \n96.00\n\n      \n100.00\n\n      \n31,643.00\n\n      \n21,836.00\n\n      \n21,700.00\n\n      \n8,124.00\n\n      \n2,340.00\n\n      \n6,800.00\n\n      \n103.00\n\n      \n100.00\n\n      \n39.80\n\n      \n64.00\n\n      \n56,233.00\n\n      \n118.00\n\n    \n\n  \n\n\n\n\n\n\n\ncollege.describe(include=['object'])\n# or college.describe(include=['O'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n777\n\n    \n\n    \n\n      \nunique\n\n      \n2\n\n    \n\n    \n\n      \ntop\n\n      \nYes\n\n    \n\n    \n\n      \nfreq\n\n      \n565\n\n    \n\n  \n\n\n\n\n\n\n\n\n\n[2] \nhttp://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns\n\n\n[3] \nhttp://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns\n\n\n[4] \nhttp://dataanalysispython.readthedocs.io/en/latest/pandas.html#summarizing-data-describe\n\n\n\n\nii. Pair plot\n\n\nUnlike R, seaborn does not pairplot categorical vs numerical. See more \nhere\n.\n\n\ng = sns.PairGrid(college, vars=college.iloc[:,1:11], hue='Private')\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)\n\n\n\n\n\n\niii. Box plots\n\n\nsns.boxplot(x='Private', y='Outstate', data=college);\n\n\n\n\n\n\niv. Elite variable\n\n\ncollege.loc[college['Top10perc']>50, 'Elite'] = 'Yes'\ncollege['Elite'] = college['Elite'].fillna('No')\n\nsns.boxplot(x='Elite', y='Outstate', data=college);\n\n\n\n\n\n\nv. Histograms\n\n\nIn Python, to produce some histograms with differing numbers of bins for quantitative variables, we first need to convert these variables to bins.\nWhen we create bins, we transform a continuous range of values into a discrete one. For the purposes of this exercise, we will only consider equal-width bins.\n\n\n# Bins creation\ncollege['PhD'] = pd.cut(college['PhD'], 3, labels=['Low', 'Medium', 'High'])\ncollege['Grad.Rate'] = pd.cut(college['Grad.Rate'], 5, labels=['Very low', 'Low', 'Medium', 'High', 'Very high'])\ncollege['Books'] = pd.cut(college['Books'], 2, labels=['Low', 'High'])\ncollege['Enroll'] = pd.cut(college['Enroll'], 4, labels=['Very low', 'Low', 'High', 'Very high'])\n\n\n\n\n# Plot histograms\nfig = plt.figure()\n\nplt.subplot(221)\ncollege['PhD'].value_counts().plot(kind='bar', title = 'Private');\nplt.subplot(222)\ncollege['Grad.Rate'].value_counts().plot(kind='bar', title = 'Grad.Rate');\nplt.subplot(223)\ncollege['Books'].value_counts().plot(kind='bar', title = 'Books');\nplt.subplot(224)\ncollege['Enroll'].value_counts().plot(kind='bar', title = 'Enroll');\n\nfig.subplots_adjust(hspace=1) # To add space between subplots\n\n\n\n\n\n\nvi. Continue exploring the data\n\n\n\"This exercise is \ntrivial\n and is left to the reader.\" :)",
            "title": "2.8"
        },
        {
            "location": "/sols/chapter2/exercise8/#exercise-28",
            "text": "%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\npd.options.display.float_format = '{:,.2f}'.format # Print only 2 decimal cases.",
            "title": "Exercise 2.8"
        },
        {
            "location": "/sols/chapter2/exercise8/#a-read-csv",
            "text": "college = pd.read_csv(\"../data/College.csv\") # Portable import, works on Windows as well.\ncollege   \n   \n     \n       \n       Unnamed: 0 \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n   \n   \n     \n       0 \n       Abilene Christian University \n       Yes \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.10 \n       12 \n       7041 \n       60 \n     \n     \n       1 \n       Adelphi University \n       Yes \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.20 \n       16 \n       10527 \n       56 \n     \n     \n       2 \n       Adrian College \n       Yes \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.90 \n       30 \n       8735 \n       54 \n     \n     \n       3 \n       Agnes Scott College \n       Yes \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.70 \n       37 \n       19016 \n       59 \n     \n     \n       4 \n       Alaska Pacific University \n       Yes \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.90 \n       2 \n       10922 \n       15 \n     \n     \n       5 \n       Albertson College \n       Yes \n       587 \n       479 \n       158 \n       38 \n       62 \n       678 \n       41 \n       13500 \n       3335 \n       500 \n       675 \n       67 \n       73 \n       9.40 \n       11 \n       9727 \n       55 \n     \n     \n       6 \n       Albertus Magnus College \n       Yes \n       353 \n       340 \n       103 \n       17 \n       45 \n       416 \n       230 \n       13290 \n       5720 \n       500 \n       1500 \n       90 \n       93 \n       11.50 \n       26 \n       8861 \n       63 \n     \n     \n       7 \n       Albion College \n       Yes \n       1899 \n       1720 \n       489 \n       37 \n       68 \n       1594 \n       32 \n       13868 \n       4826 \n       450 \n       850 \n       89 \n       100 \n       13.70 \n       37 \n       11487 \n       73 \n     \n     \n       8 \n       Albright College \n       Yes \n       1038 \n       839 \n       227 \n       30 \n       63 \n       973 \n       306 \n       15595 \n       4400 \n       300 \n       500 \n       79 \n       84 \n       11.30 \n       23 \n       11644 \n       80 \n     \n     \n       9 \n       Alderson-Broaddus College \n       Yes \n       582 \n       498 \n       172 \n       21 \n       44 \n       799 \n       78 \n       10468 \n       3380 \n       660 \n       1800 \n       40 \n       41 \n       11.50 \n       15 \n       8991 \n       52 \n     \n     \n       10 \n       Alfred University \n       Yes \n       1732 \n       1425 \n       472 \n       37 \n       75 \n       1830 \n       110 \n       16548 \n       5406 \n       500 \n       600 \n       82 \n       88 \n       11.30 \n       31 \n       10932 \n       73 \n     \n     \n       11 \n       Allegheny College \n       Yes \n       2652 \n       1900 \n       484 \n       44 \n       77 \n       1707 \n       44 \n       17080 \n       4440 \n       400 \n       600 \n       73 \n       91 \n       9.90 \n       41 \n       11711 \n       76 \n     \n     \n       12 \n       Allentown Coll. of St. Francis de Sales \n       Yes \n       1179 \n       780 \n       290 \n       38 \n       64 \n       1130 \n       638 \n       9690 \n       4785 \n       600 \n       1000 \n       60 \n       84 \n       13.30 \n       21 \n       7940 \n       74 \n     \n     \n       13 \n       Alma College \n       Yes \n       1267 \n       1080 \n       385 \n       44 \n       73 \n       1306 \n       28 \n       12572 \n       4552 \n       400 \n       400 \n       79 \n       87 \n       15.30 \n       32 \n       9305 \n       68 \n     \n     \n       14 \n       Alverno College \n       Yes \n       494 \n       313 \n       157 \n       23 \n       46 \n       1317 \n       1235 \n       8352 \n       3640 \n       650 \n       2449 \n       36 \n       69 \n       11.10 \n       26 \n       8127 \n       55 \n     \n     \n       15 \n       American International College \n       Yes \n       1420 \n       1093 \n       220 \n       9 \n       22 \n       1018 \n       287 \n       8700 \n       4780 \n       450 \n       1400 \n       78 \n       84 \n       14.70 \n       19 \n       7355 \n       69 \n     \n     \n       16 \n       Amherst College \n       Yes \n       4302 \n       992 \n       418 \n       83 \n       96 \n       1593 \n       5 \n       19760 \n       5300 \n       660 \n       1598 \n       93 \n       98 \n       8.40 \n       63 \n       21424 \n       100 \n     \n     \n       17 \n       Anderson University \n       Yes \n       1216 \n       908 \n       423 \n       19 \n       40 \n       1819 \n       281 \n       10100 \n       3520 \n       550 \n       1100 \n       48 \n       61 \n       12.10 \n       14 \n       7994 \n       59 \n     \n     \n       18 \n       Andrews University \n       Yes \n       1130 \n       704 \n       322 \n       14 \n       23 \n       1586 \n       326 \n       9996 \n       3090 \n       900 \n       1320 \n       62 \n       66 \n       11.50 \n       18 \n       10908 \n       46 \n     \n     \n       19 \n       Angelo State University \n       No \n       3540 \n       2001 \n       1016 \n       24 \n       54 \n       4190 \n       1512 \n       5130 \n       3592 \n       500 \n       2000 \n       60 \n       62 \n       23.10 \n       5 \n       4010 \n       34 \n     \n     \n       20 \n       Antioch University \n       Yes \n       713 \n       661 \n       252 \n       25 \n       44 \n       712 \n       23 \n       15476 \n       3336 \n       400 \n       1100 \n       69 \n       82 \n       11.30 \n       35 \n       42926 \n       48 \n     \n     \n       21 \n       Appalachian State University \n       No \n       7313 \n       4664 \n       1910 \n       20 \n       63 \n       9940 \n       1035 \n       6806 \n       2540 \n       96 \n       2000 \n       83 \n       96 \n       18.30 \n       14 \n       5854 \n       70 \n     \n     \n       22 \n       Aquinas College \n       Yes \n       619 \n       516 \n       219 \n       20 \n       51 \n       1251 \n       767 \n       11208 \n       4124 \n       350 \n       1615 \n       55 \n       65 \n       12.70 \n       25 \n       6584 \n       65 \n     \n     \n       23 \n       Arizona State University Main campus \n       No \n       12809 \n       10308 \n       3761 \n       24 \n       49 \n       22593 \n       7585 \n       7434 \n       4850 \n       700 \n       2100 \n       88 \n       93 \n       18.90 \n       5 \n       4602 \n       48 \n     \n     \n       24 \n       Arkansas College (Lyon College) \n       Yes \n       708 \n       334 \n       166 \n       46 \n       74 \n       530 \n       182 \n       8644 \n       3922 \n       500 \n       800 \n       79 \n       88 \n       12.60 \n       24 \n       14579 \n       54 \n     \n     \n       25 \n       Arkansas Tech University \n       No \n       1734 \n       1729 \n       951 \n       12 \n       52 \n       3602 \n       939 \n       3460 \n       2650 \n       450 \n       1000 \n       57 \n       60 \n       19.60 \n       5 \n       4739 \n       48 \n     \n     \n       26 \n       Assumption College \n       Yes \n       2135 \n       1700 \n       491 \n       23 \n       59 \n       1708 \n       689 \n       12000 \n       5920 \n       500 \n       500 \n       93 \n       93 \n       13.80 \n       30 \n       7100 \n       88 \n     \n     \n       27 \n       Auburn University-Main Campus \n       No \n       7548 \n       6791 \n       3070 \n       25 \n       57 \n       16262 \n       1716 \n       6300 \n       3933 \n       600 \n       1908 \n       85 \n       91 \n       16.70 \n       18 \n       6642 \n       69 \n     \n     \n       28 \n       Augsburg College \n       Yes \n       662 \n       513 \n       257 \n       12 \n       30 \n       2074 \n       726 \n       11902 \n       4372 \n       540 \n       950 \n       65 \n       65 \n       12.80 \n       31 \n       7836 \n       58 \n     \n     \n       29 \n       Augustana College IL \n       Yes \n       1879 \n       1658 \n       497 \n       36 \n       69 \n       1950 \n       38 \n       13353 \n       4173 \n       540 \n       821 \n       78 \n       83 \n       12.70 \n       40 \n       9220 \n       71 \n     \n     \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n     \n     \n       747 \n       Westfield State College \n       No \n       3100 \n       2150 \n       825 \n       3 \n       20 \n       3234 \n       941 \n       5542 \n       3788 \n       500 \n       1300 \n       75 \n       79 \n       15.70 \n       20 \n       4222 \n       65 \n     \n     \n       748 \n       Westminster College MO \n       Yes \n       662 \n       553 \n       184 \n       20 \n       43 \n       665 \n       37 \n       10720 \n       4050 \n       600 \n       1650 \n       66 \n       70 \n       12.50 \n       20 \n       7925 \n       62 \n     \n     \n       749 \n       Westminster College \n       Yes \n       996 \n       866 \n       377 \n       29 \n       58 \n       1411 \n       72 \n       12065 \n       3615 \n       430 \n       685 \n       62 \n       78 \n       12.50 \n       41 \n       8596 \n       80 \n     \n     \n       750 \n       Westminster College of Salt Lake City \n       Yes \n       917 \n       720 \n       213 \n       21 \n       60 \n       979 \n       743 \n       8820 \n       4050 \n       600 \n       2025 \n       68 \n       83 \n       10.50 \n       34 \n       7170 \n       50 \n     \n     \n       751 \n       Westmont College \n       No \n       950 \n       713 \n       351 \n       42 \n       72 \n       1276 \n       9 \n       14320 \n       5304 \n       490 \n       1410 \n       77 \n       77 \n       14.90 \n       17 \n       8837 \n       87 \n     \n     \n       752 \n       Wheaton College IL \n       Yes \n       1432 \n       920 \n       548 \n       56 \n       84 \n       2200 \n       56 \n       11480 \n       4200 \n       530 \n       1400 \n       81 \n       83 \n       12.70 \n       40 \n       11916 \n       85 \n     \n     \n       753 \n       Westminster College PA \n       Yes \n       1738 \n       1373 \n       417 \n       21 \n       55 \n       1335 \n       30 \n       18460 \n       5970 \n       700 \n       850 \n       92 \n       96 \n       13.20 \n       41 \n       22704 \n       71 \n     \n     \n       754 \n       Wheeling Jesuit College \n       Yes \n       903 \n       755 \n       213 \n       15 \n       49 \n       971 \n       305 \n       10500 \n       4545 \n       600 \n       600 \n       66 \n       71 \n       14.10 \n       27 \n       7494 \n       72 \n     \n     \n       755 \n       Whitman College \n       Yes \n       1861 \n       998 \n       359 \n       45 \n       77 \n       1220 \n       46 \n       16670 \n       4900 \n       750 \n       800 \n       80 \n       83 \n       10.50 \n       51 \n       13198 \n       72 \n     \n     \n       756 \n       Whittier College \n       Yes \n       1681 \n       1069 \n       344 \n       35 \n       63 \n       1235 \n       30 \n       16249 \n       5699 \n       500 \n       1998 \n       84 \n       92 \n       13.60 \n       29 \n       11778 \n       52 \n     \n     \n       757 \n       Whitworth College \n       Yes \n       1121 \n       926 \n       372 \n       43 \n       70 \n       1270 \n       160 \n       12660 \n       4500 \n       678 \n       2424 \n       80 \n       80 \n       16.90 \n       20 \n       8328 \n       80 \n     \n     \n       758 \n       Widener University \n       Yes \n       2139 \n       1492 \n       502 \n       24 \n       64 \n       2186 \n       2171 \n       12350 \n       5370 \n       500 \n       1350 \n       88 \n       86 \n       12.60 \n       19 \n       9603 \n       63 \n     \n     \n       759 \n       Wilkes University \n       Yes \n       1631 \n       1431 \n       434 \n       15 \n       36 \n       1803 \n       603 \n       11150 \n       5130 \n       550 \n       1260 \n       78 \n       92 \n       13.30 \n       24 \n       8543 \n       67 \n     \n     \n       760 \n       Willamette University \n       Yes \n       1658 \n       1327 \n       395 \n       49 \n       80 \n       1595 \n       159 \n       14800 \n       4620 \n       400 \n       790 \n       91 \n       94 \n       13.30 \n       37 \n       10779 \n       68 \n     \n     \n       761 \n       William Jewell College \n       Yes \n       663 \n       547 \n       315 \n       32 \n       67 \n       1279 \n       75 \n       10060 \n       2970 \n       500 \n       2600 \n       74 \n       80 \n       11.20 \n       19 \n       7885 \n       59 \n     \n     \n       762 \n       William Woods University \n       Yes \n       469 \n       435 \n       227 \n       17 \n       39 \n       851 \n       120 \n       10535 \n       4365 \n       550 \n       3700 \n       39 \n       66 \n       12.90 \n       16 \n       7438 \n       52 \n     \n     \n       763 \n       Williams College \n       Yes \n       4186 \n       1245 \n       526 \n       81 \n       96 \n       1988 \n       29 \n       19629 \n       5790 \n       500 \n       1200 \n       94 \n       99 \n       9.00 \n       64 \n       22014 \n       99 \n     \n     \n       764 \n       Wilson College \n       Yes \n       167 \n       130 \n       46 \n       16 \n       50 \n       199 \n       676 \n       11428 \n       5084 \n       450 \n       475 \n       67 \n       76 \n       8.30 \n       43 \n       10291 \n       67 \n     \n     \n       765 \n       Wingate College \n       Yes \n       1239 \n       1017 \n       383 \n       10 \n       34 \n       1207 \n       157 \n       7820 \n       3400 \n       550 \n       1550 \n       69 \n       81 \n       13.90 \n       8 \n       7264 \n       91 \n     \n     \n       766 \n       Winona State University \n       No \n       3325 \n       2047 \n       1301 \n       20 \n       45 \n       5800 \n       872 \n       4200 \n       2700 \n       300 \n       1200 \n       53 \n       60 \n       20.20 \n       18 \n       5318 \n       58 \n     \n     \n       767 \n       Winthrop University \n       No \n       2320 \n       1805 \n       769 \n       24 \n       61 \n       3395 \n       670 \n       6400 \n       3392 \n       580 \n       2150 \n       71 \n       80 \n       12.80 \n       26 \n       6729 \n       59 \n     \n     \n       768 \n       Wisconsin Lutheran College \n       Yes \n       152 \n       128 \n       75 \n       17 \n       41 \n       282 \n       22 \n       9100 \n       3700 \n       500 \n       1400 \n       48 \n       48 \n       8.50 \n       26 \n       8960 \n       50 \n     \n     \n       769 \n       Wittenberg University \n       Yes \n       1979 \n       1739 \n       575 \n       42 \n       68 \n       1980 \n       144 \n       15948 \n       4404 \n       400 \n       800 \n       82 \n       95 \n       12.80 \n       29 \n       10414 \n       78 \n     \n     \n       770 \n       Wofford College \n       Yes \n       1501 \n       935 \n       273 \n       51 \n       83 \n       1059 \n       34 \n       12680 \n       4150 \n       605 \n       1440 \n       91 \n       92 \n       15.30 \n       42 \n       7875 \n       75 \n     \n     \n       771 \n       Worcester Polytechnic Institute \n       Yes \n       2768 \n       2314 \n       682 \n       49 \n       86 \n       2802 \n       86 \n       15884 \n       5370 \n       530 \n       730 \n       92 \n       94 \n       15.20 \n       34 \n       10774 \n       82 \n     \n     \n       772 \n       Worcester State College \n       No \n       2197 \n       1515 \n       543 \n       4 \n       26 \n       3089 \n       2029 \n       6797 \n       3900 \n       500 \n       1200 \n       60 \n       60 \n       21.00 \n       14 \n       4469 \n       40 \n     \n     \n       773 \n       Xavier University \n       Yes \n       1959 \n       1805 \n       695 \n       24 \n       47 \n       2849 \n       1107 \n       11520 \n       4960 \n       600 \n       1250 \n       73 \n       75 \n       13.30 \n       31 \n       9189 \n       83 \n     \n     \n       774 \n       Xavier University of Louisiana \n       Yes \n       2097 \n       1915 \n       695 \n       34 \n       61 \n       2793 \n       166 \n       6900 \n       4200 \n       617 \n       781 \n       67 \n       75 \n       14.40 \n       20 \n       8323 \n       49 \n     \n     \n       775 \n       Yale University \n       Yes \n       10705 \n       2453 \n       1317 \n       95 \n       99 \n       5217 \n       83 \n       19840 \n       6510 \n       630 \n       2115 \n       96 \n       96 \n       5.80 \n       49 \n       40386 \n       99 \n     \n     \n       776 \n       York College of Pennsylvania \n       Yes \n       2989 \n       1855 \n       691 \n       28 \n       63 \n       2988 \n       1726 \n       4990 \n       3560 \n       500 \n       1250 \n       75 \n       75 \n       18.10 \n       28 \n       4509 \n       99 \n     \n     777 rows \u00d7 19 columns",
            "title": "(a) Read csv"
        },
        {
            "location": "/sols/chapter2/exercise8/#b-university-names-as-index",
            "text": "The fix() function in R (similar to edit()) allows on-the-fly edit to the dataframe by invoking an editor.\nFurther details can be found  here  and  here .  # [1]\ncollege = college.set_index(\"Unnamed: 0\") # The default option 'drop=True', deletes the column\ncollege.index.name = 'Names'\ncollege.head()\n# The empty row below the columns names (e.g. Private, Apps, etc.) is there because the index has a name and that creates an additional row.   \n   \n     \n       \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n     \n       Names \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n     \n   \n   \n     \n       Abilene Christian University \n       Yes \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.10 \n       12 \n       7041 \n       60 \n     \n     \n       Adelphi University \n       Yes \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.20 \n       16 \n       10527 \n       56 \n     \n     \n       Adrian College \n       Yes \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.90 \n       30 \n       8735 \n       54 \n     \n     \n       Agnes Scott College \n       Yes \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.70 \n       37 \n       19016 \n       59 \n     \n     \n       Alaska Pacific University \n       Yes \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.90 \n       2 \n       10922 \n       15 \n     \n      [1]  https://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/advanced-indexing?ex=1    # Alternative solution: We could have done this all in one less line with:\ncollege = pd.read_csv('../data/College.csv', index_col='Unnamed: 0')\ncollege.index.name = 'Names'\ncollege.head()   \n   \n     \n       \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n     \n       Names \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n     \n   \n   \n     \n       Abilene Christian University \n       Yes \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.10 \n       12 \n       7041 \n       60 \n     \n     \n       Adelphi University \n       Yes \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.20 \n       16 \n       10527 \n       56 \n     \n     \n       Adrian College \n       Yes \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.90 \n       30 \n       8735 \n       54 \n     \n     \n       Agnes Scott College \n       Yes \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.70 \n       37 \n       19016 \n       59 \n     \n     \n       Alaska Pacific University \n       Yes \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.90 \n       2 \n       10922 \n       15",
            "title": "(b) University names as index"
        },
        {
            "location": "/sols/chapter2/exercise8/#c",
            "text": "",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter2/exercise8/#i-summary",
            "text": "college.describe(include='all')\n# [2, 3, 4] Without the 'all' option, the column 'Private' is not shown because it is categorical   \n   \n     \n       \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n   \n   \n     \n       count \n       777 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n     \n     \n       unique \n       2 \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n     \n     \n       top \n       Yes \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n     \n     \n       freq \n       565 \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n     \n     \n       mean \n       NaN \n       3,001.64 \n       2,018.80 \n       779.97 \n       27.56 \n       55.80 \n       3,699.91 \n       855.30 \n       10,440.67 \n       4,357.53 \n       549.38 \n       1,340.64 \n       72.66 \n       79.70 \n       14.09 \n       22.74 \n       9,660.17 \n       65.46 \n     \n     \n       std \n       NaN \n       3,870.20 \n       2,451.11 \n       929.18 \n       17.64 \n       19.80 \n       4,850.42 \n       1,522.43 \n       4,023.02 \n       1,096.70 \n       165.11 \n       677.07 \n       16.33 \n       14.72 \n       3.96 \n       12.39 \n       5,221.77 \n       17.18 \n     \n     \n       min \n       NaN \n       81.00 \n       72.00 \n       35.00 \n       1.00 \n       9.00 \n       139.00 \n       1.00 \n       2,340.00 \n       1,780.00 \n       96.00 \n       250.00 \n       8.00 \n       24.00 \n       2.50 \n       0.00 \n       3,186.00 \n       10.00 \n     \n     \n       25% \n       NaN \n       776.00 \n       604.00 \n       242.00 \n       15.00 \n       41.00 \n       992.00 \n       95.00 \n       7,320.00 \n       3,597.00 \n       470.00 \n       850.00 \n       62.00 \n       71.00 \n       11.50 \n       13.00 \n       6,751.00 \n       53.00 \n     \n     \n       50% \n       NaN \n       1,558.00 \n       1,110.00 \n       434.00 \n       23.00 \n       54.00 \n       1,707.00 \n       353.00 \n       9,990.00 \n       4,200.00 \n       500.00 \n       1,200.00 \n       75.00 \n       82.00 \n       13.60 \n       21.00 \n       8,377.00 \n       65.00 \n     \n     \n       75% \n       NaN \n       3,624.00 \n       2,424.00 \n       902.00 \n       35.00 \n       69.00 \n       4,005.00 \n       967.00 \n       12,925.00 \n       5,050.00 \n       600.00 \n       1,700.00 \n       85.00 \n       92.00 \n       16.50 \n       31.00 \n       10,830.00 \n       78.00 \n     \n     \n       max \n       NaN \n       48,094.00 \n       26,330.00 \n       6,392.00 \n       96.00 \n       100.00 \n       31,643.00 \n       21,836.00 \n       21,700.00 \n       8,124.00 \n       2,340.00 \n       6,800.00 \n       103.00 \n       100.00 \n       39.80 \n       64.00 \n       56,233.00 \n       118.00 \n     \n      # Alternative solution: call describe twice. One on number, and another on object.\ncollege.describe(include=['number'])\n# or college.describe(include=[np.number])   \n   \n     \n       \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n   \n   \n     \n       count \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n     \n     \n       mean \n       3,001.64 \n       2,018.80 \n       779.97 \n       27.56 \n       55.80 \n       3,699.91 \n       855.30 \n       10,440.67 \n       4,357.53 \n       549.38 \n       1,340.64 \n       72.66 \n       79.70 \n       14.09 \n       22.74 \n       9,660.17 \n       65.46 \n     \n     \n       std \n       3,870.20 \n       2,451.11 \n       929.18 \n       17.64 \n       19.80 \n       4,850.42 \n       1,522.43 \n       4,023.02 \n       1,096.70 \n       165.11 \n       677.07 \n       16.33 \n       14.72 \n       3.96 \n       12.39 \n       5,221.77 \n       17.18 \n     \n     \n       min \n       81.00 \n       72.00 \n       35.00 \n       1.00 \n       9.00 \n       139.00 \n       1.00 \n       2,340.00 \n       1,780.00 \n       96.00 \n       250.00 \n       8.00 \n       24.00 \n       2.50 \n       0.00 \n       3,186.00 \n       10.00 \n     \n     \n       25% \n       776.00 \n       604.00 \n       242.00 \n       15.00 \n       41.00 \n       992.00 \n       95.00 \n       7,320.00 \n       3,597.00 \n       470.00 \n       850.00 \n       62.00 \n       71.00 \n       11.50 \n       13.00 \n       6,751.00 \n       53.00 \n     \n     \n       50% \n       1,558.00 \n       1,110.00 \n       434.00 \n       23.00 \n       54.00 \n       1,707.00 \n       353.00 \n       9,990.00 \n       4,200.00 \n       500.00 \n       1,200.00 \n       75.00 \n       82.00 \n       13.60 \n       21.00 \n       8,377.00 \n       65.00 \n     \n     \n       75% \n       3,624.00 \n       2,424.00 \n       902.00 \n       35.00 \n       69.00 \n       4,005.00 \n       967.00 \n       12,925.00 \n       5,050.00 \n       600.00 \n       1,700.00 \n       85.00 \n       92.00 \n       16.50 \n       31.00 \n       10,830.00 \n       78.00 \n     \n     \n       max \n       48,094.00 \n       26,330.00 \n       6,392.00 \n       96.00 \n       100.00 \n       31,643.00 \n       21,836.00 \n       21,700.00 \n       8,124.00 \n       2,340.00 \n       6,800.00 \n       103.00 \n       100.00 \n       39.80 \n       64.00 \n       56,233.00 \n       118.00 \n     \n      college.describe(include=['object'])\n# or college.describe(include=['O'])   \n   \n     \n       \n       Private \n     \n   \n   \n     \n       count \n       777 \n     \n     \n       unique \n       2 \n     \n     \n       top \n       Yes \n     \n     \n       freq \n       565 \n     \n       [2]  http://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns  [3]  http://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns  [4]  http://dataanalysispython.readthedocs.io/en/latest/pandas.html#summarizing-data-describe",
            "title": "i. Summary"
        },
        {
            "location": "/sols/chapter2/exercise8/#ii-pair-plot",
            "text": "Unlike R, seaborn does not pairplot categorical vs numerical. See more  here .  g = sns.PairGrid(college, vars=college.iloc[:,1:11], hue='Private')\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)",
            "title": "ii. Pair plot"
        },
        {
            "location": "/sols/chapter2/exercise8/#iii-box-plots",
            "text": "sns.boxplot(x='Private', y='Outstate', data=college);",
            "title": "iii. Box plots"
        },
        {
            "location": "/sols/chapter2/exercise8/#iv-elite-variable",
            "text": "college.loc[college['Top10perc']>50, 'Elite'] = 'Yes'\ncollege['Elite'] = college['Elite'].fillna('No')\n\nsns.boxplot(x='Elite', y='Outstate', data=college);",
            "title": "iv. Elite variable"
        },
        {
            "location": "/sols/chapter2/exercise8/#v-histograms",
            "text": "In Python, to produce some histograms with differing numbers of bins for quantitative variables, we first need to convert these variables to bins.\nWhen we create bins, we transform a continuous range of values into a discrete one. For the purposes of this exercise, we will only consider equal-width bins.  # Bins creation\ncollege['PhD'] = pd.cut(college['PhD'], 3, labels=['Low', 'Medium', 'High'])\ncollege['Grad.Rate'] = pd.cut(college['Grad.Rate'], 5, labels=['Very low', 'Low', 'Medium', 'High', 'Very high'])\ncollege['Books'] = pd.cut(college['Books'], 2, labels=['Low', 'High'])\ncollege['Enroll'] = pd.cut(college['Enroll'], 4, labels=['Very low', 'Low', 'High', 'Very high'])  # Plot histograms\nfig = plt.figure()\n\nplt.subplot(221)\ncollege['PhD'].value_counts().plot(kind='bar', title = 'Private');\nplt.subplot(222)\ncollege['Grad.Rate'].value_counts().plot(kind='bar', title = 'Grad.Rate');\nplt.subplot(223)\ncollege['Books'].value_counts().plot(kind='bar', title = 'Books');\nplt.subplot(224)\ncollege['Enroll'].value_counts().plot(kind='bar', title = 'Enroll');\n\nfig.subplots_adjust(hspace=1) # To add space between subplots",
            "title": "v. Histograms"
        },
        {
            "location": "/sols/chapter2/exercise8/#vi-continue-exploring-the-data",
            "text": "\"This exercise is  trivial  and is left to the reader.\" :)",
            "title": "vi. Continue exploring the data"
        },
        {
            "location": "/sols/chapter2/exercise9/",
            "text": "%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.options.display.float_format = '{:,.2f}'.format # Print only 2 decimal cases.\n\n\n\n\ndf = pd.read_csv('../data/auto.csv')\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.00\n\n      \n8\n\n      \n307.00\n\n      \n130\n\n      \n3504\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.00\n\n      \n8\n\n      \n350.00\n\n      \n165\n\n      \n3693\n\n      \n11.50\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.00\n\n      \n8\n\n      \n318.00\n\n      \n150\n\n      \n3436\n\n      \n11.00\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.00\n\n      \n8\n\n      \n304.00\n\n      \n150\n\n      \n3433\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.00\n\n      \n8\n\n      \n302.00\n\n      \n140\n\n      \n3449\n\n      \n10.50\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n    \n\n      \n5\n\n      \n15.00\n\n      \n8\n\n      \n429.00\n\n      \n198\n\n      \n4341\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \nford galaxie 500\n\n    \n\n    \n\n      \n6\n\n      \n14.00\n\n      \n8\n\n      \n454.00\n\n      \n220\n\n      \n4354\n\n      \n9.00\n\n      \n70\n\n      \n1\n\n      \nchevrolet impala\n\n    \n\n    \n\n      \n7\n\n      \n14.00\n\n      \n8\n\n      \n440.00\n\n      \n215\n\n      \n4312\n\n      \n8.50\n\n      \n70\n\n      \n1\n\n      \nplymouth fury iii\n\n    \n\n    \n\n      \n8\n\n      \n14.00\n\n      \n8\n\n      \n455.00\n\n      \n225\n\n      \n4425\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \npontiac catalina\n\n    \n\n    \n\n      \n9\n\n      \n15.00\n\n      \n8\n\n      \n390.00\n\n      \n190\n\n      \n3850\n\n      \n8.50\n\n      \n70\n\n      \n1\n\n      \namc ambassador dpl\n\n    \n\n    \n\n      \n10\n\n      \n15.00\n\n      \n8\n\n      \n383.00\n\n      \n170\n\n      \n3563\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \ndodge challenger se\n\n    \n\n    \n\n      \n11\n\n      \n14.00\n\n      \n8\n\n      \n340.00\n\n      \n160\n\n      \n3609\n\n      \n8.00\n\n      \n70\n\n      \n1\n\n      \nplymouth 'cuda 340\n\n    \n\n    \n\n      \n12\n\n      \n15.00\n\n      \n8\n\n      \n400.00\n\n      \n150\n\n      \n3761\n\n      \n9.50\n\n      \n70\n\n      \n1\n\n      \nchevrolet monte carlo\n\n    \n\n    \n\n      \n13\n\n      \n14.00\n\n      \n8\n\n      \n455.00\n\n      \n225\n\n      \n3086\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \nbuick estate wagon (sw)\n\n    \n\n    \n\n      \n14\n\n      \n24.00\n\n      \n4\n\n      \n113.00\n\n      \n95\n\n      \n2372\n\n      \n15.00\n\n      \n70\n\n      \n3\n\n      \ntoyota corona mark ii\n\n    \n\n    \n\n      \n15\n\n      \n22.00\n\n      \n6\n\n      \n198.00\n\n      \n95\n\n      \n2833\n\n      \n15.50\n\n      \n70\n\n      \n1\n\n      \nplymouth duster\n\n    \n\n    \n\n      \n16\n\n      \n18.00\n\n      \n6\n\n      \n199.00\n\n      \n97\n\n      \n2774\n\n      \n15.50\n\n      \n70\n\n      \n1\n\n      \namc hornet\n\n    \n\n    \n\n      \n17\n\n      \n21.00\n\n      \n6\n\n      \n200.00\n\n      \n85\n\n      \n2587\n\n      \n16.00\n\n      \n70\n\n      \n1\n\n      \nford maverick\n\n    \n\n    \n\n      \n18\n\n      \n27.00\n\n      \n4\n\n      \n97.00\n\n      \n88\n\n      \n2130\n\n      \n14.50\n\n      \n70\n\n      \n3\n\n      \ndatsun pl510\n\n    \n\n    \n\n      \n19\n\n      \n26.00\n\n      \n4\n\n      \n97.00\n\n      \n46\n\n      \n1835\n\n      \n20.50\n\n      \n70\n\n      \n2\n\n      \nvolkswagen 1131 deluxe sedan\n\n    \n\n    \n\n      \n20\n\n      \n25.00\n\n      \n4\n\n      \n110.00\n\n      \n87\n\n      \n2672\n\n      \n17.50\n\n      \n70\n\n      \n2\n\n      \npeugeot 504\n\n    \n\n    \n\n      \n21\n\n      \n24.00\n\n      \n4\n\n      \n107.00\n\n      \n90\n\n      \n2430\n\n      \n14.50\n\n      \n70\n\n      \n2\n\n      \naudi 100 ls\n\n    \n\n    \n\n      \n22\n\n      \n25.00\n\n      \n4\n\n      \n104.00\n\n      \n95\n\n      \n2375\n\n      \n17.50\n\n      \n70\n\n      \n2\n\n      \nsaab 99e\n\n    \n\n    \n\n      \n23\n\n      \n26.00\n\n      \n4\n\n      \n121.00\n\n      \n113\n\n      \n2234\n\n      \n12.50\n\n      \n70\n\n      \n2\n\n      \nbmw 2002\n\n    \n\n    \n\n      \n24\n\n      \n21.00\n\n      \n6\n\n      \n199.00\n\n      \n90\n\n      \n2648\n\n      \n15.00\n\n      \n70\n\n      \n1\n\n      \namc gremlin\n\n    \n\n    \n\n      \n25\n\n      \n10.00\n\n      \n8\n\n      \n360.00\n\n      \n215\n\n      \n4615\n\n      \n14.00\n\n      \n70\n\n      \n1\n\n      \nford f250\n\n    \n\n    \n\n      \n26\n\n      \n10.00\n\n      \n8\n\n      \n307.00\n\n      \n200\n\n      \n4376\n\n      \n15.00\n\n      \n70\n\n      \n1\n\n      \nchevy c20\n\n    \n\n    \n\n      \n27\n\n      \n11.00\n\n      \n8\n\n      \n318.00\n\n      \n210\n\n      \n4382\n\n      \n13.50\n\n      \n70\n\n      \n1\n\n      \ndodge d200\n\n    \n\n    \n\n      \n28\n\n      \n9.00\n\n      \n8\n\n      \n304.00\n\n      \n193\n\n      \n4732\n\n      \n18.50\n\n      \n70\n\n      \n1\n\n      \nhi 1200d\n\n    \n\n    \n\n      \n29\n\n      \n27.00\n\n      \n4\n\n      \n97.00\n\n      \n88\n\n      \n2130\n\n      \n14.50\n\n      \n71\n\n      \n3\n\n      \ndatsun pl510\n\n    \n\n    \n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n    \n\n    \n\n      \n367\n\n      \n28.00\n\n      \n4\n\n      \n112.00\n\n      \n88\n\n      \n2605\n\n      \n19.60\n\n      \n82\n\n      \n1\n\n      \nchevrolet cavalier\n\n    \n\n    \n\n      \n368\n\n      \n27.00\n\n      \n4\n\n      \n112.00\n\n      \n88\n\n      \n2640\n\n      \n18.60\n\n      \n82\n\n      \n1\n\n      \nchevrolet cavalier wagon\n\n    \n\n    \n\n      \n369\n\n      \n34.00\n\n      \n4\n\n      \n112.00\n\n      \n88\n\n      \n2395\n\n      \n18.00\n\n      \n82\n\n      \n1\n\n      \nchevrolet cavalier 2-door\n\n    \n\n    \n\n      \n370\n\n      \n31.00\n\n      \n4\n\n      \n112.00\n\n      \n85\n\n      \n2575\n\n      \n16.20\n\n      \n82\n\n      \n1\n\n      \npontiac j2000 se hatchback\n\n    \n\n    \n\n      \n371\n\n      \n29.00\n\n      \n4\n\n      \n135.00\n\n      \n84\n\n      \n2525\n\n      \n16.00\n\n      \n82\n\n      \n1\n\n      \ndodge aries se\n\n    \n\n    \n\n      \n372\n\n      \n27.00\n\n      \n4\n\n      \n151.00\n\n      \n90\n\n      \n2735\n\n      \n18.00\n\n      \n82\n\n      \n1\n\n      \npontiac phoenix\n\n    \n\n    \n\n      \n373\n\n      \n24.00\n\n      \n4\n\n      \n140.00\n\n      \n92\n\n      \n2865\n\n      \n16.40\n\n      \n82\n\n      \n1\n\n      \nford fairmont futura\n\n    \n\n    \n\n      \n374\n\n      \n36.00\n\n      \n4\n\n      \n105.00\n\n      \n74\n\n      \n1980\n\n      \n15.30\n\n      \n82\n\n      \n2\n\n      \nvolkswagen rabbit l\n\n    \n\n    \n\n      \n375\n\n      \n37.00\n\n      \n4\n\n      \n91.00\n\n      \n68\n\n      \n2025\n\n      \n18.20\n\n      \n82\n\n      \n3\n\n      \nmazda glc custom l\n\n    \n\n    \n\n      \n376\n\n      \n31.00\n\n      \n4\n\n      \n91.00\n\n      \n68\n\n      \n1970\n\n      \n17.60\n\n      \n82\n\n      \n3\n\n      \nmazda glc custom\n\n    \n\n    \n\n      \n377\n\n      \n38.00\n\n      \n4\n\n      \n105.00\n\n      \n63\n\n      \n2125\n\n      \n14.70\n\n      \n82\n\n      \n1\n\n      \nplymouth horizon miser\n\n    \n\n    \n\n      \n378\n\n      \n36.00\n\n      \n4\n\n      \n98.00\n\n      \n70\n\n      \n2125\n\n      \n17.30\n\n      \n82\n\n      \n1\n\n      \nmercury lynx l\n\n    \n\n    \n\n      \n379\n\n      \n36.00\n\n      \n4\n\n      \n120.00\n\n      \n88\n\n      \n2160\n\n      \n14.50\n\n      \n82\n\n      \n3\n\n      \nnissan stanza xe\n\n    \n\n    \n\n      \n380\n\n      \n36.00\n\n      \n4\n\n      \n107.00\n\n      \n75\n\n      \n2205\n\n      \n14.50\n\n      \n82\n\n      \n3\n\n      \nhonda accord\n\n    \n\n    \n\n      \n381\n\n      \n34.00\n\n      \n4\n\n      \n108.00\n\n      \n70\n\n      \n2245\n\n      \n16.90\n\n      \n82\n\n      \n3\n\n      \ntoyota corolla\n\n    \n\n    \n\n      \n382\n\n      \n38.00\n\n      \n4\n\n      \n91.00\n\n      \n67\n\n      \n1965\n\n      \n15.00\n\n      \n82\n\n      \n3\n\n      \nhonda civic\n\n    \n\n    \n\n      \n383\n\n      \n32.00\n\n      \n4\n\n      \n91.00\n\n      \n67\n\n      \n1965\n\n      \n15.70\n\n      \n82\n\n      \n3\n\n      \nhonda civic (auto)\n\n    \n\n    \n\n      \n384\n\n      \n38.00\n\n      \n4\n\n      \n91.00\n\n      \n67\n\n      \n1995\n\n      \n16.20\n\n      \n82\n\n      \n3\n\n      \ndatsun 310 gx\n\n    \n\n    \n\n      \n385\n\n      \n25.00\n\n      \n6\n\n      \n181.00\n\n      \n110\n\n      \n2945\n\n      \n16.40\n\n      \n82\n\n      \n1\n\n      \nbuick century limited\n\n    \n\n    \n\n      \n386\n\n      \n38.00\n\n      \n6\n\n      \n262.00\n\n      \n85\n\n      \n3015\n\n      \n17.00\n\n      \n82\n\n      \n1\n\n      \noldsmobile cutlass ciera (diesel)\n\n    \n\n    \n\n      \n387\n\n      \n26.00\n\n      \n4\n\n      \n156.00\n\n      \n92\n\n      \n2585\n\n      \n14.50\n\n      \n82\n\n      \n1\n\n      \nchrysler lebaron medallion\n\n    \n\n    \n\n      \n388\n\n      \n22.00\n\n      \n6\n\n      \n232.00\n\n      \n112\n\n      \n2835\n\n      \n14.70\n\n      \n82\n\n      \n1\n\n      \nford granada l\n\n    \n\n    \n\n      \n389\n\n      \n32.00\n\n      \n4\n\n      \n144.00\n\n      \n96\n\n      \n2665\n\n      \n13.90\n\n      \n82\n\n      \n3\n\n      \ntoyota celica gt\n\n    \n\n    \n\n      \n390\n\n      \n36.00\n\n      \n4\n\n      \n135.00\n\n      \n84\n\n      \n2370\n\n      \n13.00\n\n      \n82\n\n      \n1\n\n      \ndodge charger 2.2\n\n    \n\n    \n\n      \n391\n\n      \n27.00\n\n      \n4\n\n      \n151.00\n\n      \n90\n\n      \n2950\n\n      \n17.30\n\n      \n82\n\n      \n1\n\n      \nchevrolet camaro\n\n    \n\n    \n\n      \n392\n\n      \n27.00\n\n      \n4\n\n      \n140.00\n\n      \n86\n\n      \n2790\n\n      \n15.60\n\n      \n82\n\n      \n1\n\n      \nford mustang gl\n\n    \n\n    \n\n      \n393\n\n      \n44.00\n\n      \n4\n\n      \n97.00\n\n      \n52\n\n      \n2130\n\n      \n24.60\n\n      \n82\n\n      \n2\n\n      \nvw pickup\n\n    \n\n    \n\n      \n394\n\n      \n32.00\n\n      \n4\n\n      \n135.00\n\n      \n84\n\n      \n2295\n\n      \n11.60\n\n      \n82\n\n      \n1\n\n      \ndodge rampage\n\n    \n\n    \n\n      \n395\n\n      \n28.00\n\n      \n4\n\n      \n120.00\n\n      \n79\n\n      \n2625\n\n      \n18.60\n\n      \n82\n\n      \n1\n\n      \nford ranger\n\n    \n\n    \n\n      \n396\n\n      \n31.00\n\n      \n4\n\n      \n119.00\n\n      \n82\n\n      \n2720\n\n      \n19.40\n\n      \n82\n\n      \n1\n\n      \nchevy s-10\n\n    \n\n  \n\n\n\n\n397 rows \u00d7 9 columns\n\n\n\n\n\nLooks good so far, no missing values in sight.\n\n\ndf.info()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 397 entries, 0 to 396\nData columns (total 9 columns):\nmpg             397 non-null float64\ncylinders       397 non-null int64\ndisplacement    397 non-null float64\nhorsepower      397 non-null object\nweight          397 non-null int64\nacceleration    397 non-null float64\nyear            397 non-null int64\norigin          397 non-null int64\nname            397 non-null object\ndtypes: float64(3), int64(4), object(2)\nmemory usage: 28.0+ KB\n\n\n\nIt seems suspicious that 'horsepower' is of 'object' type. Let's have a closer look.\n\n\ndf.horsepower.unique()\n\n\n\n\narray(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)\n\n\n\nOk, so there are some missing values represented by a \nquestion mark\n.\n\n\ndf = df[df.horsepower != '?'].copy() # [1]\ndf['horsepower']=pd.to_numeric(df['horsepower'])\n\n\n\n\n[1]\n\n\ndf.info()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 392 entries, 0 to 396\nData columns (total 9 columns):\nmpg             392 non-null float64\ncylinders       392 non-null int64\ndisplacement    392 non-null float64\nhorsepower      392 non-null int64\nweight          392 non-null int64\nacceleration    392 non-null float64\nyear            392 non-null int64\norigin          392 non-null int64\nname            392 non-null object\ndtypes: float64(3), int64(5), object(1)\nmemory usage: 30.6+ KB\n\n\n\na) Quantitative and qualitative predictors\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.00\n\n      \n8\n\n      \n307.00\n\n      \n130\n\n      \n3504\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.00\n\n      \n8\n\n      \n350.00\n\n      \n165\n\n      \n3693\n\n      \n11.50\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.00\n\n      \n8\n\n      \n318.00\n\n      \n150\n\n      \n3436\n\n      \n11.00\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.00\n\n      \n8\n\n      \n304.00\n\n      \n150\n\n      \n3433\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.00\n\n      \n8\n\n      \n302.00\n\n      \n140\n\n      \n3449\n\n      \n10.50\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n  \n\n\n\n\n\n\n\nQuantitative predictors:\n\n\nquantitative = df.select_dtypes(include=['number']).columns\nquantitative\n\n\n\n\nIndex(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin'],\n      dtype='object')\n\n\n\nQualitative predictors:\n\n\nqualitative = df.select_dtypes(exclude=['number']).columns\nqualitative\n\n\n\n\nIndex(['name'], dtype='object')\n\n\n\nb) Range of each quantitative predictor\n\n\na = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc['range']\n\n\n\n\nmpg               37.60\ncylinders          5.00\ndisplacement     387.00\nhorsepower       184.00\nweight         3,527.00\nacceleration      16.80\nyear              12.00\norigin             2.00\nName: range, dtype: float64\n\n\n\nc) Mean and standard deviation\n\n\na.loc[['mean','std', 'range']]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n    \n\n  \n\n  \n\n    \n\n      \nmean\n\n      \n23.45\n\n      \n5.47\n\n      \n194.41\n\n      \n104.47\n\n      \n2,977.58\n\n      \n15.54\n\n      \n75.98\n\n      \n1.58\n\n    \n\n    \n\n      \nstd\n\n      \n7.81\n\n      \n1.71\n\n      \n104.64\n\n      \n38.49\n\n      \n849.40\n\n      \n2.76\n\n      \n3.68\n\n      \n0.81\n\n    \n\n    \n\n      \nrange\n\n      \n37.60\n\n      \n5.00\n\n      \n387.00\n\n      \n184.00\n\n      \n3,527.00\n\n      \n16.80\n\n      \n12.00\n\n      \n2.00\n\n    \n\n  \n\n\n\n\n\n\n\nd) Mean and standard deviation, removing observations\n\n\ndf_b = df.drop(df.index[10:85])\nb = df_b.describe()\nb.loc['range'] = b.loc['max'] - b.loc['min']\nb.loc[['mean','std', 'range']]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n    \n\n  \n\n  \n\n    \n\n      \nmean\n\n      \n24.37\n\n      \n5.38\n\n      \n187.88\n\n      \n101.00\n\n      \n2,938.85\n\n      \n15.70\n\n      \n77.12\n\n      \n1.60\n\n    \n\n    \n\n      \nstd\n\n      \n7.87\n\n      \n1.66\n\n      \n100.17\n\n      \n36.00\n\n      \n811.64\n\n      \n2.72\n\n      \n3.13\n\n      \n0.82\n\n    \n\n    \n\n      \nrange\n\n      \n35.60\n\n      \n5.00\n\n      \n387.00\n\n      \n184.00\n\n      \n3,348.00\n\n      \n16.30\n\n      \n12.00\n\n      \n2.00\n\n    \n\n  \n\n\n\n\n\n\n\ne) Visualizing relationships between variables\n\n\nWe use some common visualization tools, namely:\n\n\n\n\nScatterplots\n\n\nBox plots\n\n\nHistograms\n\n\n\n\ng = sns.PairGrid(df, size=2)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.fig.set_size_inches(12, 12)\n\n\n\n\n\n\n\n\nThe histogram for 'acceleration' resembles a normal distribution.\n\n\n'displacement' and 'weight' have a strong linear relationship.\n\n\n'mpg' has a non-linear relationship with 'weight', 'horsepower' and 'displacement'.\n\n\n\n\nf) Predicting mpg\n\n\nBased on the previous question, we could use weight, horsepower and displacement. As seen in the scatterplots, these variables seem to have a non-linear relationship with mpg. Are these relationships statistically significant? Exercises \n3.8\n and \n3.9\n delve further into this matter.",
            "title": "2.9"
        },
        {
            "location": "/sols/chapter2/exercise9/#a-quantitative-and-qualitative-predictors",
            "text": "df.head()   \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n     \n   \n   \n     \n       0 \n       18.00 \n       8 \n       307.00 \n       130 \n       3504 \n       12.00 \n       70 \n       1 \n       chevrolet chevelle malibu \n     \n     \n       1 \n       15.00 \n       8 \n       350.00 \n       165 \n       3693 \n       11.50 \n       70 \n       1 \n       buick skylark 320 \n     \n     \n       2 \n       18.00 \n       8 \n       318.00 \n       150 \n       3436 \n       11.00 \n       70 \n       1 \n       plymouth satellite \n     \n     \n       3 \n       16.00 \n       8 \n       304.00 \n       150 \n       3433 \n       12.00 \n       70 \n       1 \n       amc rebel sst \n     \n     \n       4 \n       17.00 \n       8 \n       302.00 \n       140 \n       3449 \n       10.50 \n       70 \n       1 \n       ford torino \n     \n      Quantitative predictors:  quantitative = df.select_dtypes(include=['number']).columns\nquantitative  Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin'],\n      dtype='object')  Qualitative predictors:  qualitative = df.select_dtypes(exclude=['number']).columns\nqualitative  Index(['name'], dtype='object')",
            "title": "a) Quantitative and qualitative predictors"
        },
        {
            "location": "/sols/chapter2/exercise9/#b-range-of-each-quantitative-predictor",
            "text": "a = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc['range']  mpg               37.60\ncylinders          5.00\ndisplacement     387.00\nhorsepower       184.00\nweight         3,527.00\nacceleration      16.80\nyear              12.00\norigin             2.00\nName: range, dtype: float64",
            "title": "b) Range of each quantitative predictor"
        },
        {
            "location": "/sols/chapter2/exercise9/#c-mean-and-standard-deviation",
            "text": "a.loc[['mean','std', 'range']]   \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n     \n   \n   \n     \n       mean \n       23.45 \n       5.47 \n       194.41 \n       104.47 \n       2,977.58 \n       15.54 \n       75.98 \n       1.58 \n     \n     \n       std \n       7.81 \n       1.71 \n       104.64 \n       38.49 \n       849.40 \n       2.76 \n       3.68 \n       0.81 \n     \n     \n       range \n       37.60 \n       5.00 \n       387.00 \n       184.00 \n       3,527.00 \n       16.80 \n       12.00 \n       2.00",
            "title": "c) Mean and standard deviation"
        },
        {
            "location": "/sols/chapter2/exercise9/#d-mean-and-standard-deviation-removing-observations",
            "text": "df_b = df.drop(df.index[10:85])\nb = df_b.describe()\nb.loc['range'] = b.loc['max'] - b.loc['min']\nb.loc[['mean','std', 'range']]   \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n     \n   \n   \n     \n       mean \n       24.37 \n       5.38 \n       187.88 \n       101.00 \n       2,938.85 \n       15.70 \n       77.12 \n       1.60 \n     \n     \n       std \n       7.87 \n       1.66 \n       100.17 \n       36.00 \n       811.64 \n       2.72 \n       3.13 \n       0.82 \n     \n     \n       range \n       35.60 \n       5.00 \n       387.00 \n       184.00 \n       3,348.00 \n       16.30 \n       12.00 \n       2.00",
            "title": "d) Mean and standard deviation, removing observations"
        },
        {
            "location": "/sols/chapter2/exercise9/#e-visualizing-relationships-between-variables",
            "text": "We use some common visualization tools, namely:   Scatterplots  Box plots  Histograms   g = sns.PairGrid(df, size=2)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.fig.set_size_inches(12, 12)    The histogram for 'acceleration' resembles a normal distribution.  'displacement' and 'weight' have a strong linear relationship.  'mpg' has a non-linear relationship with 'weight', 'horsepower' and 'displacement'.",
            "title": "e) Visualizing relationships between variables"
        },
        {
            "location": "/sols/chapter2/exercise9/#f-predicting-mpg",
            "text": "Based on the previous question, we could use weight, horsepower and displacement. As seen in the scatterplots, these variables seem to have a non-linear relationship with mpg. Are these relationships statistically significant? Exercises  3.8  and  3.9  delve further into this matter.",
            "title": "f) Predicting mpg"
        },
        {
            "location": "/sols/chapter2/exercise10/",
            "text": "Exercise 2.10\n\n\n%matplotlib inline\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\na) Dataset overview\n\n\nIn Python we can load the Boston dataset using scikit-learn.\n\n\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['target'] = boston.target\n\n\n\n\nprint(boston['DESCR'])\n\n\n\n\nBoston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:\n\n    :Number of Instances: 506\n\n    :Number of Attributes: 13 numeric/categorical predictive\n\n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.\n\n**References**\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n\n\nnp.shape(df)\n\n\n\n\n(506, 14)\n\n\n\nNumber of rows and columns\n\n\n 506 rows.\n\n 14 columns.\n\n\n Rows and columns description \n\n\n Each rows is town in Boston area.\n\n Columns are features that can influence house price such as per capita crime rate by town ('CRIM').\n\n\nb) Scatterplots\n\n\n\n\nWe'll use seaborn to get a quick overview of pairwise relationships\n\n\n\n\ng = sns.PairGrid(df)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)\n\n\n\n\n\n\nplt.scatter(df['RM'], df['target'])\nplt.xlabel('RM')\nplt.ylabel('target');\n\n\n\n\n\n\n Findings \n\n\n It seems to exist a positive linear relationship between RM and target\n\n This is expected as RM is the number of rooms (more space, higher price)\n\n\nplt.scatter(df['LSTAT'], df['target'])\nplt.xlabel('LSTAT')\nplt.ylabel('target');\n\n\n\n\n\n\n Findings \n\n\n LSTAT and target seem to have a negative non-linear relationship\n\n This is expected as LSTAT is the percent of lower status people (lower status, lower incomes, cheaper houses)\n\n\nplt.scatter(df['RM'], df['LSTAT'])\nplt.xlabel('RM')\nplt.ylabel('LSTAT');\n\n\n\n\n\n\n Findings \n\n\n It seems to exist a negative non-linear relationship between LSTAT and RM\n\n It makes sense since people with less money (higher LSTAT) can't afford bigger houses (high RM)\n\n\nc) Predictors associated with capita crime rate\n\n\ndf.corrwith(df['CRIM']).sort_values()\n\n\n\n\ntarget    -0.385832\nDIS       -0.377904\nB         -0.377365\nRM        -0.219940\nZN        -0.199458\nCHAS      -0.055295\nPTRATIO    0.288250\nAGE        0.350784\nINDUS      0.404471\nNOX        0.417521\nLSTAT      0.452220\nTAX        0.579564\nRAD        0.622029\nCRIM       1.000000\ndtype: float64\n\n\n\nLooking at the previous scatterplots and the correlation of each variable with 'CRIM', we will have a closer at the 3 with the largest correlation, namely:\n\n RAD, index of accessibility to radial highways,\n\n TAX, full-value property-tax rate (in dollars per $10,000),\n* LSTAT, percentage of lower status of the population.\n\n\nax = sns.boxplot(x=\"RAD\", y=\"CRIM\", data=df)\n\n\n\n\n\n\n Findings \n\n* When RAD is equal to 24 (its highest value), average CRIM is much higher and CRIM range is much larger.\n\n\nplt.scatter(df['TAX'], df['CRIM'])\nplt.xlabel('TAX')\nplt.ylabel('CRIM');\n\n\n\n\n\n\n Findings \n\n* When TAX is equal to \n666\n, average CRIM is much higher and CRIM range is much larger.\n\n\nplt.scatter(df['LSTAT'], df['CRIM'])\nplt.xlabel('LSTAT')\nplt.ylabel('CRIM');\n\n\n\n\n\n\n Findings \n\n\n For lower values of LSTAT (< 10), CRIM is always under 10. For LSTAT > 10, there is a wider spread of CRIM.\n\n For LSTAT < 20, a large proportion of the data points is very close to CRIM = 0.\n\n\nd) Crime rate, tax rate and pupil-teacher ratio in suburbs\n\n\ndf.ix[df['CRIM'].nlargest(5).index]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n380\n\n      \n88.9762\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.671\n\n      \n6.968\n\n      \n91.9\n\n      \n1.4165\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n396.90\n\n      \n17.21\n\n      \n10.4\n\n    \n\n    \n\n      \n418\n\n      \n73.5341\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.679\n\n      \n5.957\n\n      \n100.0\n\n      \n1.8026\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n16.45\n\n      \n20.62\n\n      \n8.8\n\n    \n\n    \n\n      \n405\n\n      \n67.9208\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.693\n\n      \n5.683\n\n      \n100.0\n\n      \n1.4254\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n384.97\n\n      \n22.98\n\n      \n5.0\n\n    \n\n    \n\n      \n410\n\n      \n51.1358\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.597\n\n      \n5.757\n\n      \n100.0\n\n      \n1.4130\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n2.60\n\n      \n10.11\n\n      \n15.0\n\n    \n\n    \n\n      \n414\n\n      \n45.7461\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.693\n\n      \n4.519\n\n      \n100.0\n\n      \n1.6582\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n88.27\n\n      \n36.98\n\n      \n7.0\n\n    \n\n  \n\n\n\n\n\n\n\ndf.ix[df['TAX'].nlargest(5).index]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n488\n\n      \n0.15086\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.454\n\n      \n92.7\n\n      \n1.8209\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n395.09\n\n      \n18.06\n\n      \n15.2\n\n    \n\n    \n\n      \n489\n\n      \n0.18337\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.414\n\n      \n98.3\n\n      \n1.7554\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n344.05\n\n      \n23.97\n\n      \n7.0\n\n    \n\n    \n\n      \n490\n\n      \n0.20746\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.093\n\n      \n98.0\n\n      \n1.8226\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n318.43\n\n      \n29.68\n\n      \n8.1\n\n    \n\n    \n\n      \n491\n\n      \n0.10574\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.983\n\n      \n98.8\n\n      \n1.8681\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n390.11\n\n      \n18.07\n\n      \n13.6\n\n    \n\n    \n\n      \n492\n\n      \n0.11132\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.983\n\n      \n83.5\n\n      \n2.1099\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n396.90\n\n      \n13.35\n\n      \n20.1\n\n    \n\n  \n\n\n\n\n\n\n\ndf.ix[df['PTRATIO'].nlargest(5).index]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n354\n\n      \n0.04301\n\n      \n80.0\n\n      \n1.91\n\n      \n0.0\n\n      \n0.413\n\n      \n5.663\n\n      \n21.9\n\n      \n10.5857\n\n      \n4.0\n\n      \n334.0\n\n      \n22.0\n\n      \n382.80\n\n      \n8.05\n\n      \n18.2\n\n    \n\n    \n\n      \n355\n\n      \n0.10659\n\n      \n80.0\n\n      \n1.91\n\n      \n0.0\n\n      \n0.413\n\n      \n5.936\n\n      \n19.5\n\n      \n10.5857\n\n      \n4.0\n\n      \n334.0\n\n      \n22.0\n\n      \n376.04\n\n      \n5.57\n\n      \n20.6\n\n    \n\n    \n\n      \n127\n\n      \n0.25915\n\n      \n0.0\n\n      \n21.89\n\n      \n0.0\n\n      \n0.624\n\n      \n5.693\n\n      \n96.0\n\n      \n1.7883\n\n      \n4.0\n\n      \n437.0\n\n      \n21.2\n\n      \n392.11\n\n      \n17.19\n\n      \n16.2\n\n    \n\n    \n\n      \n128\n\n      \n0.32543\n\n      \n0.0\n\n      \n21.89\n\n      \n0.0\n\n      \n0.624\n\n      \n6.431\n\n      \n98.8\n\n      \n1.8125\n\n      \n4.0\n\n      \n437.0\n\n      \n21.2\n\n      \n396.90\n\n      \n15.39\n\n      \n18.0\n\n    \n\n    \n\n      \n129\n\n      \n0.88125\n\n      \n0.0\n\n      \n21.89\n\n      \n0.0\n\n      \n0.624\n\n      \n5.637\n\n      \n94.7\n\n      \n1.9799\n\n      \n4.0\n\n      \n437.0\n\n      \n21.2\n\n      \n396.90\n\n      \n18.34\n\n      \n14.3\n\n    \n\n  \n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n    \n\n    \n\n      \nmean\n\n      \n3.593761\n\n      \n11.363636\n\n      \n11.136779\n\n      \n0.069170\n\n      \n0.554695\n\n      \n6.284634\n\n      \n68.574901\n\n      \n3.795043\n\n      \n9.549407\n\n      \n408.237154\n\n      \n18.455534\n\n      \n356.674032\n\n      \n12.653063\n\n      \n22.532806\n\n    \n\n    \n\n      \nstd\n\n      \n8.596783\n\n      \n23.322453\n\n      \n6.860353\n\n      \n0.253994\n\n      \n0.115878\n\n      \n0.702617\n\n      \n28.148861\n\n      \n2.105710\n\n      \n8.707259\n\n      \n168.537116\n\n      \n2.164946\n\n      \n91.294864\n\n      \n7.141062\n\n      \n9.197104\n\n    \n\n    \n\n      \nmin\n\n      \n0.006320\n\n      \n0.000000\n\n      \n0.460000\n\n      \n0.000000\n\n      \n0.385000\n\n      \n3.561000\n\n      \n2.900000\n\n      \n1.129600\n\n      \n1.000000\n\n      \n187.000000\n\n      \n12.600000\n\n      \n0.320000\n\n      \n1.730000\n\n      \n5.000000\n\n    \n\n    \n\n      \n25%\n\n      \n0.082045\n\n      \n0.000000\n\n      \n5.190000\n\n      \n0.000000\n\n      \n0.449000\n\n      \n5.885500\n\n      \n45.025000\n\n      \n2.100175\n\n      \n4.000000\n\n      \n279.000000\n\n      \n17.400000\n\n      \n375.377500\n\n      \n6.950000\n\n      \n17.025000\n\n    \n\n    \n\n      \n50%\n\n      \n0.256510\n\n      \n0.000000\n\n      \n9.690000\n\n      \n0.000000\n\n      \n0.538000\n\n      \n6.208500\n\n      \n77.500000\n\n      \n3.207450\n\n      \n5.000000\n\n      \n330.000000\n\n      \n19.050000\n\n      \n391.440000\n\n      \n11.360000\n\n      \n21.200000\n\n    \n\n    \n\n      \n75%\n\n      \n3.647423\n\n      \n12.500000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.624000\n\n      \n6.623500\n\n      \n94.075000\n\n      \n5.188425\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.225000\n\n      \n16.955000\n\n      \n25.000000\n\n    \n\n    \n\n      \nmax\n\n      \n88.976200\n\n      \n100.000000\n\n      \n27.740000\n\n      \n1.000000\n\n      \n0.871000\n\n      \n8.780000\n\n      \n100.000000\n\n      \n12.126500\n\n      \n24.000000\n\n      \n711.000000\n\n      \n22.000000\n\n      \n396.900000\n\n      \n37.970000\n\n      \n50.000000\n\n    \n\n  \n\n\n\n\n\n\n\n Findings \n\n\n The 5 towns shown in CRIM table are particularly high\n\n All the towns shown in the TAX table have maximum TAX level\n* PTRATIO table shows towns with high pupil-teacher ratios but not so uneven\n\n\ne) Suburbs bounding the Charles river\n\n\ndf['CHAS'].value_counts()[1]\n\n\n\n\n35\n\n\n\n(f) Median pupil-teacher ratio\n\n\ndf['PTRATIO'].median()\n\n\n\n\n19.05\n\n\n\n(g) Suburb with lowest median value of owner occupied homes\n\n\ndf['target'].idxmin()\n\n\n\n\n398\n\n\n\na = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc[398] = df.ix[398]\na\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n    \n\n    \n\n      \nmean\n\n      \n3.593761\n\n      \n11.363636\n\n      \n11.136779\n\n      \n0.069170\n\n      \n0.554695\n\n      \n6.284634\n\n      \n68.574901\n\n      \n3.795043\n\n      \n9.549407\n\n      \n408.237154\n\n      \n18.455534\n\n      \n356.674032\n\n      \n12.653063\n\n      \n22.532806\n\n    \n\n    \n\n      \nstd\n\n      \n8.596783\n\n      \n23.322453\n\n      \n6.860353\n\n      \n0.253994\n\n      \n0.115878\n\n      \n0.702617\n\n      \n28.148861\n\n      \n2.105710\n\n      \n8.707259\n\n      \n168.537116\n\n      \n2.164946\n\n      \n91.294864\n\n      \n7.141062\n\n      \n9.197104\n\n    \n\n    \n\n      \nmin\n\n      \n0.006320\n\n      \n0.000000\n\n      \n0.460000\n\n      \n0.000000\n\n      \n0.385000\n\n      \n3.561000\n\n      \n2.900000\n\n      \n1.129600\n\n      \n1.000000\n\n      \n187.000000\n\n      \n12.600000\n\n      \n0.320000\n\n      \n1.730000\n\n      \n5.000000\n\n    \n\n    \n\n      \n25%\n\n      \n0.082045\n\n      \n0.000000\n\n      \n5.190000\n\n      \n0.000000\n\n      \n0.449000\n\n      \n5.885500\n\n      \n45.025000\n\n      \n2.100175\n\n      \n4.000000\n\n      \n279.000000\n\n      \n17.400000\n\n      \n375.377500\n\n      \n6.950000\n\n      \n17.025000\n\n    \n\n    \n\n      \n50%\n\n      \n0.256510\n\n      \n0.000000\n\n      \n9.690000\n\n      \n0.000000\n\n      \n0.538000\n\n      \n6.208500\n\n      \n77.500000\n\n      \n3.207450\n\n      \n5.000000\n\n      \n330.000000\n\n      \n19.050000\n\n      \n391.440000\n\n      \n11.360000\n\n      \n21.200000\n\n    \n\n    \n\n      \n75%\n\n      \n3.647423\n\n      \n12.500000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.624000\n\n      \n6.623500\n\n      \n94.075000\n\n      \n5.188425\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.225000\n\n      \n16.955000\n\n      \n25.000000\n\n    \n\n    \n\n      \nmax\n\n      \n88.976200\n\n      \n100.000000\n\n      \n27.740000\n\n      \n1.000000\n\n      \n0.871000\n\n      \n8.780000\n\n      \n100.000000\n\n      \n12.126500\n\n      \n24.000000\n\n      \n711.000000\n\n      \n22.000000\n\n      \n396.900000\n\n      \n37.970000\n\n      \n50.000000\n\n    \n\n    \n\n      \nrange\n\n      \n88.969880\n\n      \n100.000000\n\n      \n27.280000\n\n      \n1.000000\n\n      \n0.486000\n\n      \n5.219000\n\n      \n97.100000\n\n      \n10.996900\n\n      \n23.000000\n\n      \n524.000000\n\n      \n9.400000\n\n      \n396.580000\n\n      \n36.240000\n\n      \n45.000000\n\n    \n\n    \n\n      \n398\n\n      \n38.351800\n\n      \n0.000000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.693000\n\n      \n5.453000\n\n      \n100.000000\n\n      \n1.489600\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.900000\n\n      \n30.590000\n\n      \n5.000000\n\n    \n\n  \n\n\n\n\n\n\n\n Findings \n\n\n The suburb with the lowest median value is 398.\n\n Relative to the other towns, this suburb has high CRIM, ZN below quantile 75%, above mean INDUS, does not bound the Charles river, above mean NOX, RM below quantile 25%, maximum AGE, DIS near to the minimum value, maximum RAD, TAX in quantile 75%, PTRATIO as well, B maximum and LSTAT above quantile 75%.\n\n\nh) Number of rooms per dwelling\n\n\nlen(df[df['RM']>7])\n\n\n\n\n64\n\n\n\nlen(df[df['RM']>8])\n\n\n\n\n13\n\n\n\nlen(df[df['RM']>8])\n\n\n\n\n13\n\n\n\ndf[df['RM']>8].describe()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n    \n\n    \n\n      \nmean\n\n      \n0.718795\n\n      \n13.615385\n\n      \n7.078462\n\n      \n0.153846\n\n      \n0.539238\n\n      \n8.348538\n\n      \n71.538462\n\n      \n3.430192\n\n      \n7.461538\n\n      \n325.076923\n\n      \n16.361538\n\n      \n385.210769\n\n      \n4.310000\n\n      \n44.200000\n\n    \n\n    \n\n      \nstd\n\n      \n0.901640\n\n      \n26.298094\n\n      \n5.392767\n\n      \n0.375534\n\n      \n0.092352\n\n      \n0.251261\n\n      \n24.608723\n\n      \n1.883955\n\n      \n5.332532\n\n      \n110.971063\n\n      \n2.410580\n\n      \n10.529359\n\n      \n1.373566\n\n      \n8.092383\n\n    \n\n    \n\n      \nmin\n\n      \n0.020090\n\n      \n0.000000\n\n      \n2.680000\n\n      \n0.000000\n\n      \n0.416100\n\n      \n8.034000\n\n      \n8.400000\n\n      \n1.801000\n\n      \n2.000000\n\n      \n224.000000\n\n      \n13.000000\n\n      \n354.550000\n\n      \n2.470000\n\n      \n21.900000\n\n    \n\n    \n\n      \n25%\n\n      \n0.331470\n\n      \n0.000000\n\n      \n3.970000\n\n      \n0.000000\n\n      \n0.504000\n\n      \n8.247000\n\n      \n70.400000\n\n      \n2.288500\n\n      \n5.000000\n\n      \n264.000000\n\n      \n14.700000\n\n      \n384.540000\n\n      \n3.320000\n\n      \n41.700000\n\n    \n\n    \n\n      \n50%\n\n      \n0.520140\n\n      \n0.000000\n\n      \n6.200000\n\n      \n0.000000\n\n      \n0.507000\n\n      \n8.297000\n\n      \n78.300000\n\n      \n2.894400\n\n      \n7.000000\n\n      \n307.000000\n\n      \n17.400000\n\n      \n386.860000\n\n      \n4.140000\n\n      \n48.300000\n\n    \n\n    \n\n      \n75%\n\n      \n0.578340\n\n      \n20.000000\n\n      \n6.200000\n\n      \n0.000000\n\n      \n0.605000\n\n      \n8.398000\n\n      \n86.500000\n\n      \n3.651900\n\n      \n8.000000\n\n      \n307.000000\n\n      \n17.400000\n\n      \n389.700000\n\n      \n5.120000\n\n      \n50.000000\n\n    \n\n    \n\n      \nmax\n\n      \n3.474280\n\n      \n95.000000\n\n      \n19.580000\n\n      \n1.000000\n\n      \n0.718000\n\n      \n8.780000\n\n      \n93.900000\n\n      \n8.906700\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.900000\n\n      \n7.440000\n\n      \n50.000000\n\n    \n\n  \n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n    \n\n    \n\n      \nmean\n\n      \n3.593761\n\n      \n11.363636\n\n      \n11.136779\n\n      \n0.069170\n\n      \n0.554695\n\n      \n6.284634\n\n      \n68.574901\n\n      \n3.795043\n\n      \n9.549407\n\n      \n408.237154\n\n      \n18.455534\n\n      \n356.674032\n\n      \n12.653063\n\n      \n22.532806\n\n    \n\n    \n\n      \nstd\n\n      \n8.596783\n\n      \n23.322453\n\n      \n6.860353\n\n      \n0.253994\n\n      \n0.115878\n\n      \n0.702617\n\n      \n28.148861\n\n      \n2.105710\n\n      \n8.707259\n\n      \n168.537116\n\n      \n2.164946\n\n      \n91.294864\n\n      \n7.141062\n\n      \n9.197104\n\n    \n\n    \n\n      \nmin\n\n      \n0.006320\n\n      \n0.000000\n\n      \n0.460000\n\n      \n0.000000\n\n      \n0.385000\n\n      \n3.561000\n\n      \n2.900000\n\n      \n1.129600\n\n      \n1.000000\n\n      \n187.000000\n\n      \n12.600000\n\n      \n0.320000\n\n      \n1.730000\n\n      \n5.000000\n\n    \n\n    \n\n      \n25%\n\n      \n0.082045\n\n      \n0.000000\n\n      \n5.190000\n\n      \n0.000000\n\n      \n0.449000\n\n      \n5.885500\n\n      \n45.025000\n\n      \n2.100175\n\n      \n4.000000\n\n      \n279.000000\n\n      \n17.400000\n\n      \n375.377500\n\n      \n6.950000\n\n      \n17.025000\n\n    \n\n    \n\n      \n50%\n\n      \n0.256510\n\n      \n0.000000\n\n      \n9.690000\n\n      \n0.000000\n\n      \n0.538000\n\n      \n6.208500\n\n      \n77.500000\n\n      \n3.207450\n\n      \n5.000000\n\n      \n330.000000\n\n      \n19.050000\n\n      \n391.440000\n\n      \n11.360000\n\n      \n21.200000\n\n    \n\n    \n\n      \n75%\n\n      \n3.647423\n\n      \n12.500000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.624000\n\n      \n6.623500\n\n      \n94.075000\n\n      \n5.188425\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.225000\n\n      \n16.955000\n\n      \n25.000000\n\n    \n\n    \n\n      \nmax\n\n      \n88.976200\n\n      \n100.000000\n\n      \n27.740000\n\n      \n1.000000\n\n      \n0.871000\n\n      \n8.780000\n\n      \n100.000000\n\n      \n12.126500\n\n      \n24.000000\n\n      \n711.000000\n\n      \n22.000000\n\n      \n396.900000\n\n      \n37.970000\n\n      \n50.000000\n\n    \n\n  \n\n\n\n\n\n\n\n Comments \n\n\n CRIM is lower,\n\n INDUS proportion is lower,\n* % of lower status of the population (LSTAT) is lower.",
            "title": "2.10"
        },
        {
            "location": "/sols/chapter2/exercise10/#exercise-210",
            "text": "%matplotlib inline\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt",
            "title": "Exercise 2.10"
        },
        {
            "location": "/sols/chapter2/exercise10/#a-dataset-overview",
            "text": "In Python we can load the Boston dataset using scikit-learn.  from sklearn.datasets import load_boston\n\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['target'] = boston.target  print(boston['DESCR'])  Boston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:\n\n    :Number of Instances: 506\n\n    :Number of Attributes: 13 numeric/categorical predictive\n\n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.\n\n**References**\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)  df.head()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2 \n     \n      np.shape(df)  (506, 14)  Number of rows and columns   506 rows.  14 columns.   Rows and columns description    Each rows is town in Boston area.  Columns are features that can influence house price such as per capita crime rate by town ('CRIM').",
            "title": "a) Dataset overview"
        },
        {
            "location": "/sols/chapter2/exercise10/#b-scatterplots",
            "text": "We'll use seaborn to get a quick overview of pairwise relationships   g = sns.PairGrid(df)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)   plt.scatter(df['RM'], df['target'])\nplt.xlabel('RM')\nplt.ylabel('target');    Findings    It seems to exist a positive linear relationship between RM and target  This is expected as RM is the number of rooms (more space, higher price)  plt.scatter(df['LSTAT'], df['target'])\nplt.xlabel('LSTAT')\nplt.ylabel('target');    Findings    LSTAT and target seem to have a negative non-linear relationship  This is expected as LSTAT is the percent of lower status people (lower status, lower incomes, cheaper houses)  plt.scatter(df['RM'], df['LSTAT'])\nplt.xlabel('RM')\nplt.ylabel('LSTAT');    Findings    It seems to exist a negative non-linear relationship between LSTAT and RM  It makes sense since people with less money (higher LSTAT) can't afford bigger houses (high RM)",
            "title": "b) Scatterplots"
        },
        {
            "location": "/sols/chapter2/exercise10/#c-predictors-associated-with-capita-crime-rate",
            "text": "df.corrwith(df['CRIM']).sort_values()  target    -0.385832\nDIS       -0.377904\nB         -0.377365\nRM        -0.219940\nZN        -0.199458\nCHAS      -0.055295\nPTRATIO    0.288250\nAGE        0.350784\nINDUS      0.404471\nNOX        0.417521\nLSTAT      0.452220\nTAX        0.579564\nRAD        0.622029\nCRIM       1.000000\ndtype: float64  Looking at the previous scatterplots and the correlation of each variable with 'CRIM', we will have a closer at the 3 with the largest correlation, namely:  RAD, index of accessibility to radial highways,  TAX, full-value property-tax rate (in dollars per $10,000),\n* LSTAT, percentage of lower status of the population.  ax = sns.boxplot(x=\"RAD\", y=\"CRIM\", data=df)    Findings  \n* When RAD is equal to 24 (its highest value), average CRIM is much higher and CRIM range is much larger.  plt.scatter(df['TAX'], df['CRIM'])\nplt.xlabel('TAX')\nplt.ylabel('CRIM');    Findings  \n* When TAX is equal to  666 , average CRIM is much higher and CRIM range is much larger.  plt.scatter(df['LSTAT'], df['CRIM'])\nplt.xlabel('LSTAT')\nplt.ylabel('CRIM');    Findings    For lower values of LSTAT (< 10), CRIM is always under 10. For LSTAT > 10, there is a wider spread of CRIM.  For LSTAT < 20, a large proportion of the data points is very close to CRIM = 0.",
            "title": "c) Predictors associated with capita crime rate"
        },
        {
            "location": "/sols/chapter2/exercise10/#d-crime-rate-tax-rate-and-pupil-teacher-ratio-in-suburbs",
            "text": "df.ix[df['CRIM'].nlargest(5).index]   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       380 \n       88.9762 \n       0.0 \n       18.1 \n       0.0 \n       0.671 \n       6.968 \n       91.9 \n       1.4165 \n       24.0 \n       666.0 \n       20.2 \n       396.90 \n       17.21 \n       10.4 \n     \n     \n       418 \n       73.5341 \n       0.0 \n       18.1 \n       0.0 \n       0.679 \n       5.957 \n       100.0 \n       1.8026 \n       24.0 \n       666.0 \n       20.2 \n       16.45 \n       20.62 \n       8.8 \n     \n     \n       405 \n       67.9208 \n       0.0 \n       18.1 \n       0.0 \n       0.693 \n       5.683 \n       100.0 \n       1.4254 \n       24.0 \n       666.0 \n       20.2 \n       384.97 \n       22.98 \n       5.0 \n     \n     \n       410 \n       51.1358 \n       0.0 \n       18.1 \n       0.0 \n       0.597 \n       5.757 \n       100.0 \n       1.4130 \n       24.0 \n       666.0 \n       20.2 \n       2.60 \n       10.11 \n       15.0 \n     \n     \n       414 \n       45.7461 \n       0.0 \n       18.1 \n       0.0 \n       0.693 \n       4.519 \n       100.0 \n       1.6582 \n       24.0 \n       666.0 \n       20.2 \n       88.27 \n       36.98 \n       7.0 \n     \n      df.ix[df['TAX'].nlargest(5).index]   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       488 \n       0.15086 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.454 \n       92.7 \n       1.8209 \n       4.0 \n       711.0 \n       20.1 \n       395.09 \n       18.06 \n       15.2 \n     \n     \n       489 \n       0.18337 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.414 \n       98.3 \n       1.7554 \n       4.0 \n       711.0 \n       20.1 \n       344.05 \n       23.97 \n       7.0 \n     \n     \n       490 \n       0.20746 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.093 \n       98.0 \n       1.8226 \n       4.0 \n       711.0 \n       20.1 \n       318.43 \n       29.68 \n       8.1 \n     \n     \n       491 \n       0.10574 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.983 \n       98.8 \n       1.8681 \n       4.0 \n       711.0 \n       20.1 \n       390.11 \n       18.07 \n       13.6 \n     \n     \n       492 \n       0.11132 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.983 \n       83.5 \n       2.1099 \n       4.0 \n       711.0 \n       20.1 \n       396.90 \n       13.35 \n       20.1 \n     \n      df.ix[df['PTRATIO'].nlargest(5).index]   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       354 \n       0.04301 \n       80.0 \n       1.91 \n       0.0 \n       0.413 \n       5.663 \n       21.9 \n       10.5857 \n       4.0 \n       334.0 \n       22.0 \n       382.80 \n       8.05 \n       18.2 \n     \n     \n       355 \n       0.10659 \n       80.0 \n       1.91 \n       0.0 \n       0.413 \n       5.936 \n       19.5 \n       10.5857 \n       4.0 \n       334.0 \n       22.0 \n       376.04 \n       5.57 \n       20.6 \n     \n     \n       127 \n       0.25915 \n       0.0 \n       21.89 \n       0.0 \n       0.624 \n       5.693 \n       96.0 \n       1.7883 \n       4.0 \n       437.0 \n       21.2 \n       392.11 \n       17.19 \n       16.2 \n     \n     \n       128 \n       0.32543 \n       0.0 \n       21.89 \n       0.0 \n       0.624 \n       6.431 \n       98.8 \n       1.8125 \n       4.0 \n       437.0 \n       21.2 \n       396.90 \n       15.39 \n       18.0 \n     \n     \n       129 \n       0.88125 \n       0.0 \n       21.89 \n       0.0 \n       0.624 \n       5.637 \n       94.7 \n       1.9799 \n       4.0 \n       437.0 \n       21.2 \n       396.90 \n       18.34 \n       14.3 \n     \n      df.describe()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n     \n     \n       mean \n       3.593761 \n       11.363636 \n       11.136779 \n       0.069170 \n       0.554695 \n       6.284634 \n       68.574901 \n       3.795043 \n       9.549407 \n       408.237154 \n       18.455534 \n       356.674032 \n       12.653063 \n       22.532806 \n     \n     \n       std \n       8.596783 \n       23.322453 \n       6.860353 \n       0.253994 \n       0.115878 \n       0.702617 \n       28.148861 \n       2.105710 \n       8.707259 \n       168.537116 \n       2.164946 \n       91.294864 \n       7.141062 \n       9.197104 \n     \n     \n       min \n       0.006320 \n       0.000000 \n       0.460000 \n       0.000000 \n       0.385000 \n       3.561000 \n       2.900000 \n       1.129600 \n       1.000000 \n       187.000000 \n       12.600000 \n       0.320000 \n       1.730000 \n       5.000000 \n     \n     \n       25% \n       0.082045 \n       0.000000 \n       5.190000 \n       0.000000 \n       0.449000 \n       5.885500 \n       45.025000 \n       2.100175 \n       4.000000 \n       279.000000 \n       17.400000 \n       375.377500 \n       6.950000 \n       17.025000 \n     \n     \n       50% \n       0.256510 \n       0.000000 \n       9.690000 \n       0.000000 \n       0.538000 \n       6.208500 \n       77.500000 \n       3.207450 \n       5.000000 \n       330.000000 \n       19.050000 \n       391.440000 \n       11.360000 \n       21.200000 \n     \n     \n       75% \n       3.647423 \n       12.500000 \n       18.100000 \n       0.000000 \n       0.624000 \n       6.623500 \n       94.075000 \n       5.188425 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.225000 \n       16.955000 \n       25.000000 \n     \n     \n       max \n       88.976200 \n       100.000000 \n       27.740000 \n       1.000000 \n       0.871000 \n       8.780000 \n       100.000000 \n       12.126500 \n       24.000000 \n       711.000000 \n       22.000000 \n       396.900000 \n       37.970000 \n       50.000000 \n     \n       Findings    The 5 towns shown in CRIM table are particularly high  All the towns shown in the TAX table have maximum TAX level\n* PTRATIO table shows towns with high pupil-teacher ratios but not so uneven",
            "title": "d) Crime rate, tax rate and pupil-teacher ratio in suburbs"
        },
        {
            "location": "/sols/chapter2/exercise10/#e-suburbs-bounding-the-charles-river",
            "text": "df['CHAS'].value_counts()[1]  35",
            "title": "e) Suburbs bounding the Charles river"
        },
        {
            "location": "/sols/chapter2/exercise10/#f-median-pupil-teacher-ratio",
            "text": "df['PTRATIO'].median()  19.05",
            "title": "(f) Median pupil-teacher ratio"
        },
        {
            "location": "/sols/chapter2/exercise10/#g-suburb-with-lowest-median-value-of-owner-occupied-homes",
            "text": "df['target'].idxmin()  398  a = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc[398] = df.ix[398]\na   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n     \n     \n       mean \n       3.593761 \n       11.363636 \n       11.136779 \n       0.069170 \n       0.554695 \n       6.284634 \n       68.574901 \n       3.795043 \n       9.549407 \n       408.237154 \n       18.455534 \n       356.674032 \n       12.653063 \n       22.532806 \n     \n     \n       std \n       8.596783 \n       23.322453 \n       6.860353 \n       0.253994 \n       0.115878 \n       0.702617 \n       28.148861 \n       2.105710 \n       8.707259 \n       168.537116 \n       2.164946 \n       91.294864 \n       7.141062 \n       9.197104 \n     \n     \n       min \n       0.006320 \n       0.000000 \n       0.460000 \n       0.000000 \n       0.385000 \n       3.561000 \n       2.900000 \n       1.129600 \n       1.000000 \n       187.000000 \n       12.600000 \n       0.320000 \n       1.730000 \n       5.000000 \n     \n     \n       25% \n       0.082045 \n       0.000000 \n       5.190000 \n       0.000000 \n       0.449000 \n       5.885500 \n       45.025000 \n       2.100175 \n       4.000000 \n       279.000000 \n       17.400000 \n       375.377500 \n       6.950000 \n       17.025000 \n     \n     \n       50% \n       0.256510 \n       0.000000 \n       9.690000 \n       0.000000 \n       0.538000 \n       6.208500 \n       77.500000 \n       3.207450 \n       5.000000 \n       330.000000 \n       19.050000 \n       391.440000 \n       11.360000 \n       21.200000 \n     \n     \n       75% \n       3.647423 \n       12.500000 \n       18.100000 \n       0.000000 \n       0.624000 \n       6.623500 \n       94.075000 \n       5.188425 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.225000 \n       16.955000 \n       25.000000 \n     \n     \n       max \n       88.976200 \n       100.000000 \n       27.740000 \n       1.000000 \n       0.871000 \n       8.780000 \n       100.000000 \n       12.126500 \n       24.000000 \n       711.000000 \n       22.000000 \n       396.900000 \n       37.970000 \n       50.000000 \n     \n     \n       range \n       88.969880 \n       100.000000 \n       27.280000 \n       1.000000 \n       0.486000 \n       5.219000 \n       97.100000 \n       10.996900 \n       23.000000 \n       524.000000 \n       9.400000 \n       396.580000 \n       36.240000 \n       45.000000 \n     \n     \n       398 \n       38.351800 \n       0.000000 \n       18.100000 \n       0.000000 \n       0.693000 \n       5.453000 \n       100.000000 \n       1.489600 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.900000 \n       30.590000 \n       5.000000 \n     \n       Findings    The suburb with the lowest median value is 398.  Relative to the other towns, this suburb has high CRIM, ZN below quantile 75%, above mean INDUS, does not bound the Charles river, above mean NOX, RM below quantile 25%, maximum AGE, DIS near to the minimum value, maximum RAD, TAX in quantile 75%, PTRATIO as well, B maximum and LSTAT above quantile 75%.",
            "title": "(g) Suburb with lowest median value of owner occupied homes"
        },
        {
            "location": "/sols/chapter2/exercise10/#h-number-of-rooms-per-dwelling",
            "text": "len(df[df['RM']>7])  64  len(df[df['RM']>8])  13  len(df[df['RM']>8])  13  df[df['RM']>8].describe()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n     \n     \n       mean \n       0.718795 \n       13.615385 \n       7.078462 \n       0.153846 \n       0.539238 \n       8.348538 \n       71.538462 \n       3.430192 \n       7.461538 \n       325.076923 \n       16.361538 \n       385.210769 \n       4.310000 \n       44.200000 \n     \n     \n       std \n       0.901640 \n       26.298094 \n       5.392767 \n       0.375534 \n       0.092352 \n       0.251261 \n       24.608723 \n       1.883955 \n       5.332532 \n       110.971063 \n       2.410580 \n       10.529359 \n       1.373566 \n       8.092383 \n     \n     \n       min \n       0.020090 \n       0.000000 \n       2.680000 \n       0.000000 \n       0.416100 \n       8.034000 \n       8.400000 \n       1.801000 \n       2.000000 \n       224.000000 \n       13.000000 \n       354.550000 \n       2.470000 \n       21.900000 \n     \n     \n       25% \n       0.331470 \n       0.000000 \n       3.970000 \n       0.000000 \n       0.504000 \n       8.247000 \n       70.400000 \n       2.288500 \n       5.000000 \n       264.000000 \n       14.700000 \n       384.540000 \n       3.320000 \n       41.700000 \n     \n     \n       50% \n       0.520140 \n       0.000000 \n       6.200000 \n       0.000000 \n       0.507000 \n       8.297000 \n       78.300000 \n       2.894400 \n       7.000000 \n       307.000000 \n       17.400000 \n       386.860000 \n       4.140000 \n       48.300000 \n     \n     \n       75% \n       0.578340 \n       20.000000 \n       6.200000 \n       0.000000 \n       0.605000 \n       8.398000 \n       86.500000 \n       3.651900 \n       8.000000 \n       307.000000 \n       17.400000 \n       389.700000 \n       5.120000 \n       50.000000 \n     \n     \n       max \n       3.474280 \n       95.000000 \n       19.580000 \n       1.000000 \n       0.718000 \n       8.780000 \n       93.900000 \n       8.906700 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.900000 \n       7.440000 \n       50.000000 \n     \n      df.describe()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n     \n     \n       mean \n       3.593761 \n       11.363636 \n       11.136779 \n       0.069170 \n       0.554695 \n       6.284634 \n       68.574901 \n       3.795043 \n       9.549407 \n       408.237154 \n       18.455534 \n       356.674032 \n       12.653063 \n       22.532806 \n     \n     \n       std \n       8.596783 \n       23.322453 \n       6.860353 \n       0.253994 \n       0.115878 \n       0.702617 \n       28.148861 \n       2.105710 \n       8.707259 \n       168.537116 \n       2.164946 \n       91.294864 \n       7.141062 \n       9.197104 \n     \n     \n       min \n       0.006320 \n       0.000000 \n       0.460000 \n       0.000000 \n       0.385000 \n       3.561000 \n       2.900000 \n       1.129600 \n       1.000000 \n       187.000000 \n       12.600000 \n       0.320000 \n       1.730000 \n       5.000000 \n     \n     \n       25% \n       0.082045 \n       0.000000 \n       5.190000 \n       0.000000 \n       0.449000 \n       5.885500 \n       45.025000 \n       2.100175 \n       4.000000 \n       279.000000 \n       17.400000 \n       375.377500 \n       6.950000 \n       17.025000 \n     \n     \n       50% \n       0.256510 \n       0.000000 \n       9.690000 \n       0.000000 \n       0.538000 \n       6.208500 \n       77.500000 \n       3.207450 \n       5.000000 \n       330.000000 \n       19.050000 \n       391.440000 \n       11.360000 \n       21.200000 \n     \n     \n       75% \n       3.647423 \n       12.500000 \n       18.100000 \n       0.000000 \n       0.624000 \n       6.623500 \n       94.075000 \n       5.188425 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.225000 \n       16.955000 \n       25.000000 \n     \n     \n       max \n       88.976200 \n       100.000000 \n       27.740000 \n       1.000000 \n       0.871000 \n       8.780000 \n       100.000000 \n       12.126500 \n       24.000000 \n       711.000000 \n       22.000000 \n       396.900000 \n       37.970000 \n       50.000000 \n     \n       Comments    CRIM is lower,  INDUS proportion is lower,\n* % of lower status of the population (LSTAT) is lower.",
            "title": "h) Number of rooms per dwelling"
        },
        {
            "location": "/sols/chapter3/exercise1/",
            "text": "Exercise 3.1\n\n\nThe t-statistics computed on Table 3.4 are computed individually for each coefficient since they are independent variables. Accordingly, there are 4 null hypotheses that we are testing:\n\n\n\n\n\\(H_0\\)\n for \"TV\": \nin the presence of\n Radio and Newspaper ads (and in addition to the intercept), there is no relationship between TV and Sales;\n\n\n\\(H_0\\)\n for \"Radio\": \nin the presence of\n TV and Newspaper ads (and in addition to the intercept), there is no relationship between Radio and Sales;\n\n\n\\(H_0\\)\n for \"Newspaper\": \nin the presence of\n TV and Radio ads (and in addition to the intercept), there is no relationship between Newspaper and Sales;\n\n\n\\(H_0\\)\n for the intercept: \nin the absence of\n TV, Radio and Newspaper ads, Sales are zero;\n\n\n\n\nversus the 4 corresponding alternative hypotheses:\n\n\n\\(H_a\\)\n: There is some relationship between TV/Radio/Newspaper and Sales, or Sales are non-zero in the absence of the other variables.\n\n\n\nMathematically, this can be written as\n\n\n\\(H_0:\\)\n \n\\(\\beta_i=0\\)\n, for \n\\(i = 0,1,2,3\\)\n,\n\n\nversus the 4 corresponding alternative hypotheses\n\n\n\\(H_a:\\)\n \n\\(\\beta_i\\neq0\\)\n, for \n\\(i = 0,1,2,3\\)\n.\n\n\nAs can been seen on Table 3.4 (and below with Python), for all the variables the p-value is practically zero, except for \nNewspaper\n for which it is very high, namely .86, much larger than the typical confidence levels, 0.05, 0.01 and 0.001.  Given the t-statistics and the p-values we can reject the null hypothesis for the intercept, TV and Radio, but not for Newspaper.\n\n\nThis means that we can conclude that \nthere is a relationship between TV and Sales, and between Radio and Sales\n. Also rejecting \n\\(\\beta_0=0\\)\n, allows us to conclude that \nin the absence of TV, Radio and Newspaper, Sales are non-zero\n. Not being able to reject the null hypothesis \n\\(\\beta_{Newspaper}=0\\)\n, suggests that there is indeed \nno relationship between Newspaper and Sales, in the presence of TV and Radio\n.\n\n\nAdditional comment\n\n\nAt a 5% p-value, there would be a 19% chance of having one appear as significant out of 3 variables, even if there was no relationship for all of them. \n\n\n(\n\\(1-.95^4\\)\n) \n\n\nAuxiliary calculations\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv('../data/Advertising.csv')\n\nfrom statsmodels.formula.api import ols\nmodel = ols(\"Sales ~ TV + Radio + Newspaper\", df).fit()\nprint(model.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Tue, 24 Oct 2017   Prob (F-statistic):           1.58e-96\nTime:                        10:19:37   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nRadio          0.1885      0.009     21.893      0.000       0.172       0.206\nNewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nFurther reading\n\n\nISL:\n\n\n\n\nPage 67, 68\n\n\nFootnote page 68\n\n\n\n\n\\(H_0\\)\n:\n\n\n\n\nhttp://quant.stackexchange.com/questions/16056/null-and-alternative-hypothesis-for-multiple-linear-regression\n\n\n\n\nMultiple regression:\n\n\n\n\nhttps://www.datarobot.com/blog/multiple-regression-using-statsmodels/\n\n\nhttps://www.coursera.org/learn/regression-modeling-practice/lecture/xQRab/python-lesson-1-multiple-regression\n\n\nhttp://www.scipy-lectures.org/packages/statistics/index.html#multiple-regression-including-multiple-factors\n\n\nhttp://stackoverflow.com/questions/11479064/multiple-linear-regression-in-python",
            "title": "3.1"
        },
        {
            "location": "/sols/chapter3/exercise1/#exercise-31",
            "text": "The t-statistics computed on Table 3.4 are computed individually for each coefficient since they are independent variables. Accordingly, there are 4 null hypotheses that we are testing:   \\(H_0\\)  for \"TV\":  in the presence of  Radio and Newspaper ads (and in addition to the intercept), there is no relationship between TV and Sales;  \\(H_0\\)  for \"Radio\":  in the presence of  TV and Newspaper ads (and in addition to the intercept), there is no relationship between Radio and Sales;  \\(H_0\\)  for \"Newspaper\":  in the presence of  TV and Radio ads (and in addition to the intercept), there is no relationship between Newspaper and Sales;  \\(H_0\\)  for the intercept:  in the absence of  TV, Radio and Newspaper ads, Sales are zero;   versus the 4 corresponding alternative hypotheses:  \\(H_a\\) : There is some relationship between TV/Radio/Newspaper and Sales, or Sales are non-zero in the absence of the other variables.  \nMathematically, this can be written as  \\(H_0:\\)   \\(\\beta_i=0\\) , for  \\(i = 0,1,2,3\\) ,  versus the 4 corresponding alternative hypotheses  \\(H_a:\\)   \\(\\beta_i\\neq0\\) , for  \\(i = 0,1,2,3\\) .  As can been seen on Table 3.4 (and below with Python), for all the variables the p-value is practically zero, except for  Newspaper  for which it is very high, namely .86, much larger than the typical confidence levels, 0.05, 0.01 and 0.001.  Given the t-statistics and the p-values we can reject the null hypothesis for the intercept, TV and Radio, but not for Newspaper.  This means that we can conclude that  there is a relationship between TV and Sales, and between Radio and Sales . Also rejecting  \\(\\beta_0=0\\) , allows us to conclude that  in the absence of TV, Radio and Newspaper, Sales are non-zero . Not being able to reject the null hypothesis  \\(\\beta_{Newspaper}=0\\) , suggests that there is indeed  no relationship between Newspaper and Sales, in the presence of TV and Radio .",
            "title": "Exercise 3.1"
        },
        {
            "location": "/sols/chapter3/exercise1/#additional-comment",
            "text": "At a 5% p-value, there would be a 19% chance of having one appear as significant out of 3 variables, even if there was no relationship for all of them.   ( \\(1-.95^4\\) )",
            "title": "Additional comment"
        },
        {
            "location": "/sols/chapter3/exercise1/#auxiliary-calculations",
            "text": "import pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv('../data/Advertising.csv')\n\nfrom statsmodels.formula.api import ols\nmodel = ols(\"Sales ~ TV + Radio + Newspaper\", df).fit()\nprint(model.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Tue, 24 Oct 2017   Prob (F-statistic):           1.58e-96\nTime:                        10:19:37   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nRadio          0.1885      0.009     21.893      0.000       0.172       0.206\nNewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "title": "Auxiliary calculations"
        },
        {
            "location": "/sols/chapter3/exercise1/#further-reading",
            "text": "ISL:   Page 67, 68  Footnote page 68   \\(H_0\\) :   http://quant.stackexchange.com/questions/16056/null-and-alternative-hypothesis-for-multiple-linear-regression   Multiple regression:   https://www.datarobot.com/blog/multiple-regression-using-statsmodels/  https://www.coursera.org/learn/regression-modeling-practice/lecture/xQRab/python-lesson-1-multiple-regression  http://www.scipy-lectures.org/packages/statistics/index.html#multiple-regression-including-multiple-factors  http://stackoverflow.com/questions/11479064/multiple-linear-regression-in-python",
            "title": "Further reading"
        },
        {
            "location": "/sols/chapter3/exercise2/",
            "text": "Exercise 3.2\n\n\nBoth models share a similar principle: the output values are computed based on their K closest points (nearest neighbours) value.\n\n\nThe KNN classifier starts by identifying the K nearest neighbours. Then, the output of each K observation is considered and, by majority vote, we determine the label of our observation. Example: if we are trying to classify an observation as 'blue' or 'red', and in the K nearest neighbours we have two of them classified as 'blue' and one as 'red', our observation will be classified as 'blue'. Notice that if we have a tie, a common solution is to increase or decrease K.\n\n\nRegarding the KNN regression method, it also starts by identifying the K nearest neighbours. However, in this situation, we compute our output averaging the output of the K nearest neighbours. Example: if our K nearest neighbours have as output the values 3,4 and 5, our output will be (3+4+5)/3 = 4.",
            "title": "3.2"
        },
        {
            "location": "/sols/chapter3/exercise2/#exercise-32",
            "text": "Both models share a similar principle: the output values are computed based on their K closest points (nearest neighbours) value.  The KNN classifier starts by identifying the K nearest neighbours. Then, the output of each K observation is considered and, by majority vote, we determine the label of our observation. Example: if we are trying to classify an observation as 'blue' or 'red', and in the K nearest neighbours we have two of them classified as 'blue' and one as 'red', our observation will be classified as 'blue'. Notice that if we have a tie, a common solution is to increase or decrease K.  Regarding the KNN regression method, it also starts by identifying the K nearest neighbours. However, in this situation, we compute our output averaging the output of the K nearest neighbours. Example: if our K nearest neighbours have as output the values 3,4 and 5, our output will be (3+4+5)/3 = 4.",
            "title": "Exercise 3.2"
        },
        {
            "location": "/sols/chapter3/exercise3/",
            "text": "Exercise 3.3\n\n\n(a)\n\n\n\\(Y = \\beta_0 + \\beta_1 \\times GPA + \\beta_2 \\times IQ + \\beta_3 \\times Gender + \\beta_4 \\times GPA \\times IQ + \\beta_5 \\times GPA \\times Gender\\)\n\n\nFor a fixed value of GPA and IQ, the difference between female and male is given by:\n\n\n\\(Y_{female} - Y_{male} = \\beta_3 + \\beta_5 \\times GPA = 35 - 10 GPA\\)\n, \n\n\nwhich depends on GPA. It is clear that in the normal range of the GPA (0 to 4.0), the difference in expected salary between female and male ranges linearly from 35 to -5. In particular, if GPA > 3.5, males earn on average more than females. Therefore, \n the correct answer is (iii)\n.\n\n\n(b)\n\n\nThe \npredicted salary is 137.1 (thousand dollars)\n. Given the coefficients from the fit, GPA = 4.0, IQ = 110 and Gender = 1, the model predicts:\n\n\n\\(Y = \\beta_0 + \\beta_1 GPA + \\beta_2 IQ + \\beta_3 Gender + \\beta_4 (GPA \\times IQ) + \\beta_5 (GPA \\times Gender)\\)\n\n\n\\(Y = 50 + 20 \\times 4.0 + 0.07 \\times 110 + 35 \\times 1  + 0.01 \\times 4.0 \\times 110 + (-10) \\times 4.0 \\times 1\\)\n = 137.1\n\n\n(c)\n\n\nFalse\n. Although the coefficient for the GPA/IQ interaction term is very small, specially when compared to the other coefficients, this does not indicate whether there is an interaction effect. First, this coefficient is multiplied by the product  of IQ and GPA which ranges from 0 to a few hundred, so that the contribution to the response would tipically add up to a value between 2 and 6, let's say. Secondly, and more importantly, evidence for the interaction effect has to be evaluated with a t-statistic or an F-statistic for a null hypothesis (\n\\(H_0: \\beta_4 = 0\\)\n), yielding a certain p-value. This requires the standard error of which we have no information, and therefore cannot conclude whether there is evidence for a interaction effect.\n\n\nAdditional calculations\n\n\n50 + 20*4 + 0.07*110 + 35*1 + 0.01*4*110 + (-10)*4*1\n\n\n\n\n137.1",
            "title": "3.3"
        },
        {
            "location": "/sols/chapter3/exercise3/#exercise-33",
            "text": "",
            "title": "Exercise 3.3"
        },
        {
            "location": "/sols/chapter3/exercise3/#a",
            "text": "\\(Y = \\beta_0 + \\beta_1 \\times GPA + \\beta_2 \\times IQ + \\beta_3 \\times Gender + \\beta_4 \\times GPA \\times IQ + \\beta_5 \\times GPA \\times Gender\\)  For a fixed value of GPA and IQ, the difference between female and male is given by:  \\(Y_{female} - Y_{male} = \\beta_3 + \\beta_5 \\times GPA = 35 - 10 GPA\\) ,   which depends on GPA. It is clear that in the normal range of the GPA (0 to 4.0), the difference in expected salary between female and male ranges linearly from 35 to -5. In particular, if GPA > 3.5, males earn on average more than females. Therefore,   the correct answer is (iii) .",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise3/#b",
            "text": "The  predicted salary is 137.1 (thousand dollars) . Given the coefficients from the fit, GPA = 4.0, IQ = 110 and Gender = 1, the model predicts:  \\(Y = \\beta_0 + \\beta_1 GPA + \\beta_2 IQ + \\beta_3 Gender + \\beta_4 (GPA \\times IQ) + \\beta_5 (GPA \\times Gender)\\)  \\(Y = 50 + 20 \\times 4.0 + 0.07 \\times 110 + 35 \\times 1  + 0.01 \\times 4.0 \\times 110 + (-10) \\times 4.0 \\times 1\\)  = 137.1",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise3/#c",
            "text": "False . Although the coefficient for the GPA/IQ interaction term is very small, specially when compared to the other coefficients, this does not indicate whether there is an interaction effect. First, this coefficient is multiplied by the product  of IQ and GPA which ranges from 0 to a few hundred, so that the contribution to the response would tipically add up to a value between 2 and 6, let's say. Secondly, and more importantly, evidence for the interaction effect has to be evaluated with a t-statistic or an F-statistic for a null hypothesis ( \\(H_0: \\beta_4 = 0\\) ), yielding a certain p-value. This requires the standard error of which we have no information, and therefore cannot conclude whether there is evidence for a interaction effect.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise3/#additional-calculations",
            "text": "50 + 20*4 + 0.07*110 + 35*1 + 0.01*4*110 + (-10)*4*1  137.1",
            "title": "Additional calculations"
        },
        {
            "location": "/sols/chapter3/exercise4/",
            "text": "Exercise 3.4\n\n\n(a) \nCubic regression will have lower Residual Sum of Squares (RSS).\n The cubic regression model is more flexible than the linear regression model. Accordingly, the cubic regression model can fit the data better and achieve a lower training RSS than the linear regression model.\n\n\n(b) \nLinear regression will have lower RSS. \n In general, more flexible models have less bias and higher variance. By contrast, more rigid models have high bias and lower variance. Since it is said that true relationship between the predictor and the response is linear, we know that, in this case, the linear regression model will have low bias. Consequently, this model will perform better than the cubic regression model, which is expected to have higher variance. \n\n\n(c) \n Cubic regression will have lower RSS. \n Same reason as in (a). Since the model is more flexible, it is able to fit the data better.\n\n\n(d) \nNot enough information to tell.\n Due to its flexibility, it is generally expected that the cubic regression model has lower bias and higher variance than the linear regression model. In this exercise, we know that the true relationship is non-linear, but we don't know how far it is from linear. This means that we don't have any idea about how high the bias of the linear regression model can be. If the model is just slightly non-linear, the linear regression will be able to model the data and achieve low bias. Thus, we would expect the linear model to have low bias and low variance. This could be enough (or not) to beat the cubic regression model, which is expected to have low bias and high variance. However, if the true relationship is substantially non-linear, the linear model will not be able to model the data and its bias will be high. With high bias and low variance, the linear regression model is beaten by a cubic model without \noverfitting\n problems. It will always depend on the bias-variance trade-off and, in general, on the size of the training set and the magnitude of the noise. We would need more information to know which model would have lower RSS.",
            "title": "3.4"
        },
        {
            "location": "/sols/chapter3/exercise4/#exercise-34",
            "text": "(a)  Cubic regression will have lower Residual Sum of Squares (RSS).  The cubic regression model is more flexible than the linear regression model. Accordingly, the cubic regression model can fit the data better and achieve a lower training RSS than the linear regression model.  (b)  Linear regression will have lower RSS.   In general, more flexible models have less bias and higher variance. By contrast, more rigid models have high bias and lower variance. Since it is said that true relationship between the predictor and the response is linear, we know that, in this case, the linear regression model will have low bias. Consequently, this model will perform better than the cubic regression model, which is expected to have higher variance.   (c)   Cubic regression will have lower RSS.   Same reason as in (a). Since the model is more flexible, it is able to fit the data better.  (d)  Not enough information to tell.  Due to its flexibility, it is generally expected that the cubic regression model has lower bias and higher variance than the linear regression model. In this exercise, we know that the true relationship is non-linear, but we don't know how far it is from linear. This means that we don't have any idea about how high the bias of the linear regression model can be. If the model is just slightly non-linear, the linear regression will be able to model the data and achieve low bias. Thus, we would expect the linear model to have low bias and low variance. This could be enough (or not) to beat the cubic regression model, which is expected to have low bias and high variance. However, if the true relationship is substantially non-linear, the linear model will not be able to model the data and its bias will be high. With high bias and low variance, the linear regression model is beaten by a cubic model without  overfitting  problems. It will always depend on the bias-variance trade-off and, in general, on the size of the training set and the magnitude of the noise. We would need more information to know which model would have lower RSS.",
            "title": "Exercise 3.4"
        },
        {
            "location": "/sols/chapter4/exercise1/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.1"
        },
        {
            "location": "/sols/chapter4/exercise2/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.2"
        },
        {
            "location": "/sols/chapter4/exercise3/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.3"
        },
        {
            "location": "/sols/chapter4/exercise4/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.4"
        },
        {
            "location": "/sols/chapter4/exercise5/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.5"
        },
        {
            "location": "/sols/chapter4/exercise6/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.6"
        },
        {
            "location": "/sols/chapter4/exercise7/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.7"
        },
        {
            "location": "/sols/chapter4/exercise8/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.8"
        },
        {
            "location": "/sols/chapter4/exercise9/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.9"
        },
        {
            "location": "/sols/chapter4/exercise10/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.10"
        },
        {
            "location": "/sols/chapter4/exercise11/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.11"
        },
        {
            "location": "/sols/chapter4/exercise12/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.12"
        },
        {
            "location": "/sols/chapter4/exercise13/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 4 will be out in December 2017.",
            "title": "4.13"
        },
        {
            "location": "/sols/chapter5/exercise1/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.1"
        },
        {
            "location": "/sols/chapter5/exercise2/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.2"
        },
        {
            "location": "/sols/chapter5/exercise3/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.3"
        },
        {
            "location": "/sols/chapter5/exercise4/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.4"
        },
        {
            "location": "/sols/chapter5/exercise5/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.5"
        },
        {
            "location": "/sols/chapter5/exercise6/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.6"
        },
        {
            "location": "/sols/chapter5/exercise7/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.7"
        },
        {
            "location": "/sols/chapter5/exercise8/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.8"
        },
        {
            "location": "/sols/chapter5/exercise9/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 5 will be out in January 2018.",
            "title": "5.9"
        },
        {
            "location": "/about/",
            "text": "This is a project by \nbotlnec\n, a group of friends working in different areas of data science. \n\n\nThis project was developed by two members of the Botlnec team and is included in a set of activities that this group of friends has been developping.\n\n\nPedro Marcelino\n\n\nPedro works as a researcher at \nLNEC\n and he is also a PhD candidate at \nIST\n.\nCurrently, he is developing a new approach to the maintenance management of transport infrastructures using machine learning.\nIn his spare time, he likes to work on TreeTree2, read books and play sports. \n\n\nJo\u00e3o Rico\n\n\nJo\u00e3o is a researcher at LNEC interested in machine and human learning.",
            "title": "About"
        },
        {
            "location": "/about/#pedro-marcelino",
            "text": "Pedro works as a researcher at  LNEC  and he is also a PhD candidate at  IST .\nCurrently, he is developing a new approach to the maintenance management of transport infrastructures using machine learning.\nIn his spare time, he likes to work on TreeTree2, read books and play sports.",
            "title": "Pedro Marcelino"
        },
        {
            "location": "/about/#joao-rico",
            "text": "Jo\u00e3o is a researcher at LNEC interested in machine and human learning.",
            "title": "Jo\u00e3o Rico"
        }
    ]
}