{
    "docs": [
        {
            "location": "/",
            "text": "An Introduction to Statistical Learning: with Applications in R... with Python!\n\n\nThis page contains the solutions to the exercises proposed in\n\n\n\n\n'An Introduction to Statistical Learning with Applications in R' (ISLR) by James, Witten, Hastie and Tibshirani\n [1].\n\n\n\n\nBoth conceptual and applied exercises were solved.\nAn effort was made to detail all the answers and to provide a set of bibliographical references that we found useful.\nThe exercises were solved using Python instead of R.\nYou are welcome to \ncollaborate\n.\n\n\nNote [03.October.2017]: we will release each chapter's solutions on a monthly basis (at least).\n\n\nSolutions\n\n\n\n\n\n\n\n\nChapter 2\n\n\nChapter 3\n\n\nChapter 4\n\n\nChapter 5\n\n\nChapter 6\n\n\nChapter 7\n\n\nChapter 8\n\n\nChapter 9\n\n\nChapter 10\n\n\n\n\n\n\n\n\n\n\n2.1\n\n\n3.1\n\n\n4.1\n\n\n5.1\n\n\n6.1\n\n\n7.1\n\n\n8.1\n\n\n9.1\n\n\n10.1\n\n\n\n\n\n\n2.2\n\n\n3.2\n\n\n4.2\n\n\n5.2\n\n\n6.2\n\n\n7.2\n\n\n8.2\n\n\n9.2\n\n\n10.2\n\n\n\n\n\n\n2.3\n\n\n3.3\n\n\n4.3\n\n\n5.3\n\n\n6.3\n\n\n7.3\n\n\n8.3\n\n\n9.3\n\n\n10.3\n\n\n\n\n\n\n2.4\n\n\n3.4\n\n\n4.4\n\n\n5.4\n\n\n6.4\n\n\n7.4\n\n\n8.4\n\n\n9.4\n\n\n10.4\n\n\n\n\n\n\n2.5\n\n\n3.5\n\n\n4.5\n\n\n5.5\n\n\n6.5\n\n\n7.5\n\n\n8.5\n\n\n9.5\n\n\n10.5\n\n\n\n\n\n\n2.6\n\n\n3.6\n\n\n4.6\n\n\n5.6\n\n\n6.6\n\n\n7.6\n\n\n8.6\n\n\n9.6\n\n\n10.6\n\n\n\n\n\n\n2.7\n\n\n3.7\n\n\n4.7\n\n\n5.7\n\n\n6.7\n\n\n7.7\n\n\n8.7\n\n\n9.7\n\n\n10.7\n\n\n\n\n\n\n2.8\n\n\n3.8\n\n\n4.8\n\n\n5.8\n\n\n6.8\n\n\n7.8\n\n\n8.8\n\n\n9.8\n\n\n10.8\n\n\n\n\n\n\n2.9\n\n\n3.9\n\n\n4.9\n\n\n5.9\n\n\n6.9\n\n\n7.9\n\n\n8.9\n\n\n\n\n10.9\n\n\n\n\n\n\n2.10\n\n\n3.10\n\n\n4.10\n\n\n\n\n6.10\n\n\n7.10\n\n\n8.10\n\n\n\n\n10.10\n\n\n\n\n\n\n\n\n3.11\n\n\n4.11\n\n\n\n\n6.11\n\n\n7.11\n\n\n8.11\n\n\n\n\n10.11\n\n\n\n\n\n\n\n\n3.12\n\n\n4.12\n\n\n\n\n\n\n7.12\n\n\n8.12\n\n\n\n\n\n\n\n\n\n\n\n\n3.13\n\n\n4.13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotivation\n\n\nThe main motivation of this project was learning.\nToday there are several good books and other resources from which to learn the material we covered, and we spent some time choosing a good learning project.\nWe chose \nISLR\n because it is an excellent, clear introduction to statistical learning, that  keeps a nice balance between theory, intuition, mathematical rigour and programming.\n\nOur main goal was to use the exercises as an excuse to improve our proficiency using Python's data science stack.\n\nWe had done other data science projects with Python, but, as we imagined, we still had a bit more to learn (and still do!).\nSince the book was written with R in mind, it made the use of Python a cool additional challenge.\nWe are strong advocates of the \nactive learning\n principles, and this project, once more, reinforced them in our minds.\nIf you're starting out in machine learning with Python (or R!), we recommend you try it!\n\n\nTechnical requirements, and How to Install\n\n\nThis project was developed using Python 3.5 on Jupyter notebooks (Jupyter Lab, in fact).\nWe tried to stay within the standard Python data science stack as much as possible.\nAccordingly, our main Python packages were numpy, matplotlib, pandas, seaborn, statsmodels and scikit-learn.\nYou should be able to run this with the standard Python setup, and the additional libraries we list below.\n\n\nIf you're just starting out with Python, here's a more complete 'how-to'. We recommend using \nAnaconda\n whether you are using Linux, Mac or Windows. Anaconda allows you to easily manage several Python environments.\nAn environment is a collection of installed Python packages.\nImagine that you have two projects with different requirements: a recent one with, say, Python 3.5 and matplotlib 4.0, and a legacy project with Python 2.7 and matplotlib 3.5.\nA good environment manager helps you install libraries and allows you to switch between both environments easily, avoiding dependencies migraines.\nYou can even work on both at the same time.\nYou don't want to know what the alternative is, to not using an environment manager.\nSo after installing Anaconda, the easiest way is to create a new environment and just install the libraries we list below one by one.\nAfter this is done, just make sure the desired environment is active (for example, on Linux and Mac, type 'source activate \n', and you're good to go). \n\n\nHere's the list of packages we installed:\n\n\n\n\njupyterlab (but this should run just as well on regular ipython notebooks)\n\n\nnumpy\n\n\npandas\n\n\nmatplotlib\n\n\nsklearn\n\n\nseaborn\n\n\nipywidgets (so that a seaborn import warning goes away)\n\n\nstatsmodels\n\n\nmlxtend \n\n\n\n\nIn addition, we chose mkdocs to present these solutions in a website format, for a better presentation. We might change to a different scheme in the future (any suggestions), but meanwhile we used these libraries:\n\n\n\n\nmkdocs\n\n\nmkdocs-cinder\n\n\npymdown-extensions #for latex \n\n\n\n\nHow to colaborate\n\n\nIf you want to collaborate, you can open an issue in our GitHub project and give us your suggestions on how to improve these solutions.\nOn GitHub, you can also fork this project and send a pull request to fix any mistakes that you have found.\nAlternatively, you can also go for the classical way of collaboration and send us an \ne-mail\n.\nAny effort to improve the quality of these solutions will be appreciated.\n\n\nMain references\n\n\nIn addition to thinking hard about them, to solve the exercises we followed several references.\nBesides ISLR [1], which is \navailable for free\n and explains almost everything you need to know to solve the exercises, \nwe also read some other books that provide a self-contained introduction to the field of statistical learning [2, 3, 4]. \nWe also spent some quality time on \nCrossValidate\n.\nFor the Python data science stack we think Wes McKinney's book [5] is a good choice, as well as Jake VanderPlas' [6].\nAdditional references for some of the exercises are scattered througout the solutions.\n\n\n\n\n[1] \nJames, G., Witten, D., Hastie, T., Tibshirani, R., 2015, An Introduction to Statistical Learning with Applications in R, Springer.\n \n[available for free]\n\n\n[2] \nHastie, T., Tibshirani, R. 2006, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.\n \n[available for free]\n\n\n[3] \nBishop, C.M., 2006, Pattern Recognition and Machine Learning, Springer.\n\n\n[4] \nMurphy, K.P., 2012, Machine Learning: A Probabilistic Perspective, MIT Press.\n\n\n[5] \nMcKinney, W., 2012, Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython\n\n\n[6] \nJake VanderPlas, 2016, Python Data Science Handbook: Essential Tools for Working with Data, O'Reilly Media.\n\n\n\n\nLearning resources\n\n\nFortunately, online resources are becoming more and more an essential tool for self-learning strategies.\nDuring the course of this project, we found several resources that can help you on your learning path. \nHere are the best ones we found. \n\n\n\n\nStanford's online course by the authors of ISLR\n\n\nAndrew Ng's Machine Learning course\n\n\n\n\nOther solutions to ISLR\n\n\nThere are other solutions to ISLR, though most of them do not use Python.\nBelow you can find a link to the solutions we found that were reasonably complete.\nWe did a occasional check with some of these, and they might be a good complementary resource.\nA special mention to \nJWarmenhoven's github repo\n, which uses Python to reproduce figures, tables and calculations of the main text of the chapters and labs.\nPlease let us know if you find any other significant solutions. \n\n\n\n\nJWarmenhoven: Python for the chapters text and labs\n\n\nJames Crouser's Exercises and Python Labs\n\n\nAsadoughi's\n\n\nJohn Weatherwax's solutions to the conceptual and practical exercises in R\n\n\nPierre Pacquay's\n\n\nyahwes'\n\n\n\n\nMIT License\n\n\nCopyright (c) [2017] [ISLP]\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "Home"
        },
        {
            "location": "/#an-introduction-to-statistical-learning-with-applications-in-r-with-python",
            "text": "This page contains the solutions to the exercises proposed in   'An Introduction to Statistical Learning with Applications in R' (ISLR) by James, Witten, Hastie and Tibshirani  [1].   Both conceptual and applied exercises were solved.\nAn effort was made to detail all the answers and to provide a set of bibliographical references that we found useful.\nThe exercises were solved using Python instead of R.\nYou are welcome to  collaborate .  Note [03.October.2017]: we will release each chapter's solutions on a monthly basis (at least).",
            "title": "An Introduction to Statistical Learning: with Applications in R... with Python!"
        },
        {
            "location": "/#solutions",
            "text": "Chapter 2  Chapter 3  Chapter 4  Chapter 5  Chapter 6  Chapter 7  Chapter 8  Chapter 9  Chapter 10      2.1  3.1  4.1  5.1  6.1  7.1  8.1  9.1  10.1    2.2  3.2  4.2  5.2  6.2  7.2  8.2  9.2  10.2    2.3  3.3  4.3  5.3  6.3  7.3  8.3  9.3  10.3    2.4  3.4  4.4  5.4  6.4  7.4  8.4  9.4  10.4    2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5  10.5    2.6  3.6  4.6  5.6  6.6  7.6  8.6  9.6  10.6    2.7  3.7  4.7  5.7  6.7  7.7  8.7  9.7  10.7    2.8  3.8  4.8  5.8  6.8  7.8  8.8  9.8  10.8    2.9  3.9  4.9  5.9  6.9  7.9  8.9   10.9    2.10  3.10  4.10   6.10  7.10  8.10   10.10     3.11  4.11   6.11  7.11  8.11   10.11     3.12  4.12    7.12  8.12       3.13  4.13           3.14            3.15",
            "title": "Solutions"
        },
        {
            "location": "/#motivation",
            "text": "The main motivation of this project was learning.\nToday there are several good books and other resources from which to learn the material we covered, and we spent some time choosing a good learning project.\nWe chose  ISLR  because it is an excellent, clear introduction to statistical learning, that  keeps a nice balance between theory, intuition, mathematical rigour and programming. Our main goal was to use the exercises as an excuse to improve our proficiency using Python's data science stack. \nWe had done other data science projects with Python, but, as we imagined, we still had a bit more to learn (and still do!).\nSince the book was written with R in mind, it made the use of Python a cool additional challenge.\nWe are strong advocates of the  active learning  principles, and this project, once more, reinforced them in our minds.\nIf you're starting out in machine learning with Python (or R!), we recommend you try it!",
            "title": "Motivation"
        },
        {
            "location": "/#technical-requirements-and-how-to-install",
            "text": "This project was developed using Python 3.5 on Jupyter notebooks (Jupyter Lab, in fact).\nWe tried to stay within the standard Python data science stack as much as possible.\nAccordingly, our main Python packages were numpy, matplotlib, pandas, seaborn, statsmodels and scikit-learn.\nYou should be able to run this with the standard Python setup, and the additional libraries we list below.  If you're just starting out with Python, here's a more complete 'how-to'. We recommend using  Anaconda  whether you are using Linux, Mac or Windows. Anaconda allows you to easily manage several Python environments.\nAn environment is a collection of installed Python packages.\nImagine that you have two projects with different requirements: a recent one with, say, Python 3.5 and matplotlib 4.0, and a legacy project with Python 2.7 and matplotlib 3.5.\nA good environment manager helps you install libraries and allows you to switch between both environments easily, avoiding dependencies migraines.\nYou can even work on both at the same time.\nYou don't want to know what the alternative is, to not using an environment manager.\nSo after installing Anaconda, the easiest way is to create a new environment and just install the libraries we list below one by one.\nAfter this is done, just make sure the desired environment is active (for example, on Linux and Mac, type 'source activate  ', and you're good to go).   Here's the list of packages we installed:   jupyterlab (but this should run just as well on regular ipython notebooks)  numpy  pandas  matplotlib  sklearn  seaborn  ipywidgets (so that a seaborn import warning goes away)  statsmodels  mlxtend    In addition, we chose mkdocs to present these solutions in a website format, for a better presentation. We might change to a different scheme in the future (any suggestions), but meanwhile we used these libraries:   mkdocs  mkdocs-cinder  pymdown-extensions #for latex",
            "title": "Technical requirements, and How to Install"
        },
        {
            "location": "/#how-to-colaborate",
            "text": "If you want to collaborate, you can open an issue in our GitHub project and give us your suggestions on how to improve these solutions.\nOn GitHub, you can also fork this project and send a pull request to fix any mistakes that you have found.\nAlternatively, you can also go for the classical way of collaboration and send us an  e-mail .\nAny effort to improve the quality of these solutions will be appreciated.",
            "title": "How to colaborate"
        },
        {
            "location": "/#main-references",
            "text": "In addition to thinking hard about them, to solve the exercises we followed several references.\nBesides ISLR [1], which is  available for free  and explains almost everything you need to know to solve the exercises, \nwe also read some other books that provide a self-contained introduction to the field of statistical learning [2, 3, 4]. \nWe also spent some quality time on  CrossValidate .\nFor the Python data science stack we think Wes McKinney's book [5] is a good choice, as well as Jake VanderPlas' [6].\nAdditional references for some of the exercises are scattered througout the solutions.   [1]  James, G., Witten, D., Hastie, T., Tibshirani, R., 2015, An Introduction to Statistical Learning with Applications in R, Springer.   [available for free]  [2]  Hastie, T., Tibshirani, R. 2006, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.   [available for free]  [3]  Bishop, C.M., 2006, Pattern Recognition and Machine Learning, Springer.  [4]  Murphy, K.P., 2012, Machine Learning: A Probabilistic Perspective, MIT Press.  [5]  McKinney, W., 2012, Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython  [6]  Jake VanderPlas, 2016, Python Data Science Handbook: Essential Tools for Working with Data, O'Reilly Media.",
            "title": "Main references"
        },
        {
            "location": "/#learning-resources",
            "text": "Fortunately, online resources are becoming more and more an essential tool for self-learning strategies.\nDuring the course of this project, we found several resources that can help you on your learning path. \nHere are the best ones we found.    Stanford's online course by the authors of ISLR  Andrew Ng's Machine Learning course",
            "title": "Learning resources"
        },
        {
            "location": "/#other-solutions-to-islr",
            "text": "There are other solutions to ISLR, though most of them do not use Python.\nBelow you can find a link to the solutions we found that were reasonably complete.\nWe did a occasional check with some of these, and they might be a good complementary resource.\nA special mention to  JWarmenhoven's github repo , which uses Python to reproduce figures, tables and calculations of the main text of the chapters and labs.\nPlease let us know if you find any other significant solutions.    JWarmenhoven: Python for the chapters text and labs  James Crouser's Exercises and Python Labs  Asadoughi's  John Weatherwax's solutions to the conceptual and practical exercises in R  Pierre Pacquay's  yahwes'",
            "title": "Other solutions to ISLR"
        },
        {
            "location": "/#mit-license",
            "text": "Copyright (c) [2017] [ISLP]  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "MIT License"
        },
        {
            "location": "/sols/chapter2/exercise1/",
            "text": "Exercise 2.1\n\n\nTo answer to this exercise, we need to understand the \nsources of error\n in a statistical learning method. For regression, assuming \nY = f(X) + \\varepsilon\nY = f(X) + \\varepsilon\n, where \nE[\\varepsilon]=0\nE[\\varepsilon]=0\n and \nVar[\\varepsilon]=\\sigma_\\varepsilon^2\nVar[\\varepsilon]=\\sigma_\\varepsilon^2\n, we can always obtain a decomposition of the test mean squared error, \nE[(Y - \\hat{f}(x_0))^2\nE[(Y - \\hat{f}(x_0))^2\n, into the sum of the irreducible error, the squared bias and the variance [1, page 223]:\n\n\n\\begin{align}\n\\mathrm{E}\\Big[\\big(Y - \\hat{f}(x)\\big)^2 \\Big|\\, X=x_0 \\Big]\n & =  \\sigma_\\varepsilon^2 + \\mathrm{Bias}^2\\big[\\hat{f}(x_0)\\big] + \\mathrm{Var}\\left[ \\hat{f}(x_0) \\right] , \\\n\\end{align}\nwhere \n\\sigma_\\varepsilon^2\n\\sigma_\\varepsilon^2\n is the noise or irreducible error, \n\n\n\n\n\\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}\n\n\n\\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}\n\n\n\n\nand\n\n\n\n\n\\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}\n\n\n\\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}\n\n\n\n\nSince the irreducible error corresponds to the lowest achievable error, a good test set performance of a statistical learning method requires low variance as well as low squared bias.\n\n\nWhen we approximate a problem (possibly very complex), by a simpler model we introduce an error known as \nbias\n.\nThe simplest non-trivial example might be approximating a non-linear relationship (for example, a quadratic one) by a linear function of parameters and predictors. In this case, will have a always non-zero test error, regardless of how well we fit the model parameters, how large the training set is, or even how small the noise is (even zero). The more the true model deviates from a linear one, the larger this error will be.\n\n\nOn the other hand, \nvariance\n refers to the amount by which the estimation function would change if it was estimated using a different training set.\nThe training set is used to estimate the model parameters, which means that we obtain different estimates from different training sets. We hope however that this difference is small, and we say that between estimates from different training sets is small, in which case we say that the learning method has low variance.\nOn the other hand, a method for which small changes in the training set might lead to large changes in the estimated model parameters is referred as a method with high variance.\n\n\nIn general, more flexible methods have less bias and have higher variance. This is referred to as the  \nbias-variance trade-off\n since a low test mean squared error requires both low bias and low variance.\n\n\n(a) Extremely large sample, few predictors\n\n\nA flexible method is expected to be \nbetter\n. \n\n\nSince the sample size is extremely large and the number of predictors is small, a more flexible method would be able to better fit the data, while not fitting the noise due to the very large sample size. In other words a more flexible model would have the upside of a less bias, without much risk of \noverfitting\n.\n\n\n(b) Awful lot of predictors, small sample\n\n\nA flexible method is expected to be  \nworse\n.\n\n\nIt is very likely that when the number of predictors is extremely large and the number of observations is small a flexible model would fit the noise, meaning that, given another random data set of the same distribuition, the fit would likely be significantly different. Therefore one would be better off using a less flexible method, which will have more bias, but will be less likely to overfit.\n\n\n(c) Highly non-linear relationship\n\n\nA flexible method is expected to be  \nbetter\n.\n\n\nA more flexible method will likely be necessary to model a highly non-linear relationship, otherwise the model will be too biased and not capture the non-linearities of the model. No matter how large the sample size, a less flexible model would always be limited.\n\n\n(d) Extremely high variance\n\n\nA flexible method is expected to be  \nworse\n.\n\n\nSince the variance is extremely high, a more flexible model will fit the noise more and thus very likely overfit. A less flexible model will be more likely to still capture the essential 'features' of the model without picking up extraneous ones induced by the noise.\n\n\nFurther reading\n\n\nThe bias-variance decomposition is a fundamental aspect of machine learning which is present not only in regression. This decomposition has been generalized to more general loss functions and to classification learning methods. See, for example, [2].\n\n\n\n\n[1] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. \nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n. Springer, 2009.\n\n\n[2] James, Gareth M. \n\"Variance and bias for general loss functions.\"\n  Machine learning 51.2 (2003): 115-135.",
            "title": "2.1"
        },
        {
            "location": "/sols/chapter2/exercise1/#exercise-21",
            "text": "To answer to this exercise, we need to understand the  sources of error  in a statistical learning method. For regression, assuming  Y = f(X) + \\varepsilon Y = f(X) + \\varepsilon , where  E[\\varepsilon]=0 E[\\varepsilon]=0  and  Var[\\varepsilon]=\\sigma_\\varepsilon^2 Var[\\varepsilon]=\\sigma_\\varepsilon^2 , we can always obtain a decomposition of the test mean squared error,  E[(Y - \\hat{f}(x_0))^2 E[(Y - \\hat{f}(x_0))^2 , into the sum of the irreducible error, the squared bias and the variance [1, page 223]:  \\begin{align}\n\\mathrm{E}\\Big[\\big(Y - \\hat{f}(x)\\big)^2 \\Big|\\, X=x_0 \\Big]\n & =  \\sigma_\\varepsilon^2 + \\mathrm{Bias}^2\\big[\\hat{f}(x_0)\\big] + \\mathrm{Var}\\left[ \\hat{f}(x_0) \\right] , \\\n\\end{align}\nwhere  \\sigma_\\varepsilon^2 \\sigma_\\varepsilon^2  is the noise or irreducible error,    \\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}  \\begin{align}\n \\mathrm{Bias}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}\\big[\\hat{f}(x_0) - f(x_0)\\big],\n\\end{align}   and   \\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}  \\begin{align}\n\\mathrm{Var}\\big[\\hat{f}(x_0)\\big] = \\mathrm{E}[\\hat{f}(x_0)^2] - \\mathrm{E}[\\hat{f}(x_0)]^2.\n\\end{align}   Since the irreducible error corresponds to the lowest achievable error, a good test set performance of a statistical learning method requires low variance as well as low squared bias.  When we approximate a problem (possibly very complex), by a simpler model we introduce an error known as  bias .\nThe simplest non-trivial example might be approximating a non-linear relationship (for example, a quadratic one) by a linear function of parameters and predictors. In this case, will have a always non-zero test error, regardless of how well we fit the model parameters, how large the training set is, or even how small the noise is (even zero). The more the true model deviates from a linear one, the larger this error will be.  On the other hand,  variance  refers to the amount by which the estimation function would change if it was estimated using a different training set.\nThe training set is used to estimate the model parameters, which means that we obtain different estimates from different training sets. We hope however that this difference is small, and we say that between estimates from different training sets is small, in which case we say that the learning method has low variance.\nOn the other hand, a method for which small changes in the training set might lead to large changes in the estimated model parameters is referred as a method with high variance.  In general, more flexible methods have less bias and have higher variance. This is referred to as the   bias-variance trade-off  since a low test mean squared error requires both low bias and low variance.",
            "title": "Exercise 2.1"
        },
        {
            "location": "/sols/chapter2/exercise1/#a-extremely-large-sample-few-predictors",
            "text": "A flexible method is expected to be  better .   Since the sample size is extremely large and the number of predictors is small, a more flexible method would be able to better fit the data, while not fitting the noise due to the very large sample size. In other words a more flexible model would have the upside of a less bias, without much risk of  overfitting .",
            "title": "(a) Extremely large sample, few predictors"
        },
        {
            "location": "/sols/chapter2/exercise1/#b-awful-lot-of-predictors-small-sample",
            "text": "A flexible method is expected to be   worse .  It is very likely that when the number of predictors is extremely large and the number of observations is small a flexible model would fit the noise, meaning that, given another random data set of the same distribuition, the fit would likely be significantly different. Therefore one would be better off using a less flexible method, which will have more bias, but will be less likely to overfit.",
            "title": "(b) Awful lot of predictors, small sample"
        },
        {
            "location": "/sols/chapter2/exercise1/#c-highly-non-linear-relationship",
            "text": "A flexible method is expected to be   better .  A more flexible method will likely be necessary to model a highly non-linear relationship, otherwise the model will be too biased and not capture the non-linearities of the model. No matter how large the sample size, a less flexible model would always be limited.",
            "title": "(c) Highly non-linear relationship"
        },
        {
            "location": "/sols/chapter2/exercise1/#d-extremely-high-variance",
            "text": "A flexible method is expected to be   worse .  Since the variance is extremely high, a more flexible model will fit the noise more and thus very likely overfit. A less flexible model will be more likely to still capture the essential 'features' of the model without picking up extraneous ones induced by the noise.",
            "title": "(d) Extremely high variance"
        },
        {
            "location": "/sols/chapter2/exercise1/#further-reading",
            "text": "The bias-variance decomposition is a fundamental aspect of machine learning which is present not only in regression. This decomposition has been generalized to more general loss functions and to classification learning methods. See, for example, [2].   [1] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman.  The Elements of Statistical Learning: Data Mining, Inference, and Prediction . Springer, 2009.  [2] James, Gareth M.  \"Variance and bias for general loss functions.\"   Machine learning 51.2 (2003): 115-135.",
            "title": "Further reading"
        },
        {
            "location": "/sols/chapter2/exercise2/",
            "text": "Exercise 2.2\n\n\n(a) CEO salary\n\n\nRegression\n, since the variable we are interested in, CEO salary, is quantitative.\nSince we want to understand the factors affecting CEO salary we are most interested in \ninference\n.\nThe number of predictors is 3, and the sample size is 500 \n(p=3, n=500)\n.\n\n\n(b) New product launch: success or failure?\n\n\nClassification\n, because we are studying the outcome of the launch of a new product, which we are representing in one of two categories: success or failure.\nIn this case we wish to know whether a new launch will be a success or a failure more than understand why, so we are more interested in \nprediction\n.\nThe number of predictors is 13, and the sample size is 20 \n(p=13, n=20)\n.\n\n\n(c) Change in the US dollar\n\n\nRegression\n, since the variable we want to predict is a real number, and we are more interested in \nprediction\n.\nThe number of predictors is 3, and the sample size is 52 \n(p=3, n=52)\n.",
            "title": "2.2"
        },
        {
            "location": "/sols/chapter2/exercise2/#exercise-22",
            "text": "",
            "title": "Exercise 2.2"
        },
        {
            "location": "/sols/chapter2/exercise2/#a-ceo-salary",
            "text": "Regression , since the variable we are interested in, CEO salary, is quantitative.\nSince we want to understand the factors affecting CEO salary we are most interested in  inference .\nThe number of predictors is 3, and the sample size is 500  (p=3, n=500) .",
            "title": "(a) CEO salary"
        },
        {
            "location": "/sols/chapter2/exercise2/#b-new-product-launch-success-or-failure",
            "text": "Classification , because we are studying the outcome of the launch of a new product, which we are representing in one of two categories: success or failure.\nIn this case we wish to know whether a new launch will be a success or a failure more than understand why, so we are more interested in  prediction .\nThe number of predictors is 13, and the sample size is 20  (p=13, n=20) .",
            "title": "(b) New product launch: success or failure?"
        },
        {
            "location": "/sols/chapter2/exercise2/#c-change-in-the-us-dollar",
            "text": "Regression , since the variable we want to predict is a real number, and we are more interested in  prediction .\nThe number of predictors is 3, and the sample size is 52  (p=3, n=52) .",
            "title": "(c) Change in the US dollar"
        },
        {
            "location": "/sols/chapter2/exercise3/",
            "text": "Exercise 2.3\n\n\nIn (a) we provide a rough sketch of the general case, which is merely illustrative of the general behaviour of each quantity as described in (b). They are similar to the left panel of  figure 2.9 of the text and the left panel of figure 2.12. \n\n\n(a) Sketch of bias, variance, training, test and Bayes errors\n\n\n# the functions chosen here were chosen just as a rough, quick way to sketch the functions in a plot\n# they do not represent in any way an analytical formula for these quantities or anything of the sort\n# these formulas would depend on the model and fitting procedure in any case\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0.0, 10.0, 0.02)\n\ndef squared_bias(x):\n    return .002*(-x+10)**3\ndef variance(x):\n    return .002*x**3 \ndef training_error(x):\n    return 2.38936 - 0.825077*x + 0.176655*x**2 - 0.0182319*x**3 + 0.00067091*x**4\ndef test_error(x):\n    return 3 - 0.6*x + .06*x**2\ndef bayes_error(x):\n    return x + 1 - x\n\nplt.xkcd()\n#frame = plt.gca()\n#frame.axes.xaxis.set_ticklabels([])\nplt.figure(figsize=(10, 8))\nplt.plot(x,squared_bias(x), label='squared bias')\nplt.plot(x, variance(x), label='variance')\nplt.plot(x, training_error(x), label='training error')\nplt.plot(x, test_error(x), label='test error')\nplt.plot(x, bayes_error(x), label='Bayes error')\nplt.legend(loc='upper center')\nplt.xlabel('model flexibility')\nplt.show()\n\n#arbitrary units\n\n\n\n\n\n\n(b) Why these shapes?\n\n\nSquared bias.\n This is the error in our model introduced by the difference of our approximation and the true underlying function. A more flexible model will be increasingly similar,  and the squared bias therefore dimishes as the flexibility increases. (It might even reach zero if for example the underlying function is a polynomial and by increasing the flexibility at a certain point we include the polynomials of this degree in our hypothesis space.)\n\n\nVariance.\n In the limit of a model with no flexibility the variance will be zero, since the model fit will be independent of the data. As the flexibility increases the variance will increase as well since the noise in a particular training set will correspondingly captured by the model. The curve described by the variance is an monotonically increasing function of the flexibility of the model.\n\n\nTraining error.\n The training error is given by the average (squared) difference between the predictions of the model and the observations. If a model if very unflexible this can be quite high, but as the flexibility increases this difference will decrease. If we consider polynomials for example increasing the flexibility of the model might mean increasing the degree of the polynomial to be fitted. The additional degrees of freedom will decrease the average difference and reduce the training error.\n\n\nTest error.\n Ths expected test error is given by the formula: Variance + Bias + Bayes error, all of which are non-negative. The Bayes error is constant and a lower bound for the test error. The test error has a minimum at an intermediate level of flexibility: not too flexible, so that the variance does not dominate, and not too unflexible, so that the squared bias is not too high. The plot of the test error thus resembles sort of an upward (deformed) parabola: high for unflexible models, decreasing as flexibility increases until it reaches a minimum. Then the variance starts to dominate and the test error starts increasing. The distance between this minimum and the Bayes irreducible error gives us an idea of how well the best function in the hypothesis space will fit.\n\n\nBayes error.\n This term is constant since by definition it does not depend on X and therefore on the flexibility of the model.",
            "title": "2.3"
        },
        {
            "location": "/sols/chapter2/exercise3/#exercise-23",
            "text": "In (a) we provide a rough sketch of the general case, which is merely illustrative of the general behaviour of each quantity as described in (b). They are similar to the left panel of  figure 2.9 of the text and the left panel of figure 2.12.",
            "title": "Exercise 2.3"
        },
        {
            "location": "/sols/chapter2/exercise3/#a-sketch-of-bias-variance-training-test-and-bayes-errors",
            "text": "# the functions chosen here were chosen just as a rough, quick way to sketch the functions in a plot\n# they do not represent in any way an analytical formula for these quantities or anything of the sort\n# these formulas would depend on the model and fitting procedure in any case\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0.0, 10.0, 0.02)\n\ndef squared_bias(x):\n    return .002*(-x+10)**3\ndef variance(x):\n    return .002*x**3 \ndef training_error(x):\n    return 2.38936 - 0.825077*x + 0.176655*x**2 - 0.0182319*x**3 + 0.00067091*x**4\ndef test_error(x):\n    return 3 - 0.6*x + .06*x**2\ndef bayes_error(x):\n    return x + 1 - x\n\nplt.xkcd()\n#frame = plt.gca()\n#frame.axes.xaxis.set_ticklabels([])\nplt.figure(figsize=(10, 8))\nplt.plot(x,squared_bias(x), label='squared bias')\nplt.plot(x, variance(x), label='variance')\nplt.plot(x, training_error(x), label='training error')\nplt.plot(x, test_error(x), label='test error')\nplt.plot(x, bayes_error(x), label='Bayes error')\nplt.legend(loc='upper center')\nplt.xlabel('model flexibility')\nplt.show()\n\n#arbitrary units",
            "title": "(a) Sketch of bias, variance, training, test and Bayes errors"
        },
        {
            "location": "/sols/chapter2/exercise3/#b-why-these-shapes",
            "text": "Squared bias.  This is the error in our model introduced by the difference of our approximation and the true underlying function. A more flexible model will be increasingly similar,  and the squared bias therefore dimishes as the flexibility increases. (It might even reach zero if for example the underlying function is a polynomial and by increasing the flexibility at a certain point we include the polynomials of this degree in our hypothesis space.)  Variance.  In the limit of a model with no flexibility the variance will be zero, since the model fit will be independent of the data. As the flexibility increases the variance will increase as well since the noise in a particular training set will correspondingly captured by the model. The curve described by the variance is an monotonically increasing function of the flexibility of the model.  Training error.  The training error is given by the average (squared) difference between the predictions of the model and the observations. If a model if very unflexible this can be quite high, but as the flexibility increases this difference will decrease. If we consider polynomials for example increasing the flexibility of the model might mean increasing the degree of the polynomial to be fitted. The additional degrees of freedom will decrease the average difference and reduce the training error.  Test error.  Ths expected test error is given by the formula: Variance + Bias + Bayes error, all of which are non-negative. The Bayes error is constant and a lower bound for the test error. The test error has a minimum at an intermediate level of flexibility: not too flexible, so that the variance does not dominate, and not too unflexible, so that the squared bias is not too high. The plot of the test error thus resembles sort of an upward (deformed) parabola: high for unflexible models, decreasing as flexibility increases until it reaches a minimum. Then the variance starts to dominate and the test error starts increasing. The distance between this minimum and the Bayes irreducible error gives us an idea of how well the best function in the hypothesis space will fit.  Bayes error.  This term is constant since by definition it does not depend on X and therefore on the flexibility of the model.",
            "title": "(b) Why these shapes?"
        },
        {
            "location": "/sols/chapter2/exercise4/",
            "text": "Exercise 2.4\n\n\n(a) Classification\n\n\nSpam filters\n. \nAny self-respecting email service has a good spam filter: ideally it keeps any undesired email away from your inbox, and let's through every email you're interested in.\nThe response has just two categories: 'spam' or 'not spam', and the predictors can include several different variables: whether you have corresponded with the sender, whether you have their email address in some of your non-spam emails, the words and sequences of words of the emails (for example, if the email includes any reference to a 'Nigerian prince'), whether similar emails have already been tagged as spam by other users, etc.\nThe main goal is \nprediction\n because the most important task is to accurately be able to predict whether a future email message is spam or not-spam. \nAn important aspect in this case is the rate of false positives and false negatives.\nIn the case of email it is usually much more acceptable to have false negatives (spam that got through to your inbox) than false positives (important email that was classified as spam and you have never noticed). \n\n\nDigit recognition. (or text or speech or facial recognition)\n \nAn important task required everyday around the world is the recognition of handwritten digits and addresses for postal service or document scanning and OCR (Optical Character Recognition).\nDepending on how we define the learning task, the predictors can be the original digital photographs or scans of the documents, or just a cropped, grayscale image of a single digit (that is, a N by M matrix of real numbers, each describing the gray scale value of a single pixel).\nThe response might be one of ten digits (0 to 9) or it might include commas and the full numbers (not just the digits), depending on how we decide to define the task.\nAgain here the main goal is \nprediction\n, since the most important thing is to recognize a correct address or bank account number from a letter or document. \n\n\nSocial analysis\n. Classify people into supporters of \nSporting\n, supporters of \nBenfica\n, neither or both (so the response is one of these four categories).\nAs predictors, include factors such as age, nationality, address, income, education, gender, whether they appreciate classical music, their criminal record, etc.\nHere the main goal is \ninference\n, since more than predicting whether someone supports a specific club we are interested in understanding and studying the different factors and discovering interesting relationships between them.\n\n\nOther examples\n: fraud detection, medical diagnosis, stock prediction (buy, sell, hold), astronomical objects classification, choosing \nstrawberries\n or \ncucumbers\n.\n\n\n(b) Regression\n\n\nGalton's original 'regression' to the mean\n. \nStudy the height of children and its relationship to the height of their parents.\nHere the response is the height of the child and the predictor is the height of both parents (or the mean of both).\nThe main goal would be \ninference\n, since we are trying to better understand, from a scientific perspective, how much genetics influences an individual's height.\nUnless, of course, you are an overly zealous future parent trying to predict the chances of your child playing in the NBA given your own height and your partner's - in which case your main goal might be prediction.\nThis problem is \nwhere the name 'regression' comes from\n and, perhaps surprisingly, \nGalton's statistical method\n has stood the test of time quite well.\n\n\nHouse prices\n. Buying a house is probably the biggest investment that many people do during their life time.\nAccordingly, it is natural that people want to know the value of a house in order to do the best deal possible.\nThe main goal in this cases is \nprediction\n, since we want to predict house's price considering a set of predictors. \nThere are several factors used to predict house's price. \nFor example, a \nKaggle competition\n in which we participated, suggests a total of 79 predictors to predict the final price of each house in Boston.\n\n\nWeather\n.\nThe response can be temperature, wind velocity or precipitation amount in different locations, and the predictors can be the same variables for previous times.\nThe goal is both \nprediction\n (will it rain too much this month and ruin the plantations; will there be enough snow for the snowtrip?) and \ninference\n (what causes hurricanes, or the current climate change?). \n\n\nOther examples\n:\nreturns of investment in stocks, predicting school grades from amount and type of study, predicting the popularity of a book, film, article or tweet.\n\n\n(c) Cluster analysis\n\n\nOutlier detection\n.\nIf you can define a distance between the data points or a density function in the predictors space, you can use cluster analysis to identify candidate outliers such as the data points farther from the majority of the points, or those in sparser regions.\n\n\nMarket research\n. \nUsing cluster analysis, market researchers can group consumers into market segments according to their similarities.\nThis can provide a better understanding on group characteristics, as well as it can provide useful insights about how different groups of consumers/potential customer behave and relate with different groups.\n\n\nRecommender systems\n.\nWill you think watching 'Pulp Fiction' is time well spent?\nIf enough is known about you, including some of your film preferences (and perhaps how many times you have watched 'Stalker' or 'Pulp Fiction') and some similar data about other people, we can have an indication of whether will you think rewatching 'Pulp Fiction' is worth your time...\nAre you 'closer' to the people who think the answer is 'No' or to the ones that think 'Yes'?\n\n\nOther examples\n:\ngene sequencing, data compression, social network analysis, representation learning, topography classification.",
            "title": "2.4"
        },
        {
            "location": "/sols/chapter2/exercise4/#exercise-24",
            "text": "",
            "title": "Exercise 2.4"
        },
        {
            "location": "/sols/chapter2/exercise4/#a-classification",
            "text": "Spam filters . \nAny self-respecting email service has a good spam filter: ideally it keeps any undesired email away from your inbox, and let's through every email you're interested in.\nThe response has just two categories: 'spam' or 'not spam', and the predictors can include several different variables: whether you have corresponded with the sender, whether you have their email address in some of your non-spam emails, the words and sequences of words of the emails (for example, if the email includes any reference to a 'Nigerian prince'), whether similar emails have already been tagged as spam by other users, etc.\nThe main goal is  prediction  because the most important task is to accurately be able to predict whether a future email message is spam or not-spam. \nAn important aspect in this case is the rate of false positives and false negatives.\nIn the case of email it is usually much more acceptable to have false negatives (spam that got through to your inbox) than false positives (important email that was classified as spam and you have never noticed).   Digit recognition. (or text or speech or facial recognition)  \nAn important task required everyday around the world is the recognition of handwritten digits and addresses for postal service or document scanning and OCR (Optical Character Recognition).\nDepending on how we define the learning task, the predictors can be the original digital photographs or scans of the documents, or just a cropped, grayscale image of a single digit (that is, a N by M matrix of real numbers, each describing the gray scale value of a single pixel).\nThe response might be one of ten digits (0 to 9) or it might include commas and the full numbers (not just the digits), depending on how we decide to define the task.\nAgain here the main goal is  prediction , since the most important thing is to recognize a correct address or bank account number from a letter or document.   Social analysis . Classify people into supporters of  Sporting , supporters of  Benfica , neither or both (so the response is one of these four categories).\nAs predictors, include factors such as age, nationality, address, income, education, gender, whether they appreciate classical music, their criminal record, etc.\nHere the main goal is  inference , since more than predicting whether someone supports a specific club we are interested in understanding and studying the different factors and discovering interesting relationships between them.  Other examples : fraud detection, medical diagnosis, stock prediction (buy, sell, hold), astronomical objects classification, choosing  strawberries  or  cucumbers .",
            "title": "(a) Classification"
        },
        {
            "location": "/sols/chapter2/exercise4/#b-regression",
            "text": "Galton's original 'regression' to the mean . \nStudy the height of children and its relationship to the height of their parents.\nHere the response is the height of the child and the predictor is the height of both parents (or the mean of both).\nThe main goal would be  inference , since we are trying to better understand, from a scientific perspective, how much genetics influences an individual's height.\nUnless, of course, you are an overly zealous future parent trying to predict the chances of your child playing in the NBA given your own height and your partner's - in which case your main goal might be prediction.\nThis problem is  where the name 'regression' comes from  and, perhaps surprisingly,  Galton's statistical method  has stood the test of time quite well.  House prices . Buying a house is probably the biggest investment that many people do during their life time.\nAccordingly, it is natural that people want to know the value of a house in order to do the best deal possible.\nThe main goal in this cases is  prediction , since we want to predict house's price considering a set of predictors. \nThere are several factors used to predict house's price. \nFor example, a  Kaggle competition  in which we participated, suggests a total of 79 predictors to predict the final price of each house in Boston.  Weather .\nThe response can be temperature, wind velocity or precipitation amount in different locations, and the predictors can be the same variables for previous times.\nThe goal is both  prediction  (will it rain too much this month and ruin the plantations; will there be enough snow for the snowtrip?) and  inference  (what causes hurricanes, or the current climate change?).   Other examples :\nreturns of investment in stocks, predicting school grades from amount and type of study, predicting the popularity of a book, film, article or tweet.",
            "title": "(b) Regression"
        },
        {
            "location": "/sols/chapter2/exercise4/#c-cluster-analysis",
            "text": "Outlier detection .\nIf you can define a distance between the data points or a density function in the predictors space, you can use cluster analysis to identify candidate outliers such as the data points farther from the majority of the points, or those in sparser regions.  Market research . \nUsing cluster analysis, market researchers can group consumers into market segments according to their similarities.\nThis can provide a better understanding on group characteristics, as well as it can provide useful insights about how different groups of consumers/potential customer behave and relate with different groups.  Recommender systems .\nWill you think watching 'Pulp Fiction' is time well spent?\nIf enough is known about you, including some of your film preferences (and perhaps how many times you have watched 'Stalker' or 'Pulp Fiction') and some similar data about other people, we can have an indication of whether will you think rewatching 'Pulp Fiction' is worth your time...\nAre you 'closer' to the people who think the answer is 'No' or to the ones that think 'Yes'?  Other examples :\ngene sequencing, data compression, social network analysis, representation learning, topography classification.",
            "title": "(c) Cluster analysis"
        },
        {
            "location": "/sols/chapter2/exercise5/",
            "text": "Exercise 2.5\n\n\nA very flexible approach (versus a less flexible)\n\n\nAdvantages\n\n\n\n\nLess bias.\n\n\nGiven enough data, better results.\n\n\n\n\nDisadvantages\n\n\n\n\nMore risk of overfitting.  \n\n\nHarder to train.\n\n\nLonger to train.\n\n\nComputational more demanding.\n\n\nLess clear interpretability.\n\n\n\n\nWhen is one approach preferable?\n\n\nMore flexible\n\n\n\n\nLarge sample size and small number of predictors.\n\n\nNon-linear relationship between the predictors and response.\n\n\n\n\nLess flexible\n\n\n\n\nSmall sample size and large number of predictors.\n\n\nHigh variance of the error terms.",
            "title": "2.5"
        },
        {
            "location": "/sols/chapter2/exercise5/#exercise-25",
            "text": "",
            "title": "Exercise 2.5"
        },
        {
            "location": "/sols/chapter2/exercise5/#a-very-flexible-approach-versus-a-less-flexible",
            "text": "",
            "title": "A very flexible approach (versus a less flexible)"
        },
        {
            "location": "/sols/chapter2/exercise5/#advantages",
            "text": "Less bias.  Given enough data, better results.",
            "title": "Advantages"
        },
        {
            "location": "/sols/chapter2/exercise5/#disadvantages",
            "text": "More risk of overfitting.    Harder to train.  Longer to train.  Computational more demanding.  Less clear interpretability.",
            "title": "Disadvantages"
        },
        {
            "location": "/sols/chapter2/exercise5/#when-is-one-approach-preferable",
            "text": "",
            "title": "When is one approach preferable?"
        },
        {
            "location": "/sols/chapter2/exercise5/#more-flexible",
            "text": "Large sample size and small number of predictors.  Non-linear relationship between the predictors and response.",
            "title": "More flexible"
        },
        {
            "location": "/sols/chapter2/exercise5/#less-flexible",
            "text": "Small sample size and large number of predictors.  High variance of the error terms.",
            "title": "Less flexible"
        },
        {
            "location": "/sols/chapter2/exercise6/",
            "text": "Exercise 2.6\n\n\nThat this exercise does not ask for a precise definition of parametric and non-parametric methods and that the main text does also not provide one (section 2.1.2) is a sign that this definition is not consensual, and in fact it depends on the context in which it is used in statistics [1].\nFor our purposes we follow both the main text and Murphy [2] (and a few others [3-6]), for a definition of a parametric and non-parametric model (and by model, this we mean a probability distribution or density, or a regression function).\nSo, for our purposes, a parametric model means two thin\ngs: (1) strong, explicit assumptions about the form of the model, (2) which is parametrized by a finite, fixed number of parameters that does not change with the amount of data.\n(Yes, the first part of this definition is vague.)\nConversely, a non-parametric model means that, in general, the number of parameters depends on the amount of data and that we try to keep assumptions as weak as possible.\n'Non-parametric' is thus an unfortunate name since it does not mean 'no parameters'.\nOn the contrary, they do contain parameters (often many more) but these tend to determine the model complexity rather than the form.\nThe typical examples of a parametric and non-parametric models are linear regression and KNN, respectively.\nAnother good example, is the one from the text and figures 2.4, 2.5 and 2.6.\n\n\nThe advantages of a parametric approach are, in general, that it requires less observations, is faster and more computationally tractable, and more robust (less misguided by noise).\nThe disadvantages are the stronger assumptions that make the model less flexible, perhaps unable to capture the underlying adequately regardless of the amount of training data.\n\n\nThere are also other types of models such as semi-parametric models [6], which have two components: a parametric and a non-parametric one.\nOne can also have a non-parametric model by using a parametric model but aditionally adapting the number of parameters as needed (say the degree of the polynomial in a linear regression).\n\n\nReferences\n\n\n\n\n[1] https://en.wikipedia.org/wiki/Nonparametric_statistics#Definitions\n\n\n[1] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.\n\n\n[2] Domingos, Pedro. \"A few useful things to know about machine learning.\" Communications of the ACM 55.10 (2012): 78-87.\n\n\n[3] Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013.\n\n\n[4] Bishop, C.M.: Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA (2006)\n\n\n[5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n\n\n[6] Wellner, Jon A., Chris AJ Klaassen, and Ya'acov Ritov. \"Semiparametric models: a review of progress since BKRW (1993).\" (2006): 25-44.",
            "title": "2.6"
        },
        {
            "location": "/sols/chapter2/exercise6/#exercise-26",
            "text": "That this exercise does not ask for a precise definition of parametric and non-parametric methods and that the main text does also not provide one (section 2.1.2) is a sign that this definition is not consensual, and in fact it depends on the context in which it is used in statistics [1].\nFor our purposes we follow both the main text and Murphy [2] (and a few others [3-6]), for a definition of a parametric and non-parametric model (and by model, this we mean a probability distribution or density, or a regression function).\nSo, for our purposes, a parametric model means two thin\ngs: (1) strong, explicit assumptions about the form of the model, (2) which is parametrized by a finite, fixed number of parameters that does not change with the amount of data.\n(Yes, the first part of this definition is vague.)\nConversely, a non-parametric model means that, in general, the number of parameters depends on the amount of data and that we try to keep assumptions as weak as possible.\n'Non-parametric' is thus an unfortunate name since it does not mean 'no parameters'.\nOn the contrary, they do contain parameters (often many more) but these tend to determine the model complexity rather than the form.\nThe typical examples of a parametric and non-parametric models are linear regression and KNN, respectively.\nAnother good example, is the one from the text and figures 2.4, 2.5 and 2.6.  The advantages of a parametric approach are, in general, that it requires less observations, is faster and more computationally tractable, and more robust (less misguided by noise).\nThe disadvantages are the stronger assumptions that make the model less flexible, perhaps unable to capture the underlying adequately regardless of the amount of training data.  There are also other types of models such as semi-parametric models [6], which have two components: a parametric and a non-parametric one.\nOne can also have a non-parametric model by using a parametric model but aditionally adapting the number of parameters as needed (say the degree of the polynomial in a linear regression).",
            "title": "Exercise 2.6"
        },
        {
            "location": "/sols/chapter2/exercise6/#references",
            "text": "[1] https://en.wikipedia.org/wiki/Nonparametric_statistics#Definitions  [1] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.  [2] Domingos, Pedro. \"A few useful things to know about machine learning.\" Communications of the ACM 55.10 (2012): 78-87.  [3] Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013.  [4] Bishop, C.M.: Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA (2006)  [5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.  [6] Wellner, Jon A., Chris AJ Klaassen, and Ya'acov Ritov. \"Semiparametric models: a review of progress since BKRW (1993).\" (2006): 25-44.",
            "title": "References"
        },
        {
            "location": "/sols/chapter2/exercise7/",
            "text": "Exercise 2.7\n\n\nimport numpy as np\nimport pandas as pd\n\nd = {'X1': pd.Series([0,2,0,0,-1,1]),\n     'X2': pd.Series([3,0,1,1,0,1]),\n     'X3': pd.Series([0,0,3,2,1,1]),\n     'Y': pd.Series(['Red','Red','Red','Green','Green','Red'])}\ndf = pd.DataFrame(d)\ndf.index = np.arange(1, len(df) + 1)\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nY\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n0\n\n      \n3\n\n      \n0\n\n      \nRed\n\n    \n\n    \n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \nRed\n\n    \n\n    \n\n      \n3\n\n      \n0\n\n      \n1\n\n      \n3\n\n      \nRed\n\n    \n\n    \n\n      \n4\n\n      \n0\n\n      \n1\n\n      \n2\n\n      \nGreen\n\n    \n\n    \n\n      \n5\n\n      \n-1\n\n      \n0\n\n      \n1\n\n      \nGreen\n\n    \n\n    \n\n      \n6\n\n      \n1\n\n      \n1\n\n      \n1\n\n      \nRed\n\n    \n\n  \n\n\n\n\n\n\n\n(a) Euclidian distance\n\n\nfrom math import sqrt\ndf['distance']=np.sqrt(df['X1']**2+df['X2']**2+df['X3']**2)\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nY\n\n      \ndistance\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n0\n\n      \n3\n\n      \n0\n\n      \nRed\n\n      \n3.000000\n\n    \n\n    \n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \nRed\n\n      \n2.000000\n\n    \n\n    \n\n      \n3\n\n      \n0\n\n      \n1\n\n      \n3\n\n      \nRed\n\n      \n3.162278\n\n    \n\n    \n\n      \n4\n\n      \n0\n\n      \n1\n\n      \n2\n\n      \nGreen\n\n      \n2.236068\n\n    \n\n    \n\n      \n5\n\n      \n-1\n\n      \n0\n\n      \n1\n\n      \nGreen\n\n      \n1.414214\n\n    \n\n    \n\n      \n6\n\n      \n1\n\n      \n1\n\n      \n1\n\n      \nRed\n\n      \n1.732051\n\n    \n\n  \n\n\n\n\n\n\n\n(b) K=1\n\n\ndf.sort_values(['distance'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nY\n\n      \ndistance\n\n    \n\n  \n\n  \n\n    \n\n      \n5\n\n      \n-1\n\n      \n0\n\n      \n1\n\n      \nGreen\n\n      \n1.414214\n\n    \n\n    \n\n      \n6\n\n      \n1\n\n      \n1\n\n      \n1\n\n      \nRed\n\n      \n1.732051\n\n    \n\n    \n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \nRed\n\n      \n2.000000\n\n    \n\n    \n\n      \n4\n\n      \n0\n\n      \n1\n\n      \n2\n\n      \nGreen\n\n      \n2.236068\n\n    \n\n    \n\n      \n1\n\n      \n0\n\n      \n3\n\n      \n0\n\n      \nRed\n\n      \n3.000000\n\n    \n\n    \n\n      \n3\n\n      \n0\n\n      \n1\n\n      \n3\n\n      \nRed\n\n      \n3.162278\n\n    \n\n  \n\n\n\n\n\n\n\nAs we can see by sorting the data by distance to the origin, for K=1, our prediction is \nGreen\n, since that's the value of the nearest neighbor (point 5 at distance 1.41). \n\n\n(c) K=3\n\n\nOn the other hand, for K=3 our prediction is \nRed\n, because that's the mode of the 3 nearest neigbours: Green, Red and Red (points 5, 6 and 2, respectively).\n\n\n(d) Highly non-linear Bayes decision boundary\n\n\nA large value of K leads to a smoother decision boundary, as if the non-linearities where averaged out. This happens because KNN uses majority voting and this means less emphasis on individual points. For a large value of K, we will likely have a decision boundary which varies very little from point to point, since the result of this majority voting would have to change while for most points this will be a large majority. That is, one of its nearest neighbors changing from one class to the other would still leave the majority voting the same. By contrast, when K is very small, the decision boundary will be better able to capture local non-linearities, because it the majority of neighbors can vary significantly from point to point since the are so few neighbors. Accordingly, we would expect \nthe best value of K to be small\n. \n\n\nImagine this simple example: a true linear boundary that splits the plane in two semi-planes (classes A and B), but with an additional enclave in one of the regions. That is, the true model has a small region of, say, class A inside the class B semi-plane. Would we more likely capture this region for small or large K? Say we have 3 neighboring data points in the class A enclave inside the large class B region with say 50 points. If K > 4, each of the 3 points in the enclave will be classified as B, since they always lose by majority voting (unless K is so large, say 100, that many of the points from A semi-plane region enter this voting). If, on the other hand, K < 4, each of the 3 class A points inside the enclave will be classified as class A, and we will capture this non-linearity of the regions.",
            "title": "2.7"
        },
        {
            "location": "/sols/chapter2/exercise7/#exercise-27",
            "text": "import numpy as np\nimport pandas as pd\n\nd = {'X1': pd.Series([0,2,0,0,-1,1]),\n     'X2': pd.Series([3,0,1,1,0,1]),\n     'X3': pd.Series([0,0,3,2,1,1]),\n     'Y': pd.Series(['Red','Red','Red','Green','Green','Red'])}\ndf = pd.DataFrame(d)\ndf.index = np.arange(1, len(df) + 1)\ndf   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       Y \n     \n   \n   \n     \n       1 \n       0 \n       3 \n       0 \n       Red \n     \n     \n       2 \n       2 \n       0 \n       0 \n       Red \n     \n     \n       3 \n       0 \n       1 \n       3 \n       Red \n     \n     \n       4 \n       0 \n       1 \n       2 \n       Green \n     \n     \n       5 \n       -1 \n       0 \n       1 \n       Green \n     \n     \n       6 \n       1 \n       1 \n       1 \n       Red",
            "title": "Exercise 2.7"
        },
        {
            "location": "/sols/chapter2/exercise7/#a-euclidian-distance",
            "text": "from math import sqrt\ndf['distance']=np.sqrt(df['X1']**2+df['X2']**2+df['X3']**2)\ndf   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       Y \n       distance \n     \n   \n   \n     \n       1 \n       0 \n       3 \n       0 \n       Red \n       3.000000 \n     \n     \n       2 \n       2 \n       0 \n       0 \n       Red \n       2.000000 \n     \n     \n       3 \n       0 \n       1 \n       3 \n       Red \n       3.162278 \n     \n     \n       4 \n       0 \n       1 \n       2 \n       Green \n       2.236068 \n     \n     \n       5 \n       -1 \n       0 \n       1 \n       Green \n       1.414214 \n     \n     \n       6 \n       1 \n       1 \n       1 \n       Red \n       1.732051",
            "title": "(a) Euclidian distance"
        },
        {
            "location": "/sols/chapter2/exercise7/#b-k1",
            "text": "df.sort_values(['distance'])   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       Y \n       distance \n     \n   \n   \n     \n       5 \n       -1 \n       0 \n       1 \n       Green \n       1.414214 \n     \n     \n       6 \n       1 \n       1 \n       1 \n       Red \n       1.732051 \n     \n     \n       2 \n       2 \n       0 \n       0 \n       Red \n       2.000000 \n     \n     \n       4 \n       0 \n       1 \n       2 \n       Green \n       2.236068 \n     \n     \n       1 \n       0 \n       3 \n       0 \n       Red \n       3.000000 \n     \n     \n       3 \n       0 \n       1 \n       3 \n       Red \n       3.162278 \n     \n      As we can see by sorting the data by distance to the origin, for K=1, our prediction is  Green , since that's the value of the nearest neighbor (point 5 at distance 1.41).",
            "title": "(b) K=1"
        },
        {
            "location": "/sols/chapter2/exercise7/#c-k3",
            "text": "On the other hand, for K=3 our prediction is  Red , because that's the mode of the 3 nearest neigbours: Green, Red and Red (points 5, 6 and 2, respectively).",
            "title": "(c) K=3"
        },
        {
            "location": "/sols/chapter2/exercise7/#d-highly-non-linear-bayes-decision-boundary",
            "text": "A large value of K leads to a smoother decision boundary, as if the non-linearities where averaged out. This happens because KNN uses majority voting and this means less emphasis on individual points. For a large value of K, we will likely have a decision boundary which varies very little from point to point, since the result of this majority voting would have to change while for most points this will be a large majority. That is, one of its nearest neighbors changing from one class to the other would still leave the majority voting the same. By contrast, when K is very small, the decision boundary will be better able to capture local non-linearities, because it the majority of neighbors can vary significantly from point to point since the are so few neighbors. Accordingly, we would expect  the best value of K to be small .   Imagine this simple example: a true linear boundary that splits the plane in two semi-planes (classes A and B), but with an additional enclave in one of the regions. That is, the true model has a small region of, say, class A inside the class B semi-plane. Would we more likely capture this region for small or large K? Say we have 3 neighboring data points in the class A enclave inside the large class B region with say 50 points. If K > 4, each of the 3 points in the enclave will be classified as B, since they always lose by majority voting (unless K is so large, say 100, that many of the points from A semi-plane region enter this voting). If, on the other hand, K < 4, each of the 3 class A points inside the enclave will be classified as class A, and we will capture this non-linearity of the regions.",
            "title": "(d) Highly non-linear Bayes decision boundary"
        },
        {
            "location": "/sols/chapter2/exercise8/",
            "text": "Exercise 2.8\n\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\npd.options.display.float_format = '{:,.2f}'.format # Print only 2 decimal cases.\n\n\n\n\n(a) Read csv\n\n\ncollege = pd.read_csv(\"../data/College.csv\") # Portable import, works on Windows as well.\ncollege\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nUnnamed: 0\n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nAbilene Christian University\n\n      \nYes\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.10\n\n      \n12\n\n      \n7041\n\n      \n60\n\n    \n\n    \n\n      \n1\n\n      \nAdelphi University\n\n      \nYes\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.20\n\n      \n16\n\n      \n10527\n\n      \n56\n\n    \n\n    \n\n      \n2\n\n      \nAdrian College\n\n      \nYes\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.90\n\n      \n30\n\n      \n8735\n\n      \n54\n\n    \n\n    \n\n      \n3\n\n      \nAgnes Scott College\n\n      \nYes\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.70\n\n      \n37\n\n      \n19016\n\n      \n59\n\n    \n\n    \n\n      \n4\n\n      \nAlaska Pacific University\n\n      \nYes\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.90\n\n      \n2\n\n      \n10922\n\n      \n15\n\n    \n\n    \n\n      \n5\n\n      \nAlbertson College\n\n      \nYes\n\n      \n587\n\n      \n479\n\n      \n158\n\n      \n38\n\n      \n62\n\n      \n678\n\n      \n41\n\n      \n13500\n\n      \n3335\n\n      \n500\n\n      \n675\n\n      \n67\n\n      \n73\n\n      \n9.40\n\n      \n11\n\n      \n9727\n\n      \n55\n\n    \n\n    \n\n      \n6\n\n      \nAlbertus Magnus College\n\n      \nYes\n\n      \n353\n\n      \n340\n\n      \n103\n\n      \n17\n\n      \n45\n\n      \n416\n\n      \n230\n\n      \n13290\n\n      \n5720\n\n      \n500\n\n      \n1500\n\n      \n90\n\n      \n93\n\n      \n11.50\n\n      \n26\n\n      \n8861\n\n      \n63\n\n    \n\n    \n\n      \n7\n\n      \nAlbion College\n\n      \nYes\n\n      \n1899\n\n      \n1720\n\n      \n489\n\n      \n37\n\n      \n68\n\n      \n1594\n\n      \n32\n\n      \n13868\n\n      \n4826\n\n      \n450\n\n      \n850\n\n      \n89\n\n      \n100\n\n      \n13.70\n\n      \n37\n\n      \n11487\n\n      \n73\n\n    \n\n    \n\n      \n8\n\n      \nAlbright College\n\n      \nYes\n\n      \n1038\n\n      \n839\n\n      \n227\n\n      \n30\n\n      \n63\n\n      \n973\n\n      \n306\n\n      \n15595\n\n      \n4400\n\n      \n300\n\n      \n500\n\n      \n79\n\n      \n84\n\n      \n11.30\n\n      \n23\n\n      \n11644\n\n      \n80\n\n    \n\n    \n\n      \n9\n\n      \nAlderson-Broaddus College\n\n      \nYes\n\n      \n582\n\n      \n498\n\n      \n172\n\n      \n21\n\n      \n44\n\n      \n799\n\n      \n78\n\n      \n10468\n\n      \n3380\n\n      \n660\n\n      \n1800\n\n      \n40\n\n      \n41\n\n      \n11.50\n\n      \n15\n\n      \n8991\n\n      \n52\n\n    \n\n    \n\n      \n10\n\n      \nAlfred University\n\n      \nYes\n\n      \n1732\n\n      \n1425\n\n      \n472\n\n      \n37\n\n      \n75\n\n      \n1830\n\n      \n110\n\n      \n16548\n\n      \n5406\n\n      \n500\n\n      \n600\n\n      \n82\n\n      \n88\n\n      \n11.30\n\n      \n31\n\n      \n10932\n\n      \n73\n\n    \n\n    \n\n      \n11\n\n      \nAllegheny College\n\n      \nYes\n\n      \n2652\n\n      \n1900\n\n      \n484\n\n      \n44\n\n      \n77\n\n      \n1707\n\n      \n44\n\n      \n17080\n\n      \n4440\n\n      \n400\n\n      \n600\n\n      \n73\n\n      \n91\n\n      \n9.90\n\n      \n41\n\n      \n11711\n\n      \n76\n\n    \n\n    \n\n      \n12\n\n      \nAllentown Coll. of St. Francis de Sales\n\n      \nYes\n\n      \n1179\n\n      \n780\n\n      \n290\n\n      \n38\n\n      \n64\n\n      \n1130\n\n      \n638\n\n      \n9690\n\n      \n4785\n\n      \n600\n\n      \n1000\n\n      \n60\n\n      \n84\n\n      \n13.30\n\n      \n21\n\n      \n7940\n\n      \n74\n\n    \n\n    \n\n      \n13\n\n      \nAlma College\n\n      \nYes\n\n      \n1267\n\n      \n1080\n\n      \n385\n\n      \n44\n\n      \n73\n\n      \n1306\n\n      \n28\n\n      \n12572\n\n      \n4552\n\n      \n400\n\n      \n400\n\n      \n79\n\n      \n87\n\n      \n15.30\n\n      \n32\n\n      \n9305\n\n      \n68\n\n    \n\n    \n\n      \n14\n\n      \nAlverno College\n\n      \nYes\n\n      \n494\n\n      \n313\n\n      \n157\n\n      \n23\n\n      \n46\n\n      \n1317\n\n      \n1235\n\n      \n8352\n\n      \n3640\n\n      \n650\n\n      \n2449\n\n      \n36\n\n      \n69\n\n      \n11.10\n\n      \n26\n\n      \n8127\n\n      \n55\n\n    \n\n    \n\n      \n15\n\n      \nAmerican International College\n\n      \nYes\n\n      \n1420\n\n      \n1093\n\n      \n220\n\n      \n9\n\n      \n22\n\n      \n1018\n\n      \n287\n\n      \n8700\n\n      \n4780\n\n      \n450\n\n      \n1400\n\n      \n78\n\n      \n84\n\n      \n14.70\n\n      \n19\n\n      \n7355\n\n      \n69\n\n    \n\n    \n\n      \n16\n\n      \nAmherst College\n\n      \nYes\n\n      \n4302\n\n      \n992\n\n      \n418\n\n      \n83\n\n      \n96\n\n      \n1593\n\n      \n5\n\n      \n19760\n\n      \n5300\n\n      \n660\n\n      \n1598\n\n      \n93\n\n      \n98\n\n      \n8.40\n\n      \n63\n\n      \n21424\n\n      \n100\n\n    \n\n    \n\n      \n17\n\n      \nAnderson University\n\n      \nYes\n\n      \n1216\n\n      \n908\n\n      \n423\n\n      \n19\n\n      \n40\n\n      \n1819\n\n      \n281\n\n      \n10100\n\n      \n3520\n\n      \n550\n\n      \n1100\n\n      \n48\n\n      \n61\n\n      \n12.10\n\n      \n14\n\n      \n7994\n\n      \n59\n\n    \n\n    \n\n      \n18\n\n      \nAndrews University\n\n      \nYes\n\n      \n1130\n\n      \n704\n\n      \n322\n\n      \n14\n\n      \n23\n\n      \n1586\n\n      \n326\n\n      \n9996\n\n      \n3090\n\n      \n900\n\n      \n1320\n\n      \n62\n\n      \n66\n\n      \n11.50\n\n      \n18\n\n      \n10908\n\n      \n46\n\n    \n\n    \n\n      \n19\n\n      \nAngelo State University\n\n      \nNo\n\n      \n3540\n\n      \n2001\n\n      \n1016\n\n      \n24\n\n      \n54\n\n      \n4190\n\n      \n1512\n\n      \n5130\n\n      \n3592\n\n      \n500\n\n      \n2000\n\n      \n60\n\n      \n62\n\n      \n23.10\n\n      \n5\n\n      \n4010\n\n      \n34\n\n    \n\n    \n\n      \n20\n\n      \nAntioch University\n\n      \nYes\n\n      \n713\n\n      \n661\n\n      \n252\n\n      \n25\n\n      \n44\n\n      \n712\n\n      \n23\n\n      \n15476\n\n      \n3336\n\n      \n400\n\n      \n1100\n\n      \n69\n\n      \n82\n\n      \n11.30\n\n      \n35\n\n      \n42926\n\n      \n48\n\n    \n\n    \n\n      \n21\n\n      \nAppalachian State University\n\n      \nNo\n\n      \n7313\n\n      \n4664\n\n      \n1910\n\n      \n20\n\n      \n63\n\n      \n9940\n\n      \n1035\n\n      \n6806\n\n      \n2540\n\n      \n96\n\n      \n2000\n\n      \n83\n\n      \n96\n\n      \n18.30\n\n      \n14\n\n      \n5854\n\n      \n70\n\n    \n\n    \n\n      \n22\n\n      \nAquinas College\n\n      \nYes\n\n      \n619\n\n      \n516\n\n      \n219\n\n      \n20\n\n      \n51\n\n      \n1251\n\n      \n767\n\n      \n11208\n\n      \n4124\n\n      \n350\n\n      \n1615\n\n      \n55\n\n      \n65\n\n      \n12.70\n\n      \n25\n\n      \n6584\n\n      \n65\n\n    \n\n    \n\n      \n23\n\n      \nArizona State University Main campus\n\n      \nNo\n\n      \n12809\n\n      \n10308\n\n      \n3761\n\n      \n24\n\n      \n49\n\n      \n22593\n\n      \n7585\n\n      \n7434\n\n      \n4850\n\n      \n700\n\n      \n2100\n\n      \n88\n\n      \n93\n\n      \n18.90\n\n      \n5\n\n      \n4602\n\n      \n48\n\n    \n\n    \n\n      \n24\n\n      \nArkansas College (Lyon College)\n\n      \nYes\n\n      \n708\n\n      \n334\n\n      \n166\n\n      \n46\n\n      \n74\n\n      \n530\n\n      \n182\n\n      \n8644\n\n      \n3922\n\n      \n500\n\n      \n800\n\n      \n79\n\n      \n88\n\n      \n12.60\n\n      \n24\n\n      \n14579\n\n      \n54\n\n    \n\n    \n\n      \n25\n\n      \nArkansas Tech University\n\n      \nNo\n\n      \n1734\n\n      \n1729\n\n      \n951\n\n      \n12\n\n      \n52\n\n      \n3602\n\n      \n939\n\n      \n3460\n\n      \n2650\n\n      \n450\n\n      \n1000\n\n      \n57\n\n      \n60\n\n      \n19.60\n\n      \n5\n\n      \n4739\n\n      \n48\n\n    \n\n    \n\n      \n26\n\n      \nAssumption College\n\n      \nYes\n\n      \n2135\n\n      \n1700\n\n      \n491\n\n      \n23\n\n      \n59\n\n      \n1708\n\n      \n689\n\n      \n12000\n\n      \n5920\n\n      \n500\n\n      \n500\n\n      \n93\n\n      \n93\n\n      \n13.80\n\n      \n30\n\n      \n7100\n\n      \n88\n\n    \n\n    \n\n      \n27\n\n      \nAuburn University-Main Campus\n\n      \nNo\n\n      \n7548\n\n      \n6791\n\n      \n3070\n\n      \n25\n\n      \n57\n\n      \n16262\n\n      \n1716\n\n      \n6300\n\n      \n3933\n\n      \n600\n\n      \n1908\n\n      \n85\n\n      \n91\n\n      \n16.70\n\n      \n18\n\n      \n6642\n\n      \n69\n\n    \n\n    \n\n      \n28\n\n      \nAugsburg College\n\n      \nYes\n\n      \n662\n\n      \n513\n\n      \n257\n\n      \n12\n\n      \n30\n\n      \n2074\n\n      \n726\n\n      \n11902\n\n      \n4372\n\n      \n540\n\n      \n950\n\n      \n65\n\n      \n65\n\n      \n12.80\n\n      \n31\n\n      \n7836\n\n      \n58\n\n    \n\n    \n\n      \n29\n\n      \nAugustana College IL\n\n      \nYes\n\n      \n1879\n\n      \n1658\n\n      \n497\n\n      \n36\n\n      \n69\n\n      \n1950\n\n      \n38\n\n      \n13353\n\n      \n4173\n\n      \n540\n\n      \n821\n\n      \n78\n\n      \n83\n\n      \n12.70\n\n      \n40\n\n      \n9220\n\n      \n71\n\n    \n\n    \n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n    \n\n    \n\n      \n747\n\n      \nWestfield State College\n\n      \nNo\n\n      \n3100\n\n      \n2150\n\n      \n825\n\n      \n3\n\n      \n20\n\n      \n3234\n\n      \n941\n\n      \n5542\n\n      \n3788\n\n      \n500\n\n      \n1300\n\n      \n75\n\n      \n79\n\n      \n15.70\n\n      \n20\n\n      \n4222\n\n      \n65\n\n    \n\n    \n\n      \n748\n\n      \nWestminster College MO\n\n      \nYes\n\n      \n662\n\n      \n553\n\n      \n184\n\n      \n20\n\n      \n43\n\n      \n665\n\n      \n37\n\n      \n10720\n\n      \n4050\n\n      \n600\n\n      \n1650\n\n      \n66\n\n      \n70\n\n      \n12.50\n\n      \n20\n\n      \n7925\n\n      \n62\n\n    \n\n    \n\n      \n749\n\n      \nWestminster College\n\n      \nYes\n\n      \n996\n\n      \n866\n\n      \n377\n\n      \n29\n\n      \n58\n\n      \n1411\n\n      \n72\n\n      \n12065\n\n      \n3615\n\n      \n430\n\n      \n685\n\n      \n62\n\n      \n78\n\n      \n12.50\n\n      \n41\n\n      \n8596\n\n      \n80\n\n    \n\n    \n\n      \n750\n\n      \nWestminster College of Salt Lake City\n\n      \nYes\n\n      \n917\n\n      \n720\n\n      \n213\n\n      \n21\n\n      \n60\n\n      \n979\n\n      \n743\n\n      \n8820\n\n      \n4050\n\n      \n600\n\n      \n2025\n\n      \n68\n\n      \n83\n\n      \n10.50\n\n      \n34\n\n      \n7170\n\n      \n50\n\n    \n\n    \n\n      \n751\n\n      \nWestmont College\n\n      \nNo\n\n      \n950\n\n      \n713\n\n      \n351\n\n      \n42\n\n      \n72\n\n      \n1276\n\n      \n9\n\n      \n14320\n\n      \n5304\n\n      \n490\n\n      \n1410\n\n      \n77\n\n      \n77\n\n      \n14.90\n\n      \n17\n\n      \n8837\n\n      \n87\n\n    \n\n    \n\n      \n752\n\n      \nWheaton College IL\n\n      \nYes\n\n      \n1432\n\n      \n920\n\n      \n548\n\n      \n56\n\n      \n84\n\n      \n2200\n\n      \n56\n\n      \n11480\n\n      \n4200\n\n      \n530\n\n      \n1400\n\n      \n81\n\n      \n83\n\n      \n12.70\n\n      \n40\n\n      \n11916\n\n      \n85\n\n    \n\n    \n\n      \n753\n\n      \nWestminster College PA\n\n      \nYes\n\n      \n1738\n\n      \n1373\n\n      \n417\n\n      \n21\n\n      \n55\n\n      \n1335\n\n      \n30\n\n      \n18460\n\n      \n5970\n\n      \n700\n\n      \n850\n\n      \n92\n\n      \n96\n\n      \n13.20\n\n      \n41\n\n      \n22704\n\n      \n71\n\n    \n\n    \n\n      \n754\n\n      \nWheeling Jesuit College\n\n      \nYes\n\n      \n903\n\n      \n755\n\n      \n213\n\n      \n15\n\n      \n49\n\n      \n971\n\n      \n305\n\n      \n10500\n\n      \n4545\n\n      \n600\n\n      \n600\n\n      \n66\n\n      \n71\n\n      \n14.10\n\n      \n27\n\n      \n7494\n\n      \n72\n\n    \n\n    \n\n      \n755\n\n      \nWhitman College\n\n      \nYes\n\n      \n1861\n\n      \n998\n\n      \n359\n\n      \n45\n\n      \n77\n\n      \n1220\n\n      \n46\n\n      \n16670\n\n      \n4900\n\n      \n750\n\n      \n800\n\n      \n80\n\n      \n83\n\n      \n10.50\n\n      \n51\n\n      \n13198\n\n      \n72\n\n    \n\n    \n\n      \n756\n\n      \nWhittier College\n\n      \nYes\n\n      \n1681\n\n      \n1069\n\n      \n344\n\n      \n35\n\n      \n63\n\n      \n1235\n\n      \n30\n\n      \n16249\n\n      \n5699\n\n      \n500\n\n      \n1998\n\n      \n84\n\n      \n92\n\n      \n13.60\n\n      \n29\n\n      \n11778\n\n      \n52\n\n    \n\n    \n\n      \n757\n\n      \nWhitworth College\n\n      \nYes\n\n      \n1121\n\n      \n926\n\n      \n372\n\n      \n43\n\n      \n70\n\n      \n1270\n\n      \n160\n\n      \n12660\n\n      \n4500\n\n      \n678\n\n      \n2424\n\n      \n80\n\n      \n80\n\n      \n16.90\n\n      \n20\n\n      \n8328\n\n      \n80\n\n    \n\n    \n\n      \n758\n\n      \nWidener University\n\n      \nYes\n\n      \n2139\n\n      \n1492\n\n      \n502\n\n      \n24\n\n      \n64\n\n      \n2186\n\n      \n2171\n\n      \n12350\n\n      \n5370\n\n      \n500\n\n      \n1350\n\n      \n88\n\n      \n86\n\n      \n12.60\n\n      \n19\n\n      \n9603\n\n      \n63\n\n    \n\n    \n\n      \n759\n\n      \nWilkes University\n\n      \nYes\n\n      \n1631\n\n      \n1431\n\n      \n434\n\n      \n15\n\n      \n36\n\n      \n1803\n\n      \n603\n\n      \n11150\n\n      \n5130\n\n      \n550\n\n      \n1260\n\n      \n78\n\n      \n92\n\n      \n13.30\n\n      \n24\n\n      \n8543\n\n      \n67\n\n    \n\n    \n\n      \n760\n\n      \nWillamette University\n\n      \nYes\n\n      \n1658\n\n      \n1327\n\n      \n395\n\n      \n49\n\n      \n80\n\n      \n1595\n\n      \n159\n\n      \n14800\n\n      \n4620\n\n      \n400\n\n      \n790\n\n      \n91\n\n      \n94\n\n      \n13.30\n\n      \n37\n\n      \n10779\n\n      \n68\n\n    \n\n    \n\n      \n761\n\n      \nWilliam Jewell College\n\n      \nYes\n\n      \n663\n\n      \n547\n\n      \n315\n\n      \n32\n\n      \n67\n\n      \n1279\n\n      \n75\n\n      \n10060\n\n      \n2970\n\n      \n500\n\n      \n2600\n\n      \n74\n\n      \n80\n\n      \n11.20\n\n      \n19\n\n      \n7885\n\n      \n59\n\n    \n\n    \n\n      \n762\n\n      \nWilliam Woods University\n\n      \nYes\n\n      \n469\n\n      \n435\n\n      \n227\n\n      \n17\n\n      \n39\n\n      \n851\n\n      \n120\n\n      \n10535\n\n      \n4365\n\n      \n550\n\n      \n3700\n\n      \n39\n\n      \n66\n\n      \n12.90\n\n      \n16\n\n      \n7438\n\n      \n52\n\n    \n\n    \n\n      \n763\n\n      \nWilliams College\n\n      \nYes\n\n      \n4186\n\n      \n1245\n\n      \n526\n\n      \n81\n\n      \n96\n\n      \n1988\n\n      \n29\n\n      \n19629\n\n      \n5790\n\n      \n500\n\n      \n1200\n\n      \n94\n\n      \n99\n\n      \n9.00\n\n      \n64\n\n      \n22014\n\n      \n99\n\n    \n\n    \n\n      \n764\n\n      \nWilson College\n\n      \nYes\n\n      \n167\n\n      \n130\n\n      \n46\n\n      \n16\n\n      \n50\n\n      \n199\n\n      \n676\n\n      \n11428\n\n      \n5084\n\n      \n450\n\n      \n475\n\n      \n67\n\n      \n76\n\n      \n8.30\n\n      \n43\n\n      \n10291\n\n      \n67\n\n    \n\n    \n\n      \n765\n\n      \nWingate College\n\n      \nYes\n\n      \n1239\n\n      \n1017\n\n      \n383\n\n      \n10\n\n      \n34\n\n      \n1207\n\n      \n157\n\n      \n7820\n\n      \n3400\n\n      \n550\n\n      \n1550\n\n      \n69\n\n      \n81\n\n      \n13.90\n\n      \n8\n\n      \n7264\n\n      \n91\n\n    \n\n    \n\n      \n766\n\n      \nWinona State University\n\n      \nNo\n\n      \n3325\n\n      \n2047\n\n      \n1301\n\n      \n20\n\n      \n45\n\n      \n5800\n\n      \n872\n\n      \n4200\n\n      \n2700\n\n      \n300\n\n      \n1200\n\n      \n53\n\n      \n60\n\n      \n20.20\n\n      \n18\n\n      \n5318\n\n      \n58\n\n    \n\n    \n\n      \n767\n\n      \nWinthrop University\n\n      \nNo\n\n      \n2320\n\n      \n1805\n\n      \n769\n\n      \n24\n\n      \n61\n\n      \n3395\n\n      \n670\n\n      \n6400\n\n      \n3392\n\n      \n580\n\n      \n2150\n\n      \n71\n\n      \n80\n\n      \n12.80\n\n      \n26\n\n      \n6729\n\n      \n59\n\n    \n\n    \n\n      \n768\n\n      \nWisconsin Lutheran College\n\n      \nYes\n\n      \n152\n\n      \n128\n\n      \n75\n\n      \n17\n\n      \n41\n\n      \n282\n\n      \n22\n\n      \n9100\n\n      \n3700\n\n      \n500\n\n      \n1400\n\n      \n48\n\n      \n48\n\n      \n8.50\n\n      \n26\n\n      \n8960\n\n      \n50\n\n    \n\n    \n\n      \n769\n\n      \nWittenberg University\n\n      \nYes\n\n      \n1979\n\n      \n1739\n\n      \n575\n\n      \n42\n\n      \n68\n\n      \n1980\n\n      \n144\n\n      \n15948\n\n      \n4404\n\n      \n400\n\n      \n800\n\n      \n82\n\n      \n95\n\n      \n12.80\n\n      \n29\n\n      \n10414\n\n      \n78\n\n    \n\n    \n\n      \n770\n\n      \nWofford College\n\n      \nYes\n\n      \n1501\n\n      \n935\n\n      \n273\n\n      \n51\n\n      \n83\n\n      \n1059\n\n      \n34\n\n      \n12680\n\n      \n4150\n\n      \n605\n\n      \n1440\n\n      \n91\n\n      \n92\n\n      \n15.30\n\n      \n42\n\n      \n7875\n\n      \n75\n\n    \n\n    \n\n      \n771\n\n      \nWorcester Polytechnic Institute\n\n      \nYes\n\n      \n2768\n\n      \n2314\n\n      \n682\n\n      \n49\n\n      \n86\n\n      \n2802\n\n      \n86\n\n      \n15884\n\n      \n5370\n\n      \n530\n\n      \n730\n\n      \n92\n\n      \n94\n\n      \n15.20\n\n      \n34\n\n      \n10774\n\n      \n82\n\n    \n\n    \n\n      \n772\n\n      \nWorcester State College\n\n      \nNo\n\n      \n2197\n\n      \n1515\n\n      \n543\n\n      \n4\n\n      \n26\n\n      \n3089\n\n      \n2029\n\n      \n6797\n\n      \n3900\n\n      \n500\n\n      \n1200\n\n      \n60\n\n      \n60\n\n      \n21.00\n\n      \n14\n\n      \n4469\n\n      \n40\n\n    \n\n    \n\n      \n773\n\n      \nXavier University\n\n      \nYes\n\n      \n1959\n\n      \n1805\n\n      \n695\n\n      \n24\n\n      \n47\n\n      \n2849\n\n      \n1107\n\n      \n11520\n\n      \n4960\n\n      \n600\n\n      \n1250\n\n      \n73\n\n      \n75\n\n      \n13.30\n\n      \n31\n\n      \n9189\n\n      \n83\n\n    \n\n    \n\n      \n774\n\n      \nXavier University of Louisiana\n\n      \nYes\n\n      \n2097\n\n      \n1915\n\n      \n695\n\n      \n34\n\n      \n61\n\n      \n2793\n\n      \n166\n\n      \n6900\n\n      \n4200\n\n      \n617\n\n      \n781\n\n      \n67\n\n      \n75\n\n      \n14.40\n\n      \n20\n\n      \n8323\n\n      \n49\n\n    \n\n    \n\n      \n775\n\n      \nYale University\n\n      \nYes\n\n      \n10705\n\n      \n2453\n\n      \n1317\n\n      \n95\n\n      \n99\n\n      \n5217\n\n      \n83\n\n      \n19840\n\n      \n6510\n\n      \n630\n\n      \n2115\n\n      \n96\n\n      \n96\n\n      \n5.80\n\n      \n49\n\n      \n40386\n\n      \n99\n\n    \n\n    \n\n      \n776\n\n      \nYork College of Pennsylvania\n\n      \nYes\n\n      \n2989\n\n      \n1855\n\n      \n691\n\n      \n28\n\n      \n63\n\n      \n2988\n\n      \n1726\n\n      \n4990\n\n      \n3560\n\n      \n500\n\n      \n1250\n\n      \n75\n\n      \n75\n\n      \n18.10\n\n      \n28\n\n      \n4509\n\n      \n99\n\n    \n\n  \n\n\n\n\n777 rows \u00d7 19 columns\n\n\n\n\n\n(b) University names as index\n\n\nThe fix() function in R (similar to edit()) allows on-the-fly edit to the dataframe by invoking an editor.\nFurther details can be found \nhere\n and \nhere\n.\n\n\n# [1]\ncollege = college.set_index(\"Unnamed: 0\") # The default option 'drop=True', deletes the column\ncollege.index.name = 'Names'\ncollege.head()\n# The empty row below the columns names (e.g. Private, Apps, etc.) is there because the index has a name and that creates an additional row.\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n    \n\n      \nNames\n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \nAbilene Christian University\n\n      \nYes\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.10\n\n      \n12\n\n      \n7041\n\n      \n60\n\n    \n\n    \n\n      \nAdelphi University\n\n      \nYes\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.20\n\n      \n16\n\n      \n10527\n\n      \n56\n\n    \n\n    \n\n      \nAdrian College\n\n      \nYes\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.90\n\n      \n30\n\n      \n8735\n\n      \n54\n\n    \n\n    \n\n      \nAgnes Scott College\n\n      \nYes\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.70\n\n      \n37\n\n      \n19016\n\n      \n59\n\n    \n\n    \n\n      \nAlaska Pacific University\n\n      \nYes\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.90\n\n      \n2\n\n      \n10922\n\n      \n15\n\n    \n\n  \n\n\n\n\n\n\n\n[1] \nhttps://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/advanced-indexing?ex=1\n \n\n\n# Alternative solution: We could have done this all in one less line with:\ncollege = pd.read_csv('../data/College.csv', index_col='Unnamed: 0')\ncollege.index.name = 'Names'\ncollege.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n    \n\n      \nNames\n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \nAbilene Christian University\n\n      \nYes\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.10\n\n      \n12\n\n      \n7041\n\n      \n60\n\n    \n\n    \n\n      \nAdelphi University\n\n      \nYes\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.20\n\n      \n16\n\n      \n10527\n\n      \n56\n\n    \n\n    \n\n      \nAdrian College\n\n      \nYes\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.90\n\n      \n30\n\n      \n8735\n\n      \n54\n\n    \n\n    \n\n      \nAgnes Scott College\n\n      \nYes\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.70\n\n      \n37\n\n      \n19016\n\n      \n59\n\n    \n\n    \n\n      \nAlaska Pacific University\n\n      \nYes\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.90\n\n      \n2\n\n      \n10922\n\n      \n15\n\n    \n\n  \n\n\n\n\n\n\n\n(c)\n\n\ni. Summary\n\n\ncollege.describe(include='all')\n# [2, 3, 4] Without the 'all' option, the column 'Private' is not shown because it is categorical\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n777\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n    \n\n    \n\n      \nunique\n\n      \n2\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n    \n\n    \n\n      \ntop\n\n      \nYes\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n    \n\n    \n\n      \nfreq\n\n      \n565\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n      \nnan\n\n    \n\n    \n\n      \nmean\n\n      \nNaN\n\n      \n3,001.64\n\n      \n2,018.80\n\n      \n779.97\n\n      \n27.56\n\n      \n55.80\n\n      \n3,699.91\n\n      \n855.30\n\n      \n10,440.67\n\n      \n4,357.53\n\n      \n549.38\n\n      \n1,340.64\n\n      \n72.66\n\n      \n79.70\n\n      \n14.09\n\n      \n22.74\n\n      \n9,660.17\n\n      \n65.46\n\n    \n\n    \n\n      \nstd\n\n      \nNaN\n\n      \n3,870.20\n\n      \n2,451.11\n\n      \n929.18\n\n      \n17.64\n\n      \n19.80\n\n      \n4,850.42\n\n      \n1,522.43\n\n      \n4,023.02\n\n      \n1,096.70\n\n      \n165.11\n\n      \n677.07\n\n      \n16.33\n\n      \n14.72\n\n      \n3.96\n\n      \n12.39\n\n      \n5,221.77\n\n      \n17.18\n\n    \n\n    \n\n      \nmin\n\n      \nNaN\n\n      \n81.00\n\n      \n72.00\n\n      \n35.00\n\n      \n1.00\n\n      \n9.00\n\n      \n139.00\n\n      \n1.00\n\n      \n2,340.00\n\n      \n1,780.00\n\n      \n96.00\n\n      \n250.00\n\n      \n8.00\n\n      \n24.00\n\n      \n2.50\n\n      \n0.00\n\n      \n3,186.00\n\n      \n10.00\n\n    \n\n    \n\n      \n25%\n\n      \nNaN\n\n      \n776.00\n\n      \n604.00\n\n      \n242.00\n\n      \n15.00\n\n      \n41.00\n\n      \n992.00\n\n      \n95.00\n\n      \n7,320.00\n\n      \n3,597.00\n\n      \n470.00\n\n      \n850.00\n\n      \n62.00\n\n      \n71.00\n\n      \n11.50\n\n      \n13.00\n\n      \n6,751.00\n\n      \n53.00\n\n    \n\n    \n\n      \n50%\n\n      \nNaN\n\n      \n1,558.00\n\n      \n1,110.00\n\n      \n434.00\n\n      \n23.00\n\n      \n54.00\n\n      \n1,707.00\n\n      \n353.00\n\n      \n9,990.00\n\n      \n4,200.00\n\n      \n500.00\n\n      \n1,200.00\n\n      \n75.00\n\n      \n82.00\n\n      \n13.60\n\n      \n21.00\n\n      \n8,377.00\n\n      \n65.00\n\n    \n\n    \n\n      \n75%\n\n      \nNaN\n\n      \n3,624.00\n\n      \n2,424.00\n\n      \n902.00\n\n      \n35.00\n\n      \n69.00\n\n      \n4,005.00\n\n      \n967.00\n\n      \n12,925.00\n\n      \n5,050.00\n\n      \n600.00\n\n      \n1,700.00\n\n      \n85.00\n\n      \n92.00\n\n      \n16.50\n\n      \n31.00\n\n      \n10,830.00\n\n      \n78.00\n\n    \n\n    \n\n      \nmax\n\n      \nNaN\n\n      \n48,094.00\n\n      \n26,330.00\n\n      \n6,392.00\n\n      \n96.00\n\n      \n100.00\n\n      \n31,643.00\n\n      \n21,836.00\n\n      \n21,700.00\n\n      \n8,124.00\n\n      \n2,340.00\n\n      \n6,800.00\n\n      \n103.00\n\n      \n100.00\n\n      \n39.80\n\n      \n64.00\n\n      \n56,233.00\n\n      \n118.00\n\n    \n\n  \n\n\n\n\n\n\n\n# Alternative solution: call describe twice. One on number, and another on object.\ncollege.describe(include=['number'])\n# or college.describe(include=[np.number])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n      \n777.00\n\n    \n\n    \n\n      \nmean\n\n      \n3,001.64\n\n      \n2,018.80\n\n      \n779.97\n\n      \n27.56\n\n      \n55.80\n\n      \n3,699.91\n\n      \n855.30\n\n      \n10,440.67\n\n      \n4,357.53\n\n      \n549.38\n\n      \n1,340.64\n\n      \n72.66\n\n      \n79.70\n\n      \n14.09\n\n      \n22.74\n\n      \n9,660.17\n\n      \n65.46\n\n    \n\n    \n\n      \nstd\n\n      \n3,870.20\n\n      \n2,451.11\n\n      \n929.18\n\n      \n17.64\n\n      \n19.80\n\n      \n4,850.42\n\n      \n1,522.43\n\n      \n4,023.02\n\n      \n1,096.70\n\n      \n165.11\n\n      \n677.07\n\n      \n16.33\n\n      \n14.72\n\n      \n3.96\n\n      \n12.39\n\n      \n5,221.77\n\n      \n17.18\n\n    \n\n    \n\n      \nmin\n\n      \n81.00\n\n      \n72.00\n\n      \n35.00\n\n      \n1.00\n\n      \n9.00\n\n      \n139.00\n\n      \n1.00\n\n      \n2,340.00\n\n      \n1,780.00\n\n      \n96.00\n\n      \n250.00\n\n      \n8.00\n\n      \n24.00\n\n      \n2.50\n\n      \n0.00\n\n      \n3,186.00\n\n      \n10.00\n\n    \n\n    \n\n      \n25%\n\n      \n776.00\n\n      \n604.00\n\n      \n242.00\n\n      \n15.00\n\n      \n41.00\n\n      \n992.00\n\n      \n95.00\n\n      \n7,320.00\n\n      \n3,597.00\n\n      \n470.00\n\n      \n850.00\n\n      \n62.00\n\n      \n71.00\n\n      \n11.50\n\n      \n13.00\n\n      \n6,751.00\n\n      \n53.00\n\n    \n\n    \n\n      \n50%\n\n      \n1,558.00\n\n      \n1,110.00\n\n      \n434.00\n\n      \n23.00\n\n      \n54.00\n\n      \n1,707.00\n\n      \n353.00\n\n      \n9,990.00\n\n      \n4,200.00\n\n      \n500.00\n\n      \n1,200.00\n\n      \n75.00\n\n      \n82.00\n\n      \n13.60\n\n      \n21.00\n\n      \n8,377.00\n\n      \n65.00\n\n    \n\n    \n\n      \n75%\n\n      \n3,624.00\n\n      \n2,424.00\n\n      \n902.00\n\n      \n35.00\n\n      \n69.00\n\n      \n4,005.00\n\n      \n967.00\n\n      \n12,925.00\n\n      \n5,050.00\n\n      \n600.00\n\n      \n1,700.00\n\n      \n85.00\n\n      \n92.00\n\n      \n16.50\n\n      \n31.00\n\n      \n10,830.00\n\n      \n78.00\n\n    \n\n    \n\n      \nmax\n\n      \n48,094.00\n\n      \n26,330.00\n\n      \n6,392.00\n\n      \n96.00\n\n      \n100.00\n\n      \n31,643.00\n\n      \n21,836.00\n\n      \n21,700.00\n\n      \n8,124.00\n\n      \n2,340.00\n\n      \n6,800.00\n\n      \n103.00\n\n      \n100.00\n\n      \n39.80\n\n      \n64.00\n\n      \n56,233.00\n\n      \n118.00\n\n    \n\n  \n\n\n\n\n\n\n\ncollege.describe(include=['object'])\n# or college.describe(include=['O'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n777\n\n    \n\n    \n\n      \nunique\n\n      \n2\n\n    \n\n    \n\n      \ntop\n\n      \nYes\n\n    \n\n    \n\n      \nfreq\n\n      \n565\n\n    \n\n  \n\n\n\n\n\n\n\n\n\n[2] \nhttp://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns\n\n\n[3] \nhttp://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns\n\n\n[4] \nhttp://dataanalysispython.readthedocs.io/en/latest/pandas.html#summarizing-data-describe\n\n\n\n\nii. Pair plot\n\n\nUnlike R, seaborn does not pairplot categorical vs numerical. See more \nhere\n.\n\n\ng = sns.PairGrid(college, vars=college.iloc[:,1:11], hue='Private')\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)\n\n\n\n\n\n\niii. Box plots\n\n\nsns.boxplot(x='Private', y='Outstate', data=college);\n\n\n\n\n\n\niv. Elite variable\n\n\ncollege.loc[college['Top10perc']>50, 'Elite'] = 'Yes'\ncollege['Elite'] = college['Elite'].fillna('No')\n\nsns.boxplot(x='Elite', y='Outstate', data=college);\n\n\n\n\n\n\nv. Histograms\n\n\nIn Python, to produce some histograms with differing numbers of bins for quantitative variables, we first need to convert these variables to bins.\nWhen we create bins, we transform a continuous range of values into a discrete one. For the purposes of this exercise, we will only consider equal-width bins.\n\n\n# Bins creation\ncollege['PhD'] = pd.cut(college['PhD'], 3, labels=['Low', 'Medium', 'High'])\ncollege['Grad.Rate'] = pd.cut(college['Grad.Rate'], 5, labels=['Very low', 'Low', 'Medium', 'High', 'Very high'])\ncollege['Books'] = pd.cut(college['Books'], 2, labels=['Low', 'High'])\ncollege['Enroll'] = pd.cut(college['Enroll'], 4, labels=['Very low', 'Low', 'High', 'Very high'])\n\n\n\n\n# Plot histograms\nfig = plt.figure()\n\nplt.subplot(221)\ncollege['PhD'].value_counts().plot(kind='bar', title = 'Private');\nplt.subplot(222)\ncollege['Grad.Rate'].value_counts().plot(kind='bar', title = 'Grad.Rate');\nplt.subplot(223)\ncollege['Books'].value_counts().plot(kind='bar', title = 'Books');\nplt.subplot(224)\ncollege['Enroll'].value_counts().plot(kind='bar', title = 'Enroll');\n\nfig.subplots_adjust(hspace=1) # To add space between subplots\n\n\n\n\n\n\nvi. Continue exploring the data\n\n\n\"This exercise is \ntrivial\n and is left to the reader.\" :)",
            "title": "2.8"
        },
        {
            "location": "/sols/chapter2/exercise8/#exercise-28",
            "text": "%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\npd.options.display.float_format = '{:,.2f}'.format # Print only 2 decimal cases.",
            "title": "Exercise 2.8"
        },
        {
            "location": "/sols/chapter2/exercise8/#a-read-csv",
            "text": "college = pd.read_csv(\"../data/College.csv\") # Portable import, works on Windows as well.\ncollege   \n   \n     \n       \n       Unnamed: 0 \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n   \n   \n     \n       0 \n       Abilene Christian University \n       Yes \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.10 \n       12 \n       7041 \n       60 \n     \n     \n       1 \n       Adelphi University \n       Yes \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.20 \n       16 \n       10527 \n       56 \n     \n     \n       2 \n       Adrian College \n       Yes \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.90 \n       30 \n       8735 \n       54 \n     \n     \n       3 \n       Agnes Scott College \n       Yes \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.70 \n       37 \n       19016 \n       59 \n     \n     \n       4 \n       Alaska Pacific University \n       Yes \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.90 \n       2 \n       10922 \n       15 \n     \n     \n       5 \n       Albertson College \n       Yes \n       587 \n       479 \n       158 \n       38 \n       62 \n       678 \n       41 \n       13500 \n       3335 \n       500 \n       675 \n       67 \n       73 \n       9.40 \n       11 \n       9727 \n       55 \n     \n     \n       6 \n       Albertus Magnus College \n       Yes \n       353 \n       340 \n       103 \n       17 \n       45 \n       416 \n       230 \n       13290 \n       5720 \n       500 \n       1500 \n       90 \n       93 \n       11.50 \n       26 \n       8861 \n       63 \n     \n     \n       7 \n       Albion College \n       Yes \n       1899 \n       1720 \n       489 \n       37 \n       68 \n       1594 \n       32 \n       13868 \n       4826 \n       450 \n       850 \n       89 \n       100 \n       13.70 \n       37 \n       11487 \n       73 \n     \n     \n       8 \n       Albright College \n       Yes \n       1038 \n       839 \n       227 \n       30 \n       63 \n       973 \n       306 \n       15595 \n       4400 \n       300 \n       500 \n       79 \n       84 \n       11.30 \n       23 \n       11644 \n       80 \n     \n     \n       9 \n       Alderson-Broaddus College \n       Yes \n       582 \n       498 \n       172 \n       21 \n       44 \n       799 \n       78 \n       10468 \n       3380 \n       660 \n       1800 \n       40 \n       41 \n       11.50 \n       15 \n       8991 \n       52 \n     \n     \n       10 \n       Alfred University \n       Yes \n       1732 \n       1425 \n       472 \n       37 \n       75 \n       1830 \n       110 \n       16548 \n       5406 \n       500 \n       600 \n       82 \n       88 \n       11.30 \n       31 \n       10932 \n       73 \n     \n     \n       11 \n       Allegheny College \n       Yes \n       2652 \n       1900 \n       484 \n       44 \n       77 \n       1707 \n       44 \n       17080 \n       4440 \n       400 \n       600 \n       73 \n       91 \n       9.90 \n       41 \n       11711 \n       76 \n     \n     \n       12 \n       Allentown Coll. of St. Francis de Sales \n       Yes \n       1179 \n       780 \n       290 \n       38 \n       64 \n       1130 \n       638 \n       9690 \n       4785 \n       600 \n       1000 \n       60 \n       84 \n       13.30 \n       21 \n       7940 \n       74 \n     \n     \n       13 \n       Alma College \n       Yes \n       1267 \n       1080 \n       385 \n       44 \n       73 \n       1306 \n       28 \n       12572 \n       4552 \n       400 \n       400 \n       79 \n       87 \n       15.30 \n       32 \n       9305 \n       68 \n     \n     \n       14 \n       Alverno College \n       Yes \n       494 \n       313 \n       157 \n       23 \n       46 \n       1317 \n       1235 \n       8352 \n       3640 \n       650 \n       2449 \n       36 \n       69 \n       11.10 \n       26 \n       8127 \n       55 \n     \n     \n       15 \n       American International College \n       Yes \n       1420 \n       1093 \n       220 \n       9 \n       22 \n       1018 \n       287 \n       8700 \n       4780 \n       450 \n       1400 \n       78 \n       84 \n       14.70 \n       19 \n       7355 \n       69 \n     \n     \n       16 \n       Amherst College \n       Yes \n       4302 \n       992 \n       418 \n       83 \n       96 \n       1593 \n       5 \n       19760 \n       5300 \n       660 \n       1598 \n       93 \n       98 \n       8.40 \n       63 \n       21424 \n       100 \n     \n     \n       17 \n       Anderson University \n       Yes \n       1216 \n       908 \n       423 \n       19 \n       40 \n       1819 \n       281 \n       10100 \n       3520 \n       550 \n       1100 \n       48 \n       61 \n       12.10 \n       14 \n       7994 \n       59 \n     \n     \n       18 \n       Andrews University \n       Yes \n       1130 \n       704 \n       322 \n       14 \n       23 \n       1586 \n       326 \n       9996 \n       3090 \n       900 \n       1320 \n       62 \n       66 \n       11.50 \n       18 \n       10908 \n       46 \n     \n     \n       19 \n       Angelo State University \n       No \n       3540 \n       2001 \n       1016 \n       24 \n       54 \n       4190 \n       1512 \n       5130 \n       3592 \n       500 \n       2000 \n       60 \n       62 \n       23.10 \n       5 \n       4010 \n       34 \n     \n     \n       20 \n       Antioch University \n       Yes \n       713 \n       661 \n       252 \n       25 \n       44 \n       712 \n       23 \n       15476 \n       3336 \n       400 \n       1100 \n       69 \n       82 \n       11.30 \n       35 \n       42926 \n       48 \n     \n     \n       21 \n       Appalachian State University \n       No \n       7313 \n       4664 \n       1910 \n       20 \n       63 \n       9940 \n       1035 \n       6806 \n       2540 \n       96 \n       2000 \n       83 \n       96 \n       18.30 \n       14 \n       5854 \n       70 \n     \n     \n       22 \n       Aquinas College \n       Yes \n       619 \n       516 \n       219 \n       20 \n       51 \n       1251 \n       767 \n       11208 \n       4124 \n       350 \n       1615 \n       55 \n       65 \n       12.70 \n       25 \n       6584 \n       65 \n     \n     \n       23 \n       Arizona State University Main campus \n       No \n       12809 \n       10308 \n       3761 \n       24 \n       49 \n       22593 \n       7585 \n       7434 \n       4850 \n       700 \n       2100 \n       88 \n       93 \n       18.90 \n       5 \n       4602 \n       48 \n     \n     \n       24 \n       Arkansas College (Lyon College) \n       Yes \n       708 \n       334 \n       166 \n       46 \n       74 \n       530 \n       182 \n       8644 \n       3922 \n       500 \n       800 \n       79 \n       88 \n       12.60 \n       24 \n       14579 \n       54 \n     \n     \n       25 \n       Arkansas Tech University \n       No \n       1734 \n       1729 \n       951 \n       12 \n       52 \n       3602 \n       939 \n       3460 \n       2650 \n       450 \n       1000 \n       57 \n       60 \n       19.60 \n       5 \n       4739 \n       48 \n     \n     \n       26 \n       Assumption College \n       Yes \n       2135 \n       1700 \n       491 \n       23 \n       59 \n       1708 \n       689 \n       12000 \n       5920 \n       500 \n       500 \n       93 \n       93 \n       13.80 \n       30 \n       7100 \n       88 \n     \n     \n       27 \n       Auburn University-Main Campus \n       No \n       7548 \n       6791 \n       3070 \n       25 \n       57 \n       16262 \n       1716 \n       6300 \n       3933 \n       600 \n       1908 \n       85 \n       91 \n       16.70 \n       18 \n       6642 \n       69 \n     \n     \n       28 \n       Augsburg College \n       Yes \n       662 \n       513 \n       257 \n       12 \n       30 \n       2074 \n       726 \n       11902 \n       4372 \n       540 \n       950 \n       65 \n       65 \n       12.80 \n       31 \n       7836 \n       58 \n     \n     \n       29 \n       Augustana College IL \n       Yes \n       1879 \n       1658 \n       497 \n       36 \n       69 \n       1950 \n       38 \n       13353 \n       4173 \n       540 \n       821 \n       78 \n       83 \n       12.70 \n       40 \n       9220 \n       71 \n     \n     \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n       ... \n     \n     \n       747 \n       Westfield State College \n       No \n       3100 \n       2150 \n       825 \n       3 \n       20 \n       3234 \n       941 \n       5542 \n       3788 \n       500 \n       1300 \n       75 \n       79 \n       15.70 \n       20 \n       4222 \n       65 \n     \n     \n       748 \n       Westminster College MO \n       Yes \n       662 \n       553 \n       184 \n       20 \n       43 \n       665 \n       37 \n       10720 \n       4050 \n       600 \n       1650 \n       66 \n       70 \n       12.50 \n       20 \n       7925 \n       62 \n     \n     \n       749 \n       Westminster College \n       Yes \n       996 \n       866 \n       377 \n       29 \n       58 \n       1411 \n       72 \n       12065 \n       3615 \n       430 \n       685 \n       62 \n       78 \n       12.50 \n       41 \n       8596 \n       80 \n     \n     \n       750 \n       Westminster College of Salt Lake City \n       Yes \n       917 \n       720 \n       213 \n       21 \n       60 \n       979 \n       743 \n       8820 \n       4050 \n       600 \n       2025 \n       68 \n       83 \n       10.50 \n       34 \n       7170 \n       50 \n     \n     \n       751 \n       Westmont College \n       No \n       950 \n       713 \n       351 \n       42 \n       72 \n       1276 \n       9 \n       14320 \n       5304 \n       490 \n       1410 \n       77 \n       77 \n       14.90 \n       17 \n       8837 \n       87 \n     \n     \n       752 \n       Wheaton College IL \n       Yes \n       1432 \n       920 \n       548 \n       56 \n       84 \n       2200 \n       56 \n       11480 \n       4200 \n       530 \n       1400 \n       81 \n       83 \n       12.70 \n       40 \n       11916 \n       85 \n     \n     \n       753 \n       Westminster College PA \n       Yes \n       1738 \n       1373 \n       417 \n       21 \n       55 \n       1335 \n       30 \n       18460 \n       5970 \n       700 \n       850 \n       92 \n       96 \n       13.20 \n       41 \n       22704 \n       71 \n     \n     \n       754 \n       Wheeling Jesuit College \n       Yes \n       903 \n       755 \n       213 \n       15 \n       49 \n       971 \n       305 \n       10500 \n       4545 \n       600 \n       600 \n       66 \n       71 \n       14.10 \n       27 \n       7494 \n       72 \n     \n     \n       755 \n       Whitman College \n       Yes \n       1861 \n       998 \n       359 \n       45 \n       77 \n       1220 \n       46 \n       16670 \n       4900 \n       750 \n       800 \n       80 \n       83 \n       10.50 \n       51 \n       13198 \n       72 \n     \n     \n       756 \n       Whittier College \n       Yes \n       1681 \n       1069 \n       344 \n       35 \n       63 \n       1235 \n       30 \n       16249 \n       5699 \n       500 \n       1998 \n       84 \n       92 \n       13.60 \n       29 \n       11778 \n       52 \n     \n     \n       757 \n       Whitworth College \n       Yes \n       1121 \n       926 \n       372 \n       43 \n       70 \n       1270 \n       160 \n       12660 \n       4500 \n       678 \n       2424 \n       80 \n       80 \n       16.90 \n       20 \n       8328 \n       80 \n     \n     \n       758 \n       Widener University \n       Yes \n       2139 \n       1492 \n       502 \n       24 \n       64 \n       2186 \n       2171 \n       12350 \n       5370 \n       500 \n       1350 \n       88 \n       86 \n       12.60 \n       19 \n       9603 \n       63 \n     \n     \n       759 \n       Wilkes University \n       Yes \n       1631 \n       1431 \n       434 \n       15 \n       36 \n       1803 \n       603 \n       11150 \n       5130 \n       550 \n       1260 \n       78 \n       92 \n       13.30 \n       24 \n       8543 \n       67 \n     \n     \n       760 \n       Willamette University \n       Yes \n       1658 \n       1327 \n       395 \n       49 \n       80 \n       1595 \n       159 \n       14800 \n       4620 \n       400 \n       790 \n       91 \n       94 \n       13.30 \n       37 \n       10779 \n       68 \n     \n     \n       761 \n       William Jewell College \n       Yes \n       663 \n       547 \n       315 \n       32 \n       67 \n       1279 \n       75 \n       10060 \n       2970 \n       500 \n       2600 \n       74 \n       80 \n       11.20 \n       19 \n       7885 \n       59 \n     \n     \n       762 \n       William Woods University \n       Yes \n       469 \n       435 \n       227 \n       17 \n       39 \n       851 \n       120 \n       10535 \n       4365 \n       550 \n       3700 \n       39 \n       66 \n       12.90 \n       16 \n       7438 \n       52 \n     \n     \n       763 \n       Williams College \n       Yes \n       4186 \n       1245 \n       526 \n       81 \n       96 \n       1988 \n       29 \n       19629 \n       5790 \n       500 \n       1200 \n       94 \n       99 \n       9.00 \n       64 \n       22014 \n       99 \n     \n     \n       764 \n       Wilson College \n       Yes \n       167 \n       130 \n       46 \n       16 \n       50 \n       199 \n       676 \n       11428 \n       5084 \n       450 \n       475 \n       67 \n       76 \n       8.30 \n       43 \n       10291 \n       67 \n     \n     \n       765 \n       Wingate College \n       Yes \n       1239 \n       1017 \n       383 \n       10 \n       34 \n       1207 \n       157 \n       7820 \n       3400 \n       550 \n       1550 \n       69 \n       81 \n       13.90 \n       8 \n       7264 \n       91 \n     \n     \n       766 \n       Winona State University \n       No \n       3325 \n       2047 \n       1301 \n       20 \n       45 \n       5800 \n       872 \n       4200 \n       2700 \n       300 \n       1200 \n       53 \n       60 \n       20.20 \n       18 \n       5318 \n       58 \n     \n     \n       767 \n       Winthrop University \n       No \n       2320 \n       1805 \n       769 \n       24 \n       61 \n       3395 \n       670 \n       6400 \n       3392 \n       580 \n       2150 \n       71 \n       80 \n       12.80 \n       26 \n       6729 \n       59 \n     \n     \n       768 \n       Wisconsin Lutheran College \n       Yes \n       152 \n       128 \n       75 \n       17 \n       41 \n       282 \n       22 \n       9100 \n       3700 \n       500 \n       1400 \n       48 \n       48 \n       8.50 \n       26 \n       8960 \n       50 \n     \n     \n       769 \n       Wittenberg University \n       Yes \n       1979 \n       1739 \n       575 \n       42 \n       68 \n       1980 \n       144 \n       15948 \n       4404 \n       400 \n       800 \n       82 \n       95 \n       12.80 \n       29 \n       10414 \n       78 \n     \n     \n       770 \n       Wofford College \n       Yes \n       1501 \n       935 \n       273 \n       51 \n       83 \n       1059 \n       34 \n       12680 \n       4150 \n       605 \n       1440 \n       91 \n       92 \n       15.30 \n       42 \n       7875 \n       75 \n     \n     \n       771 \n       Worcester Polytechnic Institute \n       Yes \n       2768 \n       2314 \n       682 \n       49 \n       86 \n       2802 \n       86 \n       15884 \n       5370 \n       530 \n       730 \n       92 \n       94 \n       15.20 \n       34 \n       10774 \n       82 \n     \n     \n       772 \n       Worcester State College \n       No \n       2197 \n       1515 \n       543 \n       4 \n       26 \n       3089 \n       2029 \n       6797 \n       3900 \n       500 \n       1200 \n       60 \n       60 \n       21.00 \n       14 \n       4469 \n       40 \n     \n     \n       773 \n       Xavier University \n       Yes \n       1959 \n       1805 \n       695 \n       24 \n       47 \n       2849 \n       1107 \n       11520 \n       4960 \n       600 \n       1250 \n       73 \n       75 \n       13.30 \n       31 \n       9189 \n       83 \n     \n     \n       774 \n       Xavier University of Louisiana \n       Yes \n       2097 \n       1915 \n       695 \n       34 \n       61 \n       2793 \n       166 \n       6900 \n       4200 \n       617 \n       781 \n       67 \n       75 \n       14.40 \n       20 \n       8323 \n       49 \n     \n     \n       775 \n       Yale University \n       Yes \n       10705 \n       2453 \n       1317 \n       95 \n       99 \n       5217 \n       83 \n       19840 \n       6510 \n       630 \n       2115 \n       96 \n       96 \n       5.80 \n       49 \n       40386 \n       99 \n     \n     \n       776 \n       York College of Pennsylvania \n       Yes \n       2989 \n       1855 \n       691 \n       28 \n       63 \n       2988 \n       1726 \n       4990 \n       3560 \n       500 \n       1250 \n       75 \n       75 \n       18.10 \n       28 \n       4509 \n       99 \n     \n     777 rows \u00d7 19 columns",
            "title": "(a) Read csv"
        },
        {
            "location": "/sols/chapter2/exercise8/#b-university-names-as-index",
            "text": "The fix() function in R (similar to edit()) allows on-the-fly edit to the dataframe by invoking an editor.\nFurther details can be found  here  and  here .  # [1]\ncollege = college.set_index(\"Unnamed: 0\") # The default option 'drop=True', deletes the column\ncollege.index.name = 'Names'\ncollege.head()\n# The empty row below the columns names (e.g. Private, Apps, etc.) is there because the index has a name and that creates an additional row.   \n   \n     \n       \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n     \n       Names \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n     \n   \n   \n     \n       Abilene Christian University \n       Yes \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.10 \n       12 \n       7041 \n       60 \n     \n     \n       Adelphi University \n       Yes \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.20 \n       16 \n       10527 \n       56 \n     \n     \n       Adrian College \n       Yes \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.90 \n       30 \n       8735 \n       54 \n     \n     \n       Agnes Scott College \n       Yes \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.70 \n       37 \n       19016 \n       59 \n     \n     \n       Alaska Pacific University \n       Yes \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.90 \n       2 \n       10922 \n       15 \n     \n      [1]  https://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/advanced-indexing?ex=1    # Alternative solution: We could have done this all in one less line with:\ncollege = pd.read_csv('../data/College.csv', index_col='Unnamed: 0')\ncollege.index.name = 'Names'\ncollege.head()   \n   \n     \n       \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n     \n       Names \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n     \n   \n   \n     \n       Abilene Christian University \n       Yes \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.10 \n       12 \n       7041 \n       60 \n     \n     \n       Adelphi University \n       Yes \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.20 \n       16 \n       10527 \n       56 \n     \n     \n       Adrian College \n       Yes \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.90 \n       30 \n       8735 \n       54 \n     \n     \n       Agnes Scott College \n       Yes \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.70 \n       37 \n       19016 \n       59 \n     \n     \n       Alaska Pacific University \n       Yes \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.90 \n       2 \n       10922 \n       15",
            "title": "(b) University names as index"
        },
        {
            "location": "/sols/chapter2/exercise8/#c",
            "text": "",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter2/exercise8/#i-summary",
            "text": "college.describe(include='all')\n# [2, 3, 4] Without the 'all' option, the column 'Private' is not shown because it is categorical   \n   \n     \n       \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n   \n   \n     \n       count \n       777 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n     \n     \n       unique \n       2 \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n     \n     \n       top \n       Yes \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n     \n     \n       freq \n       565 \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n       nan \n     \n     \n       mean \n       NaN \n       3,001.64 \n       2,018.80 \n       779.97 \n       27.56 \n       55.80 \n       3,699.91 \n       855.30 \n       10,440.67 \n       4,357.53 \n       549.38 \n       1,340.64 \n       72.66 \n       79.70 \n       14.09 \n       22.74 \n       9,660.17 \n       65.46 \n     \n     \n       std \n       NaN \n       3,870.20 \n       2,451.11 \n       929.18 \n       17.64 \n       19.80 \n       4,850.42 \n       1,522.43 \n       4,023.02 \n       1,096.70 \n       165.11 \n       677.07 \n       16.33 \n       14.72 \n       3.96 \n       12.39 \n       5,221.77 \n       17.18 \n     \n     \n       min \n       NaN \n       81.00 \n       72.00 \n       35.00 \n       1.00 \n       9.00 \n       139.00 \n       1.00 \n       2,340.00 \n       1,780.00 \n       96.00 \n       250.00 \n       8.00 \n       24.00 \n       2.50 \n       0.00 \n       3,186.00 \n       10.00 \n     \n     \n       25% \n       NaN \n       776.00 \n       604.00 \n       242.00 \n       15.00 \n       41.00 \n       992.00 \n       95.00 \n       7,320.00 \n       3,597.00 \n       470.00 \n       850.00 \n       62.00 \n       71.00 \n       11.50 \n       13.00 \n       6,751.00 \n       53.00 \n     \n     \n       50% \n       NaN \n       1,558.00 \n       1,110.00 \n       434.00 \n       23.00 \n       54.00 \n       1,707.00 \n       353.00 \n       9,990.00 \n       4,200.00 \n       500.00 \n       1,200.00 \n       75.00 \n       82.00 \n       13.60 \n       21.00 \n       8,377.00 \n       65.00 \n     \n     \n       75% \n       NaN \n       3,624.00 \n       2,424.00 \n       902.00 \n       35.00 \n       69.00 \n       4,005.00 \n       967.00 \n       12,925.00 \n       5,050.00 \n       600.00 \n       1,700.00 \n       85.00 \n       92.00 \n       16.50 \n       31.00 \n       10,830.00 \n       78.00 \n     \n     \n       max \n       NaN \n       48,094.00 \n       26,330.00 \n       6,392.00 \n       96.00 \n       100.00 \n       31,643.00 \n       21,836.00 \n       21,700.00 \n       8,124.00 \n       2,340.00 \n       6,800.00 \n       103.00 \n       100.00 \n       39.80 \n       64.00 \n       56,233.00 \n       118.00 \n     \n      # Alternative solution: call describe twice. One on number, and another on object.\ncollege.describe(include=['number'])\n# or college.describe(include=[np.number])   \n   \n     \n       \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n   \n   \n     \n       count \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n       777.00 \n     \n     \n       mean \n       3,001.64 \n       2,018.80 \n       779.97 \n       27.56 \n       55.80 \n       3,699.91 \n       855.30 \n       10,440.67 \n       4,357.53 \n       549.38 \n       1,340.64 \n       72.66 \n       79.70 \n       14.09 \n       22.74 \n       9,660.17 \n       65.46 \n     \n     \n       std \n       3,870.20 \n       2,451.11 \n       929.18 \n       17.64 \n       19.80 \n       4,850.42 \n       1,522.43 \n       4,023.02 \n       1,096.70 \n       165.11 \n       677.07 \n       16.33 \n       14.72 \n       3.96 \n       12.39 \n       5,221.77 \n       17.18 \n     \n     \n       min \n       81.00 \n       72.00 \n       35.00 \n       1.00 \n       9.00 \n       139.00 \n       1.00 \n       2,340.00 \n       1,780.00 \n       96.00 \n       250.00 \n       8.00 \n       24.00 \n       2.50 \n       0.00 \n       3,186.00 \n       10.00 \n     \n     \n       25% \n       776.00 \n       604.00 \n       242.00 \n       15.00 \n       41.00 \n       992.00 \n       95.00 \n       7,320.00 \n       3,597.00 \n       470.00 \n       850.00 \n       62.00 \n       71.00 \n       11.50 \n       13.00 \n       6,751.00 \n       53.00 \n     \n     \n       50% \n       1,558.00 \n       1,110.00 \n       434.00 \n       23.00 \n       54.00 \n       1,707.00 \n       353.00 \n       9,990.00 \n       4,200.00 \n       500.00 \n       1,200.00 \n       75.00 \n       82.00 \n       13.60 \n       21.00 \n       8,377.00 \n       65.00 \n     \n     \n       75% \n       3,624.00 \n       2,424.00 \n       902.00 \n       35.00 \n       69.00 \n       4,005.00 \n       967.00 \n       12,925.00 \n       5,050.00 \n       600.00 \n       1,700.00 \n       85.00 \n       92.00 \n       16.50 \n       31.00 \n       10,830.00 \n       78.00 \n     \n     \n       max \n       48,094.00 \n       26,330.00 \n       6,392.00 \n       96.00 \n       100.00 \n       31,643.00 \n       21,836.00 \n       21,700.00 \n       8,124.00 \n       2,340.00 \n       6,800.00 \n       103.00 \n       100.00 \n       39.80 \n       64.00 \n       56,233.00 \n       118.00 \n     \n      college.describe(include=['object'])\n# or college.describe(include=['O'])   \n   \n     \n       \n       Private \n     \n   \n   \n     \n       count \n       777 \n     \n     \n       unique \n       2 \n     \n     \n       top \n       Yes \n     \n     \n       freq \n       565 \n     \n       [2]  http://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns  [3]  http://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns  [4]  http://dataanalysispython.readthedocs.io/en/latest/pandas.html#summarizing-data-describe",
            "title": "i. Summary"
        },
        {
            "location": "/sols/chapter2/exercise8/#ii-pair-plot",
            "text": "Unlike R, seaborn does not pairplot categorical vs numerical. See more  here .  g = sns.PairGrid(college, vars=college.iloc[:,1:11], hue='Private')\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)",
            "title": "ii. Pair plot"
        },
        {
            "location": "/sols/chapter2/exercise8/#iii-box-plots",
            "text": "sns.boxplot(x='Private', y='Outstate', data=college);",
            "title": "iii. Box plots"
        },
        {
            "location": "/sols/chapter2/exercise8/#iv-elite-variable",
            "text": "college.loc[college['Top10perc']>50, 'Elite'] = 'Yes'\ncollege['Elite'] = college['Elite'].fillna('No')\n\nsns.boxplot(x='Elite', y='Outstate', data=college);",
            "title": "iv. Elite variable"
        },
        {
            "location": "/sols/chapter2/exercise8/#v-histograms",
            "text": "In Python, to produce some histograms with differing numbers of bins for quantitative variables, we first need to convert these variables to bins.\nWhen we create bins, we transform a continuous range of values into a discrete one. For the purposes of this exercise, we will only consider equal-width bins.  # Bins creation\ncollege['PhD'] = pd.cut(college['PhD'], 3, labels=['Low', 'Medium', 'High'])\ncollege['Grad.Rate'] = pd.cut(college['Grad.Rate'], 5, labels=['Very low', 'Low', 'Medium', 'High', 'Very high'])\ncollege['Books'] = pd.cut(college['Books'], 2, labels=['Low', 'High'])\ncollege['Enroll'] = pd.cut(college['Enroll'], 4, labels=['Very low', 'Low', 'High', 'Very high'])  # Plot histograms\nfig = plt.figure()\n\nplt.subplot(221)\ncollege['PhD'].value_counts().plot(kind='bar', title = 'Private');\nplt.subplot(222)\ncollege['Grad.Rate'].value_counts().plot(kind='bar', title = 'Grad.Rate');\nplt.subplot(223)\ncollege['Books'].value_counts().plot(kind='bar', title = 'Books');\nplt.subplot(224)\ncollege['Enroll'].value_counts().plot(kind='bar', title = 'Enroll');\n\nfig.subplots_adjust(hspace=1) # To add space between subplots",
            "title": "v. Histograms"
        },
        {
            "location": "/sols/chapter2/exercise8/#vi-continue-exploring-the-data",
            "text": "\"This exercise is  trivial  and is left to the reader.\" :)",
            "title": "vi. Continue exploring the data"
        },
        {
            "location": "/sols/chapter2/exercise9/",
            "text": "%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.options.display.float_format = '{:,.2f}'.format # Print only 2 decimal cases.\n\n\n\n\ndf = pd.read_csv('../data/auto.csv')\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.00\n\n      \n8\n\n      \n307.00\n\n      \n130\n\n      \n3504\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.00\n\n      \n8\n\n      \n350.00\n\n      \n165\n\n      \n3693\n\n      \n11.50\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.00\n\n      \n8\n\n      \n318.00\n\n      \n150\n\n      \n3436\n\n      \n11.00\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.00\n\n      \n8\n\n      \n304.00\n\n      \n150\n\n      \n3433\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.00\n\n      \n8\n\n      \n302.00\n\n      \n140\n\n      \n3449\n\n      \n10.50\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n    \n\n      \n5\n\n      \n15.00\n\n      \n8\n\n      \n429.00\n\n      \n198\n\n      \n4341\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \nford galaxie 500\n\n    \n\n    \n\n      \n6\n\n      \n14.00\n\n      \n8\n\n      \n454.00\n\n      \n220\n\n      \n4354\n\n      \n9.00\n\n      \n70\n\n      \n1\n\n      \nchevrolet impala\n\n    \n\n    \n\n      \n7\n\n      \n14.00\n\n      \n8\n\n      \n440.00\n\n      \n215\n\n      \n4312\n\n      \n8.50\n\n      \n70\n\n      \n1\n\n      \nplymouth fury iii\n\n    \n\n    \n\n      \n8\n\n      \n14.00\n\n      \n8\n\n      \n455.00\n\n      \n225\n\n      \n4425\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \npontiac catalina\n\n    \n\n    \n\n      \n9\n\n      \n15.00\n\n      \n8\n\n      \n390.00\n\n      \n190\n\n      \n3850\n\n      \n8.50\n\n      \n70\n\n      \n1\n\n      \namc ambassador dpl\n\n    \n\n    \n\n      \n10\n\n      \n15.00\n\n      \n8\n\n      \n383.00\n\n      \n170\n\n      \n3563\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \ndodge challenger se\n\n    \n\n    \n\n      \n11\n\n      \n14.00\n\n      \n8\n\n      \n340.00\n\n      \n160\n\n      \n3609\n\n      \n8.00\n\n      \n70\n\n      \n1\n\n      \nplymouth 'cuda 340\n\n    \n\n    \n\n      \n12\n\n      \n15.00\n\n      \n8\n\n      \n400.00\n\n      \n150\n\n      \n3761\n\n      \n9.50\n\n      \n70\n\n      \n1\n\n      \nchevrolet monte carlo\n\n    \n\n    \n\n      \n13\n\n      \n14.00\n\n      \n8\n\n      \n455.00\n\n      \n225\n\n      \n3086\n\n      \n10.00\n\n      \n70\n\n      \n1\n\n      \nbuick estate wagon (sw)\n\n    \n\n    \n\n      \n14\n\n      \n24.00\n\n      \n4\n\n      \n113.00\n\n      \n95\n\n      \n2372\n\n      \n15.00\n\n      \n70\n\n      \n3\n\n      \ntoyota corona mark ii\n\n    \n\n    \n\n      \n15\n\n      \n22.00\n\n      \n6\n\n      \n198.00\n\n      \n95\n\n      \n2833\n\n      \n15.50\n\n      \n70\n\n      \n1\n\n      \nplymouth duster\n\n    \n\n    \n\n      \n16\n\n      \n18.00\n\n      \n6\n\n      \n199.00\n\n      \n97\n\n      \n2774\n\n      \n15.50\n\n      \n70\n\n      \n1\n\n      \namc hornet\n\n    \n\n    \n\n      \n17\n\n      \n21.00\n\n      \n6\n\n      \n200.00\n\n      \n85\n\n      \n2587\n\n      \n16.00\n\n      \n70\n\n      \n1\n\n      \nford maverick\n\n    \n\n    \n\n      \n18\n\n      \n27.00\n\n      \n4\n\n      \n97.00\n\n      \n88\n\n      \n2130\n\n      \n14.50\n\n      \n70\n\n      \n3\n\n      \ndatsun pl510\n\n    \n\n    \n\n      \n19\n\n      \n26.00\n\n      \n4\n\n      \n97.00\n\n      \n46\n\n      \n1835\n\n      \n20.50\n\n      \n70\n\n      \n2\n\n      \nvolkswagen 1131 deluxe sedan\n\n    \n\n    \n\n      \n20\n\n      \n25.00\n\n      \n4\n\n      \n110.00\n\n      \n87\n\n      \n2672\n\n      \n17.50\n\n      \n70\n\n      \n2\n\n      \npeugeot 504\n\n    \n\n    \n\n      \n21\n\n      \n24.00\n\n      \n4\n\n      \n107.00\n\n      \n90\n\n      \n2430\n\n      \n14.50\n\n      \n70\n\n      \n2\n\n      \naudi 100 ls\n\n    \n\n    \n\n      \n22\n\n      \n25.00\n\n      \n4\n\n      \n104.00\n\n      \n95\n\n      \n2375\n\n      \n17.50\n\n      \n70\n\n      \n2\n\n      \nsaab 99e\n\n    \n\n    \n\n      \n23\n\n      \n26.00\n\n      \n4\n\n      \n121.00\n\n      \n113\n\n      \n2234\n\n      \n12.50\n\n      \n70\n\n      \n2\n\n      \nbmw 2002\n\n    \n\n    \n\n      \n24\n\n      \n21.00\n\n      \n6\n\n      \n199.00\n\n      \n90\n\n      \n2648\n\n      \n15.00\n\n      \n70\n\n      \n1\n\n      \namc gremlin\n\n    \n\n    \n\n      \n25\n\n      \n10.00\n\n      \n8\n\n      \n360.00\n\n      \n215\n\n      \n4615\n\n      \n14.00\n\n      \n70\n\n      \n1\n\n      \nford f250\n\n    \n\n    \n\n      \n26\n\n      \n10.00\n\n      \n8\n\n      \n307.00\n\n      \n200\n\n      \n4376\n\n      \n15.00\n\n      \n70\n\n      \n1\n\n      \nchevy c20\n\n    \n\n    \n\n      \n27\n\n      \n11.00\n\n      \n8\n\n      \n318.00\n\n      \n210\n\n      \n4382\n\n      \n13.50\n\n      \n70\n\n      \n1\n\n      \ndodge d200\n\n    \n\n    \n\n      \n28\n\n      \n9.00\n\n      \n8\n\n      \n304.00\n\n      \n193\n\n      \n4732\n\n      \n18.50\n\n      \n70\n\n      \n1\n\n      \nhi 1200d\n\n    \n\n    \n\n      \n29\n\n      \n27.00\n\n      \n4\n\n      \n97.00\n\n      \n88\n\n      \n2130\n\n      \n14.50\n\n      \n71\n\n      \n3\n\n      \ndatsun pl510\n\n    \n\n    \n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n      \n...\n\n    \n\n    \n\n      \n367\n\n      \n28.00\n\n      \n4\n\n      \n112.00\n\n      \n88\n\n      \n2605\n\n      \n19.60\n\n      \n82\n\n      \n1\n\n      \nchevrolet cavalier\n\n    \n\n    \n\n      \n368\n\n      \n27.00\n\n      \n4\n\n      \n112.00\n\n      \n88\n\n      \n2640\n\n      \n18.60\n\n      \n82\n\n      \n1\n\n      \nchevrolet cavalier wagon\n\n    \n\n    \n\n      \n369\n\n      \n34.00\n\n      \n4\n\n      \n112.00\n\n      \n88\n\n      \n2395\n\n      \n18.00\n\n      \n82\n\n      \n1\n\n      \nchevrolet cavalier 2-door\n\n    \n\n    \n\n      \n370\n\n      \n31.00\n\n      \n4\n\n      \n112.00\n\n      \n85\n\n      \n2575\n\n      \n16.20\n\n      \n82\n\n      \n1\n\n      \npontiac j2000 se hatchback\n\n    \n\n    \n\n      \n371\n\n      \n29.00\n\n      \n4\n\n      \n135.00\n\n      \n84\n\n      \n2525\n\n      \n16.00\n\n      \n82\n\n      \n1\n\n      \ndodge aries se\n\n    \n\n    \n\n      \n372\n\n      \n27.00\n\n      \n4\n\n      \n151.00\n\n      \n90\n\n      \n2735\n\n      \n18.00\n\n      \n82\n\n      \n1\n\n      \npontiac phoenix\n\n    \n\n    \n\n      \n373\n\n      \n24.00\n\n      \n4\n\n      \n140.00\n\n      \n92\n\n      \n2865\n\n      \n16.40\n\n      \n82\n\n      \n1\n\n      \nford fairmont futura\n\n    \n\n    \n\n      \n374\n\n      \n36.00\n\n      \n4\n\n      \n105.00\n\n      \n74\n\n      \n1980\n\n      \n15.30\n\n      \n82\n\n      \n2\n\n      \nvolkswagen rabbit l\n\n    \n\n    \n\n      \n375\n\n      \n37.00\n\n      \n4\n\n      \n91.00\n\n      \n68\n\n      \n2025\n\n      \n18.20\n\n      \n82\n\n      \n3\n\n      \nmazda glc custom l\n\n    \n\n    \n\n      \n376\n\n      \n31.00\n\n      \n4\n\n      \n91.00\n\n      \n68\n\n      \n1970\n\n      \n17.60\n\n      \n82\n\n      \n3\n\n      \nmazda glc custom\n\n    \n\n    \n\n      \n377\n\n      \n38.00\n\n      \n4\n\n      \n105.00\n\n      \n63\n\n      \n2125\n\n      \n14.70\n\n      \n82\n\n      \n1\n\n      \nplymouth horizon miser\n\n    \n\n    \n\n      \n378\n\n      \n36.00\n\n      \n4\n\n      \n98.00\n\n      \n70\n\n      \n2125\n\n      \n17.30\n\n      \n82\n\n      \n1\n\n      \nmercury lynx l\n\n    \n\n    \n\n      \n379\n\n      \n36.00\n\n      \n4\n\n      \n120.00\n\n      \n88\n\n      \n2160\n\n      \n14.50\n\n      \n82\n\n      \n3\n\n      \nnissan stanza xe\n\n    \n\n    \n\n      \n380\n\n      \n36.00\n\n      \n4\n\n      \n107.00\n\n      \n75\n\n      \n2205\n\n      \n14.50\n\n      \n82\n\n      \n3\n\n      \nhonda accord\n\n    \n\n    \n\n      \n381\n\n      \n34.00\n\n      \n4\n\n      \n108.00\n\n      \n70\n\n      \n2245\n\n      \n16.90\n\n      \n82\n\n      \n3\n\n      \ntoyota corolla\n\n    \n\n    \n\n      \n382\n\n      \n38.00\n\n      \n4\n\n      \n91.00\n\n      \n67\n\n      \n1965\n\n      \n15.00\n\n      \n82\n\n      \n3\n\n      \nhonda civic\n\n    \n\n    \n\n      \n383\n\n      \n32.00\n\n      \n4\n\n      \n91.00\n\n      \n67\n\n      \n1965\n\n      \n15.70\n\n      \n82\n\n      \n3\n\n      \nhonda civic (auto)\n\n    \n\n    \n\n      \n384\n\n      \n38.00\n\n      \n4\n\n      \n91.00\n\n      \n67\n\n      \n1995\n\n      \n16.20\n\n      \n82\n\n      \n3\n\n      \ndatsun 310 gx\n\n    \n\n    \n\n      \n385\n\n      \n25.00\n\n      \n6\n\n      \n181.00\n\n      \n110\n\n      \n2945\n\n      \n16.40\n\n      \n82\n\n      \n1\n\n      \nbuick century limited\n\n    \n\n    \n\n      \n386\n\n      \n38.00\n\n      \n6\n\n      \n262.00\n\n      \n85\n\n      \n3015\n\n      \n17.00\n\n      \n82\n\n      \n1\n\n      \noldsmobile cutlass ciera (diesel)\n\n    \n\n    \n\n      \n387\n\n      \n26.00\n\n      \n4\n\n      \n156.00\n\n      \n92\n\n      \n2585\n\n      \n14.50\n\n      \n82\n\n      \n1\n\n      \nchrysler lebaron medallion\n\n    \n\n    \n\n      \n388\n\n      \n22.00\n\n      \n6\n\n      \n232.00\n\n      \n112\n\n      \n2835\n\n      \n14.70\n\n      \n82\n\n      \n1\n\n      \nford granada l\n\n    \n\n    \n\n      \n389\n\n      \n32.00\n\n      \n4\n\n      \n144.00\n\n      \n96\n\n      \n2665\n\n      \n13.90\n\n      \n82\n\n      \n3\n\n      \ntoyota celica gt\n\n    \n\n    \n\n      \n390\n\n      \n36.00\n\n      \n4\n\n      \n135.00\n\n      \n84\n\n      \n2370\n\n      \n13.00\n\n      \n82\n\n      \n1\n\n      \ndodge charger 2.2\n\n    \n\n    \n\n      \n391\n\n      \n27.00\n\n      \n4\n\n      \n151.00\n\n      \n90\n\n      \n2950\n\n      \n17.30\n\n      \n82\n\n      \n1\n\n      \nchevrolet camaro\n\n    \n\n    \n\n      \n392\n\n      \n27.00\n\n      \n4\n\n      \n140.00\n\n      \n86\n\n      \n2790\n\n      \n15.60\n\n      \n82\n\n      \n1\n\n      \nford mustang gl\n\n    \n\n    \n\n      \n393\n\n      \n44.00\n\n      \n4\n\n      \n97.00\n\n      \n52\n\n      \n2130\n\n      \n24.60\n\n      \n82\n\n      \n2\n\n      \nvw pickup\n\n    \n\n    \n\n      \n394\n\n      \n32.00\n\n      \n4\n\n      \n135.00\n\n      \n84\n\n      \n2295\n\n      \n11.60\n\n      \n82\n\n      \n1\n\n      \ndodge rampage\n\n    \n\n    \n\n      \n395\n\n      \n28.00\n\n      \n4\n\n      \n120.00\n\n      \n79\n\n      \n2625\n\n      \n18.60\n\n      \n82\n\n      \n1\n\n      \nford ranger\n\n    \n\n    \n\n      \n396\n\n      \n31.00\n\n      \n4\n\n      \n119.00\n\n      \n82\n\n      \n2720\n\n      \n19.40\n\n      \n82\n\n      \n1\n\n      \nchevy s-10\n\n    \n\n  \n\n\n\n\n397 rows \u00d7 9 columns\n\n\n\n\n\nLooks good so far, no missing values in sight.\n\n\ndf.info()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 397 entries, 0 to 396\nData columns (total 9 columns):\nmpg             397 non-null float64\ncylinders       397 non-null int64\ndisplacement    397 non-null float64\nhorsepower      397 non-null object\nweight          397 non-null int64\nacceleration    397 non-null float64\nyear            397 non-null int64\norigin          397 non-null int64\nname            397 non-null object\ndtypes: float64(3), int64(4), object(2)\nmemory usage: 28.0+ KB\n\n\n\nIt seems suspicious that 'horsepower' is of 'object' type. Let's have a closer look.\n\n\ndf.horsepower.unique()\n\n\n\n\narray(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)\n\n\n\nOk, so there are some missing values represented by a \nquestion mark\n.\n\n\ndf = df[df.horsepower != '?'].copy() # [1]\ndf['horsepower']=pd.to_numeric(df['horsepower'])\n\n\n\n\n[1]\n\n\ndf.info()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 392 entries, 0 to 396\nData columns (total 9 columns):\nmpg             392 non-null float64\ncylinders       392 non-null int64\ndisplacement    392 non-null float64\nhorsepower      392 non-null int64\nweight          392 non-null int64\nacceleration    392 non-null float64\nyear            392 non-null int64\norigin          392 non-null int64\nname            392 non-null object\ndtypes: float64(3), int64(5), object(1)\nmemory usage: 30.6+ KB\n\n\n\na) Quantitative and qualitative predictors\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.00\n\n      \n8\n\n      \n307.00\n\n      \n130\n\n      \n3504\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.00\n\n      \n8\n\n      \n350.00\n\n      \n165\n\n      \n3693\n\n      \n11.50\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.00\n\n      \n8\n\n      \n318.00\n\n      \n150\n\n      \n3436\n\n      \n11.00\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.00\n\n      \n8\n\n      \n304.00\n\n      \n150\n\n      \n3433\n\n      \n12.00\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.00\n\n      \n8\n\n      \n302.00\n\n      \n140\n\n      \n3449\n\n      \n10.50\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n  \n\n\n\n\n\n\n\nQuantitative predictors:\n\n\nquantitative = df.select_dtypes(include=['number']).columns\nquantitative\n\n\n\n\nIndex(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin'],\n      dtype='object')\n\n\n\nQualitative predictors:\n\n\nqualitative = df.select_dtypes(exclude=['number']).columns\nqualitative\n\n\n\n\nIndex(['name'], dtype='object')\n\n\n\nb) Range of each quantitative predictor\n\n\na = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc['range']\n\n\n\n\nmpg               37.60\ncylinders          5.00\ndisplacement     387.00\nhorsepower       184.00\nweight         3,527.00\nacceleration      16.80\nyear              12.00\norigin             2.00\nName: range, dtype: float64\n\n\n\nc) Mean and standard deviation\n\n\na.loc[['mean','std', 'range']]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n    \n\n  \n\n  \n\n    \n\n      \nmean\n\n      \n23.45\n\n      \n5.47\n\n      \n194.41\n\n      \n104.47\n\n      \n2,977.58\n\n      \n15.54\n\n      \n75.98\n\n      \n1.58\n\n    \n\n    \n\n      \nstd\n\n      \n7.81\n\n      \n1.71\n\n      \n104.64\n\n      \n38.49\n\n      \n849.40\n\n      \n2.76\n\n      \n3.68\n\n      \n0.81\n\n    \n\n    \n\n      \nrange\n\n      \n37.60\n\n      \n5.00\n\n      \n387.00\n\n      \n184.00\n\n      \n3,527.00\n\n      \n16.80\n\n      \n12.00\n\n      \n2.00\n\n    \n\n  \n\n\n\n\n\n\n\nd) Mean and standard deviation, removing observations\n\n\ndf_b = df.drop(df.index[10:85])\nb = df_b.describe()\nb.loc['range'] = b.loc['max'] - b.loc['min']\nb.loc[['mean','std', 'range']]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n    \n\n  \n\n  \n\n    \n\n      \nmean\n\n      \n24.37\n\n      \n5.38\n\n      \n187.88\n\n      \n101.00\n\n      \n2,938.85\n\n      \n15.70\n\n      \n77.12\n\n      \n1.60\n\n    \n\n    \n\n      \nstd\n\n      \n7.87\n\n      \n1.66\n\n      \n100.17\n\n      \n36.00\n\n      \n811.64\n\n      \n2.72\n\n      \n3.13\n\n      \n0.82\n\n    \n\n    \n\n      \nrange\n\n      \n35.60\n\n      \n5.00\n\n      \n387.00\n\n      \n184.00\n\n      \n3,348.00\n\n      \n16.30\n\n      \n12.00\n\n      \n2.00\n\n    \n\n  \n\n\n\n\n\n\n\ne) Visualizing relationships between variables\n\n\nWe use some common visualization tools, namely:\n\n\n\n\nScatterplots\n\n\nBox plots\n\n\nHistograms\n\n\n\n\ng = sns.PairGrid(df, size=2)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.fig.set_size_inches(12, 12)\n\n\n\n\n\n\n\n\nThe histogram for 'acceleration' resembles a normal distribution.\n\n\n'displacement' and 'weight' have a strong linear relationship.\n\n\n'mpg' has a non-linear relationship with 'weight', 'horsepower' and 'displacement'.\n\n\n\n\nf) Predicting mpg\n\n\nBased on the previous question, we could use weight, horsepower and displacement. As seen in the scatterplots, these variables seem to have a non-linear relationship with mpg. Are these relationships statistically significant? Exercises \n3.8\n and \n3.9\n delve further into this matter.",
            "title": "2.9"
        },
        {
            "location": "/sols/chapter2/exercise9/#a-quantitative-and-qualitative-predictors",
            "text": "df.head()   \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n     \n   \n   \n     \n       0 \n       18.00 \n       8 \n       307.00 \n       130 \n       3504 \n       12.00 \n       70 \n       1 \n       chevrolet chevelle malibu \n     \n     \n       1 \n       15.00 \n       8 \n       350.00 \n       165 \n       3693 \n       11.50 \n       70 \n       1 \n       buick skylark 320 \n     \n     \n       2 \n       18.00 \n       8 \n       318.00 \n       150 \n       3436 \n       11.00 \n       70 \n       1 \n       plymouth satellite \n     \n     \n       3 \n       16.00 \n       8 \n       304.00 \n       150 \n       3433 \n       12.00 \n       70 \n       1 \n       amc rebel sst \n     \n     \n       4 \n       17.00 \n       8 \n       302.00 \n       140 \n       3449 \n       10.50 \n       70 \n       1 \n       ford torino \n     \n      Quantitative predictors:  quantitative = df.select_dtypes(include=['number']).columns\nquantitative  Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin'],\n      dtype='object')  Qualitative predictors:  qualitative = df.select_dtypes(exclude=['number']).columns\nqualitative  Index(['name'], dtype='object')",
            "title": "a) Quantitative and qualitative predictors"
        },
        {
            "location": "/sols/chapter2/exercise9/#b-range-of-each-quantitative-predictor",
            "text": "a = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc['range']  mpg               37.60\ncylinders          5.00\ndisplacement     387.00\nhorsepower       184.00\nweight         3,527.00\nacceleration      16.80\nyear              12.00\norigin             2.00\nName: range, dtype: float64",
            "title": "b) Range of each quantitative predictor"
        },
        {
            "location": "/sols/chapter2/exercise9/#c-mean-and-standard-deviation",
            "text": "a.loc[['mean','std', 'range']]   \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n     \n   \n   \n     \n       mean \n       23.45 \n       5.47 \n       194.41 \n       104.47 \n       2,977.58 \n       15.54 \n       75.98 \n       1.58 \n     \n     \n       std \n       7.81 \n       1.71 \n       104.64 \n       38.49 \n       849.40 \n       2.76 \n       3.68 \n       0.81 \n     \n     \n       range \n       37.60 \n       5.00 \n       387.00 \n       184.00 \n       3,527.00 \n       16.80 \n       12.00 \n       2.00",
            "title": "c) Mean and standard deviation"
        },
        {
            "location": "/sols/chapter2/exercise9/#d-mean-and-standard-deviation-removing-observations",
            "text": "df_b = df.drop(df.index[10:85])\nb = df_b.describe()\nb.loc['range'] = b.loc['max'] - b.loc['min']\nb.loc[['mean','std', 'range']]   \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n     \n   \n   \n     \n       mean \n       24.37 \n       5.38 \n       187.88 \n       101.00 \n       2,938.85 \n       15.70 \n       77.12 \n       1.60 \n     \n     \n       std \n       7.87 \n       1.66 \n       100.17 \n       36.00 \n       811.64 \n       2.72 \n       3.13 \n       0.82 \n     \n     \n       range \n       35.60 \n       5.00 \n       387.00 \n       184.00 \n       3,348.00 \n       16.30 \n       12.00 \n       2.00",
            "title": "d) Mean and standard deviation, removing observations"
        },
        {
            "location": "/sols/chapter2/exercise9/#e-visualizing-relationships-between-variables",
            "text": "We use some common visualization tools, namely:   Scatterplots  Box plots  Histograms   g = sns.PairGrid(df, size=2)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.fig.set_size_inches(12, 12)    The histogram for 'acceleration' resembles a normal distribution.  'displacement' and 'weight' have a strong linear relationship.  'mpg' has a non-linear relationship with 'weight', 'horsepower' and 'displacement'.",
            "title": "e) Visualizing relationships between variables"
        },
        {
            "location": "/sols/chapter2/exercise9/#f-predicting-mpg",
            "text": "Based on the previous question, we could use weight, horsepower and displacement. As seen in the scatterplots, these variables seem to have a non-linear relationship with mpg. Are these relationships statistically significant? Exercises  3.8  and  3.9  delve further into this matter.",
            "title": "f) Predicting mpg"
        },
        {
            "location": "/sols/chapter2/exercise10/",
            "text": "Exercise 2.10\n\n\n%matplotlib inline\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\na) Dataset overview\n\n\nIn Python we can load the Boston dataset using scikit-learn.\n\n\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['target'] = boston.target\n\n\n\n\nprint(boston['DESCR'])\n\n\n\n\nBoston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:\n\n    :Number of Instances: 506\n\n    :Number of Attributes: 13 numeric/categorical predictive\n\n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.\n\n**References**\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n\n\nnp.shape(df)\n\n\n\n\n(506, 14)\n\n\n\nNumber of rows and columns\n\n\n 506 rows.\n\n 14 columns.\n\n\n Rows and columns description \n\n\n Each rows is town in Boston area.\n\n Columns are features that can influence house price such as per capita crime rate by town ('CRIM').\n\n\nb) Scatterplots\n\n\n\n\nWe'll use seaborn to get a quick overview of pairwise relationships\n\n\n\n\ng = sns.PairGrid(df)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)\n\n\n\n\n\n\nplt.scatter(df['RM'], df['target'])\nplt.xlabel('RM')\nplt.ylabel('target');\n\n\n\n\n\n\n Findings \n\n\n It seems to exist a positive linear relationship between RM and target\n\n This is expected as RM is the number of rooms (more space, higher price)\n\n\nplt.scatter(df['LSTAT'], df['target'])\nplt.xlabel('LSTAT')\nplt.ylabel('target');\n\n\n\n\n\n\n Findings \n\n\n LSTAT and target seem to have a negative non-linear relationship\n\n This is expected as LSTAT is the percent of lower status people (lower status, lower incomes, cheaper houses)\n\n\nplt.scatter(df['RM'], df['LSTAT'])\nplt.xlabel('RM')\nplt.ylabel('LSTAT');\n\n\n\n\n\n\n Findings \n\n\n It seems to exist a negative non-linear relationship between LSTAT and RM\n\n It makes sense since people with less money (higher LSTAT) can't afford bigger houses (high RM)\n\n\nc) Predictors associated with capita crime rate\n\n\ndf.corrwith(df['CRIM']).sort_values()\n\n\n\n\ntarget    -0.385832\nDIS       -0.377904\nB         -0.377365\nRM        -0.219940\nZN        -0.199458\nCHAS      -0.055295\nPTRATIO    0.288250\nAGE        0.350784\nINDUS      0.404471\nNOX        0.417521\nLSTAT      0.452220\nTAX        0.579564\nRAD        0.622029\nCRIM       1.000000\ndtype: float64\n\n\n\nLooking at the previous scatterplots and the correlation of each variable with 'CRIM', we will have a closer at the 3 with the largest correlation, namely:\n\n RAD, index of accessibility to radial highways,\n\n TAX, full-value property-tax rate (in dollars per $10,000),\n* LSTAT, percentage of lower status of the population.\n\n\nax = sns.boxplot(x=\"RAD\", y=\"CRIM\", data=df)\n\n\n\n\n\n\n Findings \n\n* When RAD is equal to 24 (its highest value), average CRIM is much higher and CRIM range is much larger.\n\n\nplt.scatter(df['TAX'], df['CRIM'])\nplt.xlabel('TAX')\nplt.ylabel('CRIM');\n\n\n\n\n\n\n Findings \n\n* When TAX is equal to \n666\n, average CRIM is much higher and CRIM range is much larger.\n\n\nplt.scatter(df['LSTAT'], df['CRIM'])\nplt.xlabel('LSTAT')\nplt.ylabel('CRIM');\n\n\n\n\n\n\n Findings \n\n\n For lower values of LSTAT (< 10), CRIM is always under 10. For LSTAT > 10, there is a wider spread of CRIM.\n\n For LSTAT < 20, a large proportion of the data points is very close to CRIM = 0.\n\n\nd) Crime rate, tax rate and pupil-teacher ratio in suburbs\n\n\ndf.ix[df['CRIM'].nlargest(5).index]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n380\n\n      \n88.9762\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.671\n\n      \n6.968\n\n      \n91.9\n\n      \n1.4165\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n396.90\n\n      \n17.21\n\n      \n10.4\n\n    \n\n    \n\n      \n418\n\n      \n73.5341\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.679\n\n      \n5.957\n\n      \n100.0\n\n      \n1.8026\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n16.45\n\n      \n20.62\n\n      \n8.8\n\n    \n\n    \n\n      \n405\n\n      \n67.9208\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.693\n\n      \n5.683\n\n      \n100.0\n\n      \n1.4254\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n384.97\n\n      \n22.98\n\n      \n5.0\n\n    \n\n    \n\n      \n410\n\n      \n51.1358\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.597\n\n      \n5.757\n\n      \n100.0\n\n      \n1.4130\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n2.60\n\n      \n10.11\n\n      \n15.0\n\n    \n\n    \n\n      \n414\n\n      \n45.7461\n\n      \n0.0\n\n      \n18.1\n\n      \n0.0\n\n      \n0.693\n\n      \n4.519\n\n      \n100.0\n\n      \n1.6582\n\n      \n24.0\n\n      \n666.0\n\n      \n20.2\n\n      \n88.27\n\n      \n36.98\n\n      \n7.0\n\n    \n\n  \n\n\n\n\n\n\n\ndf.ix[df['TAX'].nlargest(5).index]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n488\n\n      \n0.15086\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.454\n\n      \n92.7\n\n      \n1.8209\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n395.09\n\n      \n18.06\n\n      \n15.2\n\n    \n\n    \n\n      \n489\n\n      \n0.18337\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.414\n\n      \n98.3\n\n      \n1.7554\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n344.05\n\n      \n23.97\n\n      \n7.0\n\n    \n\n    \n\n      \n490\n\n      \n0.20746\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.093\n\n      \n98.0\n\n      \n1.8226\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n318.43\n\n      \n29.68\n\n      \n8.1\n\n    \n\n    \n\n      \n491\n\n      \n0.10574\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.983\n\n      \n98.8\n\n      \n1.8681\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n390.11\n\n      \n18.07\n\n      \n13.6\n\n    \n\n    \n\n      \n492\n\n      \n0.11132\n\n      \n0.0\n\n      \n27.74\n\n      \n0.0\n\n      \n0.609\n\n      \n5.983\n\n      \n83.5\n\n      \n2.1099\n\n      \n4.0\n\n      \n711.0\n\n      \n20.1\n\n      \n396.90\n\n      \n13.35\n\n      \n20.1\n\n    \n\n  \n\n\n\n\n\n\n\ndf.ix[df['PTRATIO'].nlargest(5).index]\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \n354\n\n      \n0.04301\n\n      \n80.0\n\n      \n1.91\n\n      \n0.0\n\n      \n0.413\n\n      \n5.663\n\n      \n21.9\n\n      \n10.5857\n\n      \n4.0\n\n      \n334.0\n\n      \n22.0\n\n      \n382.80\n\n      \n8.05\n\n      \n18.2\n\n    \n\n    \n\n      \n355\n\n      \n0.10659\n\n      \n80.0\n\n      \n1.91\n\n      \n0.0\n\n      \n0.413\n\n      \n5.936\n\n      \n19.5\n\n      \n10.5857\n\n      \n4.0\n\n      \n334.0\n\n      \n22.0\n\n      \n376.04\n\n      \n5.57\n\n      \n20.6\n\n    \n\n    \n\n      \n127\n\n      \n0.25915\n\n      \n0.0\n\n      \n21.89\n\n      \n0.0\n\n      \n0.624\n\n      \n5.693\n\n      \n96.0\n\n      \n1.7883\n\n      \n4.0\n\n      \n437.0\n\n      \n21.2\n\n      \n392.11\n\n      \n17.19\n\n      \n16.2\n\n    \n\n    \n\n      \n128\n\n      \n0.32543\n\n      \n0.0\n\n      \n21.89\n\n      \n0.0\n\n      \n0.624\n\n      \n6.431\n\n      \n98.8\n\n      \n1.8125\n\n      \n4.0\n\n      \n437.0\n\n      \n21.2\n\n      \n396.90\n\n      \n15.39\n\n      \n18.0\n\n    \n\n    \n\n      \n129\n\n      \n0.88125\n\n      \n0.0\n\n      \n21.89\n\n      \n0.0\n\n      \n0.624\n\n      \n5.637\n\n      \n94.7\n\n      \n1.9799\n\n      \n4.0\n\n      \n437.0\n\n      \n21.2\n\n      \n396.90\n\n      \n18.34\n\n      \n14.3\n\n    \n\n  \n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n    \n\n    \n\n      \nmean\n\n      \n3.593761\n\n      \n11.363636\n\n      \n11.136779\n\n      \n0.069170\n\n      \n0.554695\n\n      \n6.284634\n\n      \n68.574901\n\n      \n3.795043\n\n      \n9.549407\n\n      \n408.237154\n\n      \n18.455534\n\n      \n356.674032\n\n      \n12.653063\n\n      \n22.532806\n\n    \n\n    \n\n      \nstd\n\n      \n8.596783\n\n      \n23.322453\n\n      \n6.860353\n\n      \n0.253994\n\n      \n0.115878\n\n      \n0.702617\n\n      \n28.148861\n\n      \n2.105710\n\n      \n8.707259\n\n      \n168.537116\n\n      \n2.164946\n\n      \n91.294864\n\n      \n7.141062\n\n      \n9.197104\n\n    \n\n    \n\n      \nmin\n\n      \n0.006320\n\n      \n0.000000\n\n      \n0.460000\n\n      \n0.000000\n\n      \n0.385000\n\n      \n3.561000\n\n      \n2.900000\n\n      \n1.129600\n\n      \n1.000000\n\n      \n187.000000\n\n      \n12.600000\n\n      \n0.320000\n\n      \n1.730000\n\n      \n5.000000\n\n    \n\n    \n\n      \n25%\n\n      \n0.082045\n\n      \n0.000000\n\n      \n5.190000\n\n      \n0.000000\n\n      \n0.449000\n\n      \n5.885500\n\n      \n45.025000\n\n      \n2.100175\n\n      \n4.000000\n\n      \n279.000000\n\n      \n17.400000\n\n      \n375.377500\n\n      \n6.950000\n\n      \n17.025000\n\n    \n\n    \n\n      \n50%\n\n      \n0.256510\n\n      \n0.000000\n\n      \n9.690000\n\n      \n0.000000\n\n      \n0.538000\n\n      \n6.208500\n\n      \n77.500000\n\n      \n3.207450\n\n      \n5.000000\n\n      \n330.000000\n\n      \n19.050000\n\n      \n391.440000\n\n      \n11.360000\n\n      \n21.200000\n\n    \n\n    \n\n      \n75%\n\n      \n3.647423\n\n      \n12.500000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.624000\n\n      \n6.623500\n\n      \n94.075000\n\n      \n5.188425\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.225000\n\n      \n16.955000\n\n      \n25.000000\n\n    \n\n    \n\n      \nmax\n\n      \n88.976200\n\n      \n100.000000\n\n      \n27.740000\n\n      \n1.000000\n\n      \n0.871000\n\n      \n8.780000\n\n      \n100.000000\n\n      \n12.126500\n\n      \n24.000000\n\n      \n711.000000\n\n      \n22.000000\n\n      \n396.900000\n\n      \n37.970000\n\n      \n50.000000\n\n    \n\n  \n\n\n\n\n\n\n\n Findings \n\n\n The 5 towns shown in CRIM table are particularly high\n\n All the towns shown in the TAX table have maximum TAX level\n* PTRATIO table shows towns with high pupil-teacher ratios but not so uneven\n\n\ne) Suburbs bounding the Charles river\n\n\ndf['CHAS'].value_counts()[1]\n\n\n\n\n35\n\n\n\n(f) Median pupil-teacher ratio\n\n\ndf['PTRATIO'].median()\n\n\n\n\n19.05\n\n\n\n(g) Suburb with lowest median value of owner occupied homes\n\n\ndf['target'].idxmin()\n\n\n\n\n398\n\n\n\na = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc[398] = df.ix[398]\na\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n    \n\n    \n\n      \nmean\n\n      \n3.593761\n\n      \n11.363636\n\n      \n11.136779\n\n      \n0.069170\n\n      \n0.554695\n\n      \n6.284634\n\n      \n68.574901\n\n      \n3.795043\n\n      \n9.549407\n\n      \n408.237154\n\n      \n18.455534\n\n      \n356.674032\n\n      \n12.653063\n\n      \n22.532806\n\n    \n\n    \n\n      \nstd\n\n      \n8.596783\n\n      \n23.322453\n\n      \n6.860353\n\n      \n0.253994\n\n      \n0.115878\n\n      \n0.702617\n\n      \n28.148861\n\n      \n2.105710\n\n      \n8.707259\n\n      \n168.537116\n\n      \n2.164946\n\n      \n91.294864\n\n      \n7.141062\n\n      \n9.197104\n\n    \n\n    \n\n      \nmin\n\n      \n0.006320\n\n      \n0.000000\n\n      \n0.460000\n\n      \n0.000000\n\n      \n0.385000\n\n      \n3.561000\n\n      \n2.900000\n\n      \n1.129600\n\n      \n1.000000\n\n      \n187.000000\n\n      \n12.600000\n\n      \n0.320000\n\n      \n1.730000\n\n      \n5.000000\n\n    \n\n    \n\n      \n25%\n\n      \n0.082045\n\n      \n0.000000\n\n      \n5.190000\n\n      \n0.000000\n\n      \n0.449000\n\n      \n5.885500\n\n      \n45.025000\n\n      \n2.100175\n\n      \n4.000000\n\n      \n279.000000\n\n      \n17.400000\n\n      \n375.377500\n\n      \n6.950000\n\n      \n17.025000\n\n    \n\n    \n\n      \n50%\n\n      \n0.256510\n\n      \n0.000000\n\n      \n9.690000\n\n      \n0.000000\n\n      \n0.538000\n\n      \n6.208500\n\n      \n77.500000\n\n      \n3.207450\n\n      \n5.000000\n\n      \n330.000000\n\n      \n19.050000\n\n      \n391.440000\n\n      \n11.360000\n\n      \n21.200000\n\n    \n\n    \n\n      \n75%\n\n      \n3.647423\n\n      \n12.500000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.624000\n\n      \n6.623500\n\n      \n94.075000\n\n      \n5.188425\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.225000\n\n      \n16.955000\n\n      \n25.000000\n\n    \n\n    \n\n      \nmax\n\n      \n88.976200\n\n      \n100.000000\n\n      \n27.740000\n\n      \n1.000000\n\n      \n0.871000\n\n      \n8.780000\n\n      \n100.000000\n\n      \n12.126500\n\n      \n24.000000\n\n      \n711.000000\n\n      \n22.000000\n\n      \n396.900000\n\n      \n37.970000\n\n      \n50.000000\n\n    \n\n    \n\n      \nrange\n\n      \n88.969880\n\n      \n100.000000\n\n      \n27.280000\n\n      \n1.000000\n\n      \n0.486000\n\n      \n5.219000\n\n      \n97.100000\n\n      \n10.996900\n\n      \n23.000000\n\n      \n524.000000\n\n      \n9.400000\n\n      \n396.580000\n\n      \n36.240000\n\n      \n45.000000\n\n    \n\n    \n\n      \n398\n\n      \n38.351800\n\n      \n0.000000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.693000\n\n      \n5.453000\n\n      \n100.000000\n\n      \n1.489600\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.900000\n\n      \n30.590000\n\n      \n5.000000\n\n    \n\n  \n\n\n\n\n\n\n\n Findings \n\n\n The suburb with the lowest median value is 398.\n\n Relative to the other towns, this suburb has high CRIM, ZN below quantile 75%, above mean INDUS, does not bound the Charles river, above mean NOX, RM below quantile 25%, maximum AGE, DIS near to the minimum value, maximum RAD, TAX in quantile 75%, PTRATIO as well, B maximum and LSTAT above quantile 75%.\n\n\nh) Number of rooms per dwelling\n\n\nlen(df[df['RM']>7])\n\n\n\n\n64\n\n\n\nlen(df[df['RM']>8])\n\n\n\n\n13\n\n\n\nlen(df[df['RM']>8])\n\n\n\n\n13\n\n\n\ndf[df['RM']>8].describe()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n      \n13.000000\n\n    \n\n    \n\n      \nmean\n\n      \n0.718795\n\n      \n13.615385\n\n      \n7.078462\n\n      \n0.153846\n\n      \n0.539238\n\n      \n8.348538\n\n      \n71.538462\n\n      \n3.430192\n\n      \n7.461538\n\n      \n325.076923\n\n      \n16.361538\n\n      \n385.210769\n\n      \n4.310000\n\n      \n44.200000\n\n    \n\n    \n\n      \nstd\n\n      \n0.901640\n\n      \n26.298094\n\n      \n5.392767\n\n      \n0.375534\n\n      \n0.092352\n\n      \n0.251261\n\n      \n24.608723\n\n      \n1.883955\n\n      \n5.332532\n\n      \n110.971063\n\n      \n2.410580\n\n      \n10.529359\n\n      \n1.373566\n\n      \n8.092383\n\n    \n\n    \n\n      \nmin\n\n      \n0.020090\n\n      \n0.000000\n\n      \n2.680000\n\n      \n0.000000\n\n      \n0.416100\n\n      \n8.034000\n\n      \n8.400000\n\n      \n1.801000\n\n      \n2.000000\n\n      \n224.000000\n\n      \n13.000000\n\n      \n354.550000\n\n      \n2.470000\n\n      \n21.900000\n\n    \n\n    \n\n      \n25%\n\n      \n0.331470\n\n      \n0.000000\n\n      \n3.970000\n\n      \n0.000000\n\n      \n0.504000\n\n      \n8.247000\n\n      \n70.400000\n\n      \n2.288500\n\n      \n5.000000\n\n      \n264.000000\n\n      \n14.700000\n\n      \n384.540000\n\n      \n3.320000\n\n      \n41.700000\n\n    \n\n    \n\n      \n50%\n\n      \n0.520140\n\n      \n0.000000\n\n      \n6.200000\n\n      \n0.000000\n\n      \n0.507000\n\n      \n8.297000\n\n      \n78.300000\n\n      \n2.894400\n\n      \n7.000000\n\n      \n307.000000\n\n      \n17.400000\n\n      \n386.860000\n\n      \n4.140000\n\n      \n48.300000\n\n    \n\n    \n\n      \n75%\n\n      \n0.578340\n\n      \n20.000000\n\n      \n6.200000\n\n      \n0.000000\n\n      \n0.605000\n\n      \n8.398000\n\n      \n86.500000\n\n      \n3.651900\n\n      \n8.000000\n\n      \n307.000000\n\n      \n17.400000\n\n      \n389.700000\n\n      \n5.120000\n\n      \n50.000000\n\n    \n\n    \n\n      \nmax\n\n      \n3.474280\n\n      \n95.000000\n\n      \n19.580000\n\n      \n1.000000\n\n      \n0.718000\n\n      \n8.780000\n\n      \n93.900000\n\n      \n8.906700\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.900000\n\n      \n7.440000\n\n      \n50.000000\n\n    \n\n  \n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \ntarget\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n      \n506.000000\n\n    \n\n    \n\n      \nmean\n\n      \n3.593761\n\n      \n11.363636\n\n      \n11.136779\n\n      \n0.069170\n\n      \n0.554695\n\n      \n6.284634\n\n      \n68.574901\n\n      \n3.795043\n\n      \n9.549407\n\n      \n408.237154\n\n      \n18.455534\n\n      \n356.674032\n\n      \n12.653063\n\n      \n22.532806\n\n    \n\n    \n\n      \nstd\n\n      \n8.596783\n\n      \n23.322453\n\n      \n6.860353\n\n      \n0.253994\n\n      \n0.115878\n\n      \n0.702617\n\n      \n28.148861\n\n      \n2.105710\n\n      \n8.707259\n\n      \n168.537116\n\n      \n2.164946\n\n      \n91.294864\n\n      \n7.141062\n\n      \n9.197104\n\n    \n\n    \n\n      \nmin\n\n      \n0.006320\n\n      \n0.000000\n\n      \n0.460000\n\n      \n0.000000\n\n      \n0.385000\n\n      \n3.561000\n\n      \n2.900000\n\n      \n1.129600\n\n      \n1.000000\n\n      \n187.000000\n\n      \n12.600000\n\n      \n0.320000\n\n      \n1.730000\n\n      \n5.000000\n\n    \n\n    \n\n      \n25%\n\n      \n0.082045\n\n      \n0.000000\n\n      \n5.190000\n\n      \n0.000000\n\n      \n0.449000\n\n      \n5.885500\n\n      \n45.025000\n\n      \n2.100175\n\n      \n4.000000\n\n      \n279.000000\n\n      \n17.400000\n\n      \n375.377500\n\n      \n6.950000\n\n      \n17.025000\n\n    \n\n    \n\n      \n50%\n\n      \n0.256510\n\n      \n0.000000\n\n      \n9.690000\n\n      \n0.000000\n\n      \n0.538000\n\n      \n6.208500\n\n      \n77.500000\n\n      \n3.207450\n\n      \n5.000000\n\n      \n330.000000\n\n      \n19.050000\n\n      \n391.440000\n\n      \n11.360000\n\n      \n21.200000\n\n    \n\n    \n\n      \n75%\n\n      \n3.647423\n\n      \n12.500000\n\n      \n18.100000\n\n      \n0.000000\n\n      \n0.624000\n\n      \n6.623500\n\n      \n94.075000\n\n      \n5.188425\n\n      \n24.000000\n\n      \n666.000000\n\n      \n20.200000\n\n      \n396.225000\n\n      \n16.955000\n\n      \n25.000000\n\n    \n\n    \n\n      \nmax\n\n      \n88.976200\n\n      \n100.000000\n\n      \n27.740000\n\n      \n1.000000\n\n      \n0.871000\n\n      \n8.780000\n\n      \n100.000000\n\n      \n12.126500\n\n      \n24.000000\n\n      \n711.000000\n\n      \n22.000000\n\n      \n396.900000\n\n      \n37.970000\n\n      \n50.000000\n\n    \n\n  \n\n\n\n\n\n\n\n Comments \n\n\n CRIM is lower,\n\n INDUS proportion is lower,\n* % of lower status of the population (LSTAT) is lower.",
            "title": "2.10"
        },
        {
            "location": "/sols/chapter2/exercise10/#exercise-210",
            "text": "%matplotlib inline\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt",
            "title": "Exercise 2.10"
        },
        {
            "location": "/sols/chapter2/exercise10/#a-dataset-overview",
            "text": "In Python we can load the Boston dataset using scikit-learn.  from sklearn.datasets import load_boston\n\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['target'] = boston.target  print(boston['DESCR'])  Boston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:\n\n    :Number of Instances: 506\n\n    :Number of Attributes: 13 numeric/categorical predictive\n\n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.\n\n**References**\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)  df.head()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2 \n     \n      np.shape(df)  (506, 14)  Number of rows and columns   506 rows.  14 columns.   Rows and columns description    Each rows is town in Boston area.  Columns are features that can influence house price such as per capita crime rate by town ('CRIM').",
            "title": "a) Dataset overview"
        },
        {
            "location": "/sols/chapter2/exercise10/#b-scatterplots",
            "text": "We'll use seaborn to get a quick overview of pairwise relationships   g = sns.PairGrid(df)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(plt.scatter, s=3)\ng.fig.set_size_inches(12, 12)   plt.scatter(df['RM'], df['target'])\nplt.xlabel('RM')\nplt.ylabel('target');    Findings    It seems to exist a positive linear relationship between RM and target  This is expected as RM is the number of rooms (more space, higher price)  plt.scatter(df['LSTAT'], df['target'])\nplt.xlabel('LSTAT')\nplt.ylabel('target');    Findings    LSTAT and target seem to have a negative non-linear relationship  This is expected as LSTAT is the percent of lower status people (lower status, lower incomes, cheaper houses)  plt.scatter(df['RM'], df['LSTAT'])\nplt.xlabel('RM')\nplt.ylabel('LSTAT');    Findings    It seems to exist a negative non-linear relationship between LSTAT and RM  It makes sense since people with less money (higher LSTAT) can't afford bigger houses (high RM)",
            "title": "b) Scatterplots"
        },
        {
            "location": "/sols/chapter2/exercise10/#c-predictors-associated-with-capita-crime-rate",
            "text": "df.corrwith(df['CRIM']).sort_values()  target    -0.385832\nDIS       -0.377904\nB         -0.377365\nRM        -0.219940\nZN        -0.199458\nCHAS      -0.055295\nPTRATIO    0.288250\nAGE        0.350784\nINDUS      0.404471\nNOX        0.417521\nLSTAT      0.452220\nTAX        0.579564\nRAD        0.622029\nCRIM       1.000000\ndtype: float64  Looking at the previous scatterplots and the correlation of each variable with 'CRIM', we will have a closer at the 3 with the largest correlation, namely:  RAD, index of accessibility to radial highways,  TAX, full-value property-tax rate (in dollars per $10,000),\n* LSTAT, percentage of lower status of the population.  ax = sns.boxplot(x=\"RAD\", y=\"CRIM\", data=df)    Findings  \n* When RAD is equal to 24 (its highest value), average CRIM is much higher and CRIM range is much larger.  plt.scatter(df['TAX'], df['CRIM'])\nplt.xlabel('TAX')\nplt.ylabel('CRIM');    Findings  \n* When TAX is equal to  666 , average CRIM is much higher and CRIM range is much larger.  plt.scatter(df['LSTAT'], df['CRIM'])\nplt.xlabel('LSTAT')\nplt.ylabel('CRIM');    Findings    For lower values of LSTAT (< 10), CRIM is always under 10. For LSTAT > 10, there is a wider spread of CRIM.  For LSTAT < 20, a large proportion of the data points is very close to CRIM = 0.",
            "title": "c) Predictors associated with capita crime rate"
        },
        {
            "location": "/sols/chapter2/exercise10/#d-crime-rate-tax-rate-and-pupil-teacher-ratio-in-suburbs",
            "text": "df.ix[df['CRIM'].nlargest(5).index]   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       380 \n       88.9762 \n       0.0 \n       18.1 \n       0.0 \n       0.671 \n       6.968 \n       91.9 \n       1.4165 \n       24.0 \n       666.0 \n       20.2 \n       396.90 \n       17.21 \n       10.4 \n     \n     \n       418 \n       73.5341 \n       0.0 \n       18.1 \n       0.0 \n       0.679 \n       5.957 \n       100.0 \n       1.8026 \n       24.0 \n       666.0 \n       20.2 \n       16.45 \n       20.62 \n       8.8 \n     \n     \n       405 \n       67.9208 \n       0.0 \n       18.1 \n       0.0 \n       0.693 \n       5.683 \n       100.0 \n       1.4254 \n       24.0 \n       666.0 \n       20.2 \n       384.97 \n       22.98 \n       5.0 \n     \n     \n       410 \n       51.1358 \n       0.0 \n       18.1 \n       0.0 \n       0.597 \n       5.757 \n       100.0 \n       1.4130 \n       24.0 \n       666.0 \n       20.2 \n       2.60 \n       10.11 \n       15.0 \n     \n     \n       414 \n       45.7461 \n       0.0 \n       18.1 \n       0.0 \n       0.693 \n       4.519 \n       100.0 \n       1.6582 \n       24.0 \n       666.0 \n       20.2 \n       88.27 \n       36.98 \n       7.0 \n     \n      df.ix[df['TAX'].nlargest(5).index]   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       488 \n       0.15086 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.454 \n       92.7 \n       1.8209 \n       4.0 \n       711.0 \n       20.1 \n       395.09 \n       18.06 \n       15.2 \n     \n     \n       489 \n       0.18337 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.414 \n       98.3 \n       1.7554 \n       4.0 \n       711.0 \n       20.1 \n       344.05 \n       23.97 \n       7.0 \n     \n     \n       490 \n       0.20746 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.093 \n       98.0 \n       1.8226 \n       4.0 \n       711.0 \n       20.1 \n       318.43 \n       29.68 \n       8.1 \n     \n     \n       491 \n       0.10574 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.983 \n       98.8 \n       1.8681 \n       4.0 \n       711.0 \n       20.1 \n       390.11 \n       18.07 \n       13.6 \n     \n     \n       492 \n       0.11132 \n       0.0 \n       27.74 \n       0.0 \n       0.609 \n       5.983 \n       83.5 \n       2.1099 \n       4.0 \n       711.0 \n       20.1 \n       396.90 \n       13.35 \n       20.1 \n     \n      df.ix[df['PTRATIO'].nlargest(5).index]   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       354 \n       0.04301 \n       80.0 \n       1.91 \n       0.0 \n       0.413 \n       5.663 \n       21.9 \n       10.5857 \n       4.0 \n       334.0 \n       22.0 \n       382.80 \n       8.05 \n       18.2 \n     \n     \n       355 \n       0.10659 \n       80.0 \n       1.91 \n       0.0 \n       0.413 \n       5.936 \n       19.5 \n       10.5857 \n       4.0 \n       334.0 \n       22.0 \n       376.04 \n       5.57 \n       20.6 \n     \n     \n       127 \n       0.25915 \n       0.0 \n       21.89 \n       0.0 \n       0.624 \n       5.693 \n       96.0 \n       1.7883 \n       4.0 \n       437.0 \n       21.2 \n       392.11 \n       17.19 \n       16.2 \n     \n     \n       128 \n       0.32543 \n       0.0 \n       21.89 \n       0.0 \n       0.624 \n       6.431 \n       98.8 \n       1.8125 \n       4.0 \n       437.0 \n       21.2 \n       396.90 \n       15.39 \n       18.0 \n     \n     \n       129 \n       0.88125 \n       0.0 \n       21.89 \n       0.0 \n       0.624 \n       5.637 \n       94.7 \n       1.9799 \n       4.0 \n       437.0 \n       21.2 \n       396.90 \n       18.34 \n       14.3 \n     \n      df.describe()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n     \n     \n       mean \n       3.593761 \n       11.363636 \n       11.136779 \n       0.069170 \n       0.554695 \n       6.284634 \n       68.574901 \n       3.795043 \n       9.549407 \n       408.237154 \n       18.455534 \n       356.674032 \n       12.653063 \n       22.532806 \n     \n     \n       std \n       8.596783 \n       23.322453 \n       6.860353 \n       0.253994 \n       0.115878 \n       0.702617 \n       28.148861 \n       2.105710 \n       8.707259 \n       168.537116 \n       2.164946 \n       91.294864 \n       7.141062 \n       9.197104 \n     \n     \n       min \n       0.006320 \n       0.000000 \n       0.460000 \n       0.000000 \n       0.385000 \n       3.561000 \n       2.900000 \n       1.129600 \n       1.000000 \n       187.000000 \n       12.600000 \n       0.320000 \n       1.730000 \n       5.000000 \n     \n     \n       25% \n       0.082045 \n       0.000000 \n       5.190000 \n       0.000000 \n       0.449000 \n       5.885500 \n       45.025000 \n       2.100175 \n       4.000000 \n       279.000000 \n       17.400000 \n       375.377500 \n       6.950000 \n       17.025000 \n     \n     \n       50% \n       0.256510 \n       0.000000 \n       9.690000 \n       0.000000 \n       0.538000 \n       6.208500 \n       77.500000 \n       3.207450 \n       5.000000 \n       330.000000 \n       19.050000 \n       391.440000 \n       11.360000 \n       21.200000 \n     \n     \n       75% \n       3.647423 \n       12.500000 \n       18.100000 \n       0.000000 \n       0.624000 \n       6.623500 \n       94.075000 \n       5.188425 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.225000 \n       16.955000 \n       25.000000 \n     \n     \n       max \n       88.976200 \n       100.000000 \n       27.740000 \n       1.000000 \n       0.871000 \n       8.780000 \n       100.000000 \n       12.126500 \n       24.000000 \n       711.000000 \n       22.000000 \n       396.900000 \n       37.970000 \n       50.000000 \n     \n       Findings    The 5 towns shown in CRIM table are particularly high  All the towns shown in the TAX table have maximum TAX level\n* PTRATIO table shows towns with high pupil-teacher ratios but not so uneven",
            "title": "d) Crime rate, tax rate and pupil-teacher ratio in suburbs"
        },
        {
            "location": "/sols/chapter2/exercise10/#e-suburbs-bounding-the-charles-river",
            "text": "df['CHAS'].value_counts()[1]  35",
            "title": "e) Suburbs bounding the Charles river"
        },
        {
            "location": "/sols/chapter2/exercise10/#f-median-pupil-teacher-ratio",
            "text": "df['PTRATIO'].median()  19.05",
            "title": "(f) Median pupil-teacher ratio"
        },
        {
            "location": "/sols/chapter2/exercise10/#g-suburb-with-lowest-median-value-of-owner-occupied-homes",
            "text": "df['target'].idxmin()  398  a = df.describe()\na.loc['range'] = a.loc['max'] - a.loc['min']\na.loc[398] = df.ix[398]\na   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n     \n     \n       mean \n       3.593761 \n       11.363636 \n       11.136779 \n       0.069170 \n       0.554695 \n       6.284634 \n       68.574901 \n       3.795043 \n       9.549407 \n       408.237154 \n       18.455534 \n       356.674032 \n       12.653063 \n       22.532806 \n     \n     \n       std \n       8.596783 \n       23.322453 \n       6.860353 \n       0.253994 \n       0.115878 \n       0.702617 \n       28.148861 \n       2.105710 \n       8.707259 \n       168.537116 \n       2.164946 \n       91.294864 \n       7.141062 \n       9.197104 \n     \n     \n       min \n       0.006320 \n       0.000000 \n       0.460000 \n       0.000000 \n       0.385000 \n       3.561000 \n       2.900000 \n       1.129600 \n       1.000000 \n       187.000000 \n       12.600000 \n       0.320000 \n       1.730000 \n       5.000000 \n     \n     \n       25% \n       0.082045 \n       0.000000 \n       5.190000 \n       0.000000 \n       0.449000 \n       5.885500 \n       45.025000 \n       2.100175 \n       4.000000 \n       279.000000 \n       17.400000 \n       375.377500 \n       6.950000 \n       17.025000 \n     \n     \n       50% \n       0.256510 \n       0.000000 \n       9.690000 \n       0.000000 \n       0.538000 \n       6.208500 \n       77.500000 \n       3.207450 \n       5.000000 \n       330.000000 \n       19.050000 \n       391.440000 \n       11.360000 \n       21.200000 \n     \n     \n       75% \n       3.647423 \n       12.500000 \n       18.100000 \n       0.000000 \n       0.624000 \n       6.623500 \n       94.075000 \n       5.188425 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.225000 \n       16.955000 \n       25.000000 \n     \n     \n       max \n       88.976200 \n       100.000000 \n       27.740000 \n       1.000000 \n       0.871000 \n       8.780000 \n       100.000000 \n       12.126500 \n       24.000000 \n       711.000000 \n       22.000000 \n       396.900000 \n       37.970000 \n       50.000000 \n     \n     \n       range \n       88.969880 \n       100.000000 \n       27.280000 \n       1.000000 \n       0.486000 \n       5.219000 \n       97.100000 \n       10.996900 \n       23.000000 \n       524.000000 \n       9.400000 \n       396.580000 \n       36.240000 \n       45.000000 \n     \n     \n       398 \n       38.351800 \n       0.000000 \n       18.100000 \n       0.000000 \n       0.693000 \n       5.453000 \n       100.000000 \n       1.489600 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.900000 \n       30.590000 \n       5.000000 \n     \n       Findings    The suburb with the lowest median value is 398.  Relative to the other towns, this suburb has high CRIM, ZN below quantile 75%, above mean INDUS, does not bound the Charles river, above mean NOX, RM below quantile 25%, maximum AGE, DIS near to the minimum value, maximum RAD, TAX in quantile 75%, PTRATIO as well, B maximum and LSTAT above quantile 75%.",
            "title": "(g) Suburb with lowest median value of owner occupied homes"
        },
        {
            "location": "/sols/chapter2/exercise10/#h-number-of-rooms-per-dwelling",
            "text": "len(df[df['RM']>7])  64  len(df[df['RM']>8])  13  len(df[df['RM']>8])  13  df[df['RM']>8].describe()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n       13.000000 \n     \n     \n       mean \n       0.718795 \n       13.615385 \n       7.078462 \n       0.153846 \n       0.539238 \n       8.348538 \n       71.538462 \n       3.430192 \n       7.461538 \n       325.076923 \n       16.361538 \n       385.210769 \n       4.310000 \n       44.200000 \n     \n     \n       std \n       0.901640 \n       26.298094 \n       5.392767 \n       0.375534 \n       0.092352 \n       0.251261 \n       24.608723 \n       1.883955 \n       5.332532 \n       110.971063 \n       2.410580 \n       10.529359 \n       1.373566 \n       8.092383 \n     \n     \n       min \n       0.020090 \n       0.000000 \n       2.680000 \n       0.000000 \n       0.416100 \n       8.034000 \n       8.400000 \n       1.801000 \n       2.000000 \n       224.000000 \n       13.000000 \n       354.550000 \n       2.470000 \n       21.900000 \n     \n     \n       25% \n       0.331470 \n       0.000000 \n       3.970000 \n       0.000000 \n       0.504000 \n       8.247000 \n       70.400000 \n       2.288500 \n       5.000000 \n       264.000000 \n       14.700000 \n       384.540000 \n       3.320000 \n       41.700000 \n     \n     \n       50% \n       0.520140 \n       0.000000 \n       6.200000 \n       0.000000 \n       0.507000 \n       8.297000 \n       78.300000 \n       2.894400 \n       7.000000 \n       307.000000 \n       17.400000 \n       386.860000 \n       4.140000 \n       48.300000 \n     \n     \n       75% \n       0.578340 \n       20.000000 \n       6.200000 \n       0.000000 \n       0.605000 \n       8.398000 \n       86.500000 \n       3.651900 \n       8.000000 \n       307.000000 \n       17.400000 \n       389.700000 \n       5.120000 \n       50.000000 \n     \n     \n       max \n       3.474280 \n       95.000000 \n       19.580000 \n       1.000000 \n       0.718000 \n       8.780000 \n       93.900000 \n       8.906700 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.900000 \n       7.440000 \n       50.000000 \n     \n      df.describe()   \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       target \n     \n   \n   \n     \n       count \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n       506.000000 \n     \n     \n       mean \n       3.593761 \n       11.363636 \n       11.136779 \n       0.069170 \n       0.554695 \n       6.284634 \n       68.574901 \n       3.795043 \n       9.549407 \n       408.237154 \n       18.455534 \n       356.674032 \n       12.653063 \n       22.532806 \n     \n     \n       std \n       8.596783 \n       23.322453 \n       6.860353 \n       0.253994 \n       0.115878 \n       0.702617 \n       28.148861 \n       2.105710 \n       8.707259 \n       168.537116 \n       2.164946 \n       91.294864 \n       7.141062 \n       9.197104 \n     \n     \n       min \n       0.006320 \n       0.000000 \n       0.460000 \n       0.000000 \n       0.385000 \n       3.561000 \n       2.900000 \n       1.129600 \n       1.000000 \n       187.000000 \n       12.600000 \n       0.320000 \n       1.730000 \n       5.000000 \n     \n     \n       25% \n       0.082045 \n       0.000000 \n       5.190000 \n       0.000000 \n       0.449000 \n       5.885500 \n       45.025000 \n       2.100175 \n       4.000000 \n       279.000000 \n       17.400000 \n       375.377500 \n       6.950000 \n       17.025000 \n     \n     \n       50% \n       0.256510 \n       0.000000 \n       9.690000 \n       0.000000 \n       0.538000 \n       6.208500 \n       77.500000 \n       3.207450 \n       5.000000 \n       330.000000 \n       19.050000 \n       391.440000 \n       11.360000 \n       21.200000 \n     \n     \n       75% \n       3.647423 \n       12.500000 \n       18.100000 \n       0.000000 \n       0.624000 \n       6.623500 \n       94.075000 \n       5.188425 \n       24.000000 \n       666.000000 \n       20.200000 \n       396.225000 \n       16.955000 \n       25.000000 \n     \n     \n       max \n       88.976200 \n       100.000000 \n       27.740000 \n       1.000000 \n       0.871000 \n       8.780000 \n       100.000000 \n       12.126500 \n       24.000000 \n       711.000000 \n       22.000000 \n       396.900000 \n       37.970000 \n       50.000000 \n     \n       Comments    CRIM is lower,  INDUS proportion is lower,\n* % of lower status of the population (LSTAT) is lower.",
            "title": "h) Number of rooms per dwelling"
        },
        {
            "location": "/sols/chapter3/exercise1/",
            "text": "Exercise 3.1\n\n\nThe t-statistics computed on Table 3.4 are computed individually for each coefficient since they are independent variables. Accordingly, there are 4 null hypotheses that we are testing:\n\n\n\n\nH_0\nH_0\n for \"TV\": \nin the presence of\n Radio and Newspaper ads (and in addition to the intercept), there is no relationship between TV and Sales;\n\n\nH_0\nH_0\n for \"Radio\": \nin the presence of\n TV and Newspaper ads (and in addition to the intercept), there is no relationship between Radio and Sales;\n\n\nH_0\nH_0\n for \"Newspaper\": \nin the presence of\n TV and Radio ads (and in addition to the intercept), there is no relationship between Newspaper and Sales;\n\n\nH_0\nH_0\n for the intercept: \nin the absence of\n TV, Radio and Newspaper ads, Sales are zero;\n\n\n\n\nversus the 4 corresponding alternative hypotheses:\n\n\nH_a\nH_a\n: There is some relationship between TV/Radio/Newspaper and Sales, or Sales are non-zero in the absence of the other variables.\n\n\n\nMathematically, this can be written as\n\n\nH_0:\nH_0:\n \n\\beta_i=0\n\\beta_i=0\n, for \ni = 0,1,2,3\ni = 0,1,2,3\n,\n\n\nversus the 4 corresponding alternative hypotheses\n\n\nH_a:\nH_a:\n \n\\beta_i\\neq0\n\\beta_i\\neq0\n, for \ni = 0,1,2,3\ni = 0,1,2,3\n.\n\n\nAs can been seen on Table 3.4 (and below with Python), for all the variables the p-value is practically zero, except for \nNewspaper\n for which it is very high, namely .86, much larger than the typical confidence levels, 0.05, 0.01 and 0.001.  Given the t-statistics and the p-values we can reject the null hypothesis for the intercept, TV and Radio, but not for Newspaper.\n\n\nThis means that we can conclude that \nthere is a relationship between TV and Sales, and between Radio and Sales\n. Also rejecting \n\\beta_0=0\n\\beta_0=0\n, allows us to conclude that \nin the absence of TV, Radio and Newspaper, Sales are non-zero\n. Not being able to reject the null hypothesis \n\\beta_{Newspaper}=0\n\\beta_{Newspaper}=0\n, suggests that there is indeed \nno relationship between Newspaper and Sales, in the presence of TV and Radio\n.\n\n\nAdditional comment\n\n\nAt a 5% p-value, there would be a 19% chance of having one appear as significant out of 3 variables, even if there was no relationship for all of them. \n\n\n(\n1-.95^4\n1-.95^4\n) \n\n\nAuxiliary calculations\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv('../data/Advertising.csv')\n\nfrom statsmodels.formula.api import ols\nmodel = ols(\"Sales ~ TV + Radio + Newspaper\", df).fit()\nprint(model.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Tue, 24 Oct 2017   Prob (F-statistic):           1.58e-96\nTime:                        10:19:37   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nRadio          0.1885      0.009     21.893      0.000       0.172       0.206\nNewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nFurther reading\n\n\nISL:\n\n\n\n\nPage 67, 68\n\n\nFootnote page 68\n\n\n\n\nH_0\nH_0\n:\n\n\n\n\nhttp://quant.stackexchange.com/questions/16056/null-and-alternative-hypothesis-for-multiple-linear-regression\n\n\n\n\nMultiple regression:\n\n\n\n\nhttps://www.datarobot.com/blog/multiple-regression-using-statsmodels/\n\n\nhttps://www.coursera.org/learn/regression-modeling-practice/lecture/xQRab/python-lesson-1-multiple-regression\n\n\nhttp://www.scipy-lectures.org/packages/statistics/index.html#multiple-regression-including-multiple-factors\n\n\nhttp://stackoverflow.com/questions/11479064/multiple-linear-regression-in-python",
            "title": "3.1"
        },
        {
            "location": "/sols/chapter3/exercise1/#exercise-31",
            "text": "The t-statistics computed on Table 3.4 are computed individually for each coefficient since they are independent variables. Accordingly, there are 4 null hypotheses that we are testing:   H_0 H_0  for \"TV\":  in the presence of  Radio and Newspaper ads (and in addition to the intercept), there is no relationship between TV and Sales;  H_0 H_0  for \"Radio\":  in the presence of  TV and Newspaper ads (and in addition to the intercept), there is no relationship between Radio and Sales;  H_0 H_0  for \"Newspaper\":  in the presence of  TV and Radio ads (and in addition to the intercept), there is no relationship between Newspaper and Sales;  H_0 H_0  for the intercept:  in the absence of  TV, Radio and Newspaper ads, Sales are zero;   versus the 4 corresponding alternative hypotheses:  H_a H_a : There is some relationship between TV/Radio/Newspaper and Sales, or Sales are non-zero in the absence of the other variables.  \nMathematically, this can be written as  H_0: H_0:   \\beta_i=0 \\beta_i=0 , for  i = 0,1,2,3 i = 0,1,2,3 ,  versus the 4 corresponding alternative hypotheses  H_a: H_a:   \\beta_i\\neq0 \\beta_i\\neq0 , for  i = 0,1,2,3 i = 0,1,2,3 .  As can been seen on Table 3.4 (and below with Python), for all the variables the p-value is practically zero, except for  Newspaper  for which it is very high, namely .86, much larger than the typical confidence levels, 0.05, 0.01 and 0.001.  Given the t-statistics and the p-values we can reject the null hypothesis for the intercept, TV and Radio, but not for Newspaper.  This means that we can conclude that  there is a relationship between TV and Sales, and between Radio and Sales . Also rejecting  \\beta_0=0 \\beta_0=0 , allows us to conclude that  in the absence of TV, Radio and Newspaper, Sales are non-zero . Not being able to reject the null hypothesis  \\beta_{Newspaper}=0 \\beta_{Newspaper}=0 , suggests that there is indeed  no relationship between Newspaper and Sales, in the presence of TV and Radio .",
            "title": "Exercise 3.1"
        },
        {
            "location": "/sols/chapter3/exercise1/#additional-comment",
            "text": "At a 5% p-value, there would be a 19% chance of having one appear as significant out of 3 variables, even if there was no relationship for all of them.   ( 1-.95^4 1-.95^4 )",
            "title": "Additional comment"
        },
        {
            "location": "/sols/chapter3/exercise1/#auxiliary-calculations",
            "text": "import pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv('../data/Advertising.csv')\n\nfrom statsmodels.formula.api import ols\nmodel = ols(\"Sales ~ TV + Radio + Newspaper\", df).fit()\nprint(model.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Tue, 24 Oct 2017   Prob (F-statistic):           1.58e-96\nTime:                        10:19:37   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nRadio          0.1885      0.009     21.893      0.000       0.172       0.206\nNewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "title": "Auxiliary calculations"
        },
        {
            "location": "/sols/chapter3/exercise1/#further-reading",
            "text": "ISL:   Page 67, 68  Footnote page 68   H_0 H_0 :   http://quant.stackexchange.com/questions/16056/null-and-alternative-hypothesis-for-multiple-linear-regression   Multiple regression:   https://www.datarobot.com/blog/multiple-regression-using-statsmodels/  https://www.coursera.org/learn/regression-modeling-practice/lecture/xQRab/python-lesson-1-multiple-regression  http://www.scipy-lectures.org/packages/statistics/index.html#multiple-regression-including-multiple-factors  http://stackoverflow.com/questions/11479064/multiple-linear-regression-in-python",
            "title": "Further reading"
        },
        {
            "location": "/sols/chapter3/exercise2/",
            "text": "Exercise 3.2\n\n\nBoth models share a similar principle: the output values are computed based on their K closest points (nearest neighbours) value.\n\n\nThe KNN classifier starts by identifying the K nearest neighbours. Then, the output of each K observation is considered and, by majority vote, we determine the label of our observation. Example: if we are trying to classify an observation as 'blue' or 'red', and in the K nearest neighbours we have two of them classified as 'blue' and one as 'red', our observation will be classified as 'blue'. Notice that if we have a tie, a common solution is to increase or decrease K.\n\n\nRegarding the KNN regression method, it also starts by identifying the K nearest neighbours. However, in this situation, we compute our output averaging the output of the K nearest neighbours. Example: if our K nearest neighbours have as output the values 3,4 and 5, our output will be (3+4+5)/3 = 4.",
            "title": "3.2"
        },
        {
            "location": "/sols/chapter3/exercise2/#exercise-32",
            "text": "Both models share a similar principle: the output values are computed based on their K closest points (nearest neighbours) value.  The KNN classifier starts by identifying the K nearest neighbours. Then, the output of each K observation is considered and, by majority vote, we determine the label of our observation. Example: if we are trying to classify an observation as 'blue' or 'red', and in the K nearest neighbours we have two of them classified as 'blue' and one as 'red', our observation will be classified as 'blue'. Notice that if we have a tie, a common solution is to increase or decrease K.  Regarding the KNN regression method, it also starts by identifying the K nearest neighbours. However, in this situation, we compute our output averaging the output of the K nearest neighbours. Example: if our K nearest neighbours have as output the values 3,4 and 5, our output will be (3+4+5)/3 = 4.",
            "title": "Exercise 3.2"
        },
        {
            "location": "/sols/chapter3/exercise3/",
            "text": "Exercise 3.3\n\n\n(a)\n\n\nY = \\beta_0 + \\beta_1 \\times GPA + \\beta_2 \\times IQ + \\beta_3 \\times Gender + \\beta_4 \\times GPA \\times IQ + \\beta_5 \\times GPA \\times Gender\nY = \\beta_0 + \\beta_1 \\times GPA + \\beta_2 \\times IQ + \\beta_3 \\times Gender + \\beta_4 \\times GPA \\times IQ + \\beta_5 \\times GPA \\times Gender\n\n\nFor a fixed value of GPA and IQ, the difference between female and male is given by:\n\n\nY_{female} - Y_{male} = \\beta_3 + \\beta_5 \\times GPA = 35 - 10 GPA\nY_{female} - Y_{male} = \\beta_3 + \\beta_5 \\times GPA = 35 - 10 GPA\n, \n\n\nwhich depends on GPA. It is clear that in the normal range of the GPA (0 to 4.0), the difference in expected salary between female and male ranges linearly from 35 to -5. In particular, if GPA > 3.5, males earn on average more than females. Therefore, \n the correct answer is (iii)\n.\n\n\n(b)\n\n\nThe \npredicted salary is 137.1 (thousand dollars)\n. Given the coefficients from the fit, GPA = 4.0, IQ = 110 and Gender = 1, the model predicts:\n\n\nY = \\beta_0 + \\beta_1 GPA + \\beta_2 IQ + \\beta_3 Gender + \\beta_4 (GPA \\times IQ) + \\beta_5 (GPA \\times Gender)\nY = \\beta_0 + \\beta_1 GPA + \\beta_2 IQ + \\beta_3 Gender + \\beta_4 (GPA \\times IQ) + \\beta_5 (GPA \\times Gender)\n\n\nY = 50 + 20 \\times 4.0 + 0.07 \\times 110 + 35 \\times 1  + 0.01 \\times 4.0 \\times 110 + (-10) \\times 4.0 \\times 1\nY = 50 + 20 \\times 4.0 + 0.07 \\times 110 + 35 \\times 1  + 0.01 \\times 4.0 \\times 110 + (-10) \\times 4.0 \\times 1\n = 137.1\n\n\n(c)\n\n\nFalse\n. Although the coefficient for the GPA/IQ interaction term is very small, specially when compared to the other coefficients, this does not indicate whether there is an interaction effect. First, this coefficient is multiplied by the product  of IQ and GPA which ranges from 0 to a few hundred, so that the contribution to the response would tipically add up to a value between 2 and 6, let's say. Secondly, and more importantly, evidence for the interaction effect has to be evaluated with a t-statistic or an F-statistic for a null hypothesis (\nH_0: \\beta_4 = 0\nH_0: \\beta_4 = 0\n), yielding a certain p-value. This requires the standard error of which we have no information, and therefore cannot conclude whether there is evidence for a interaction effect.\n\n\nAdditional calculations\n\n\n50 + 20*4 + 0.07*110 + 35*1 + 0.01*4*110 + (-10)*4*1\n\n\n\n\n137.1",
            "title": "3.3"
        },
        {
            "location": "/sols/chapter3/exercise3/#exercise-33",
            "text": "",
            "title": "Exercise 3.3"
        },
        {
            "location": "/sols/chapter3/exercise3/#a",
            "text": "Y = \\beta_0 + \\beta_1 \\times GPA + \\beta_2 \\times IQ + \\beta_3 \\times Gender + \\beta_4 \\times GPA \\times IQ + \\beta_5 \\times GPA \\times Gender Y = \\beta_0 + \\beta_1 \\times GPA + \\beta_2 \\times IQ + \\beta_3 \\times Gender + \\beta_4 \\times GPA \\times IQ + \\beta_5 \\times GPA \\times Gender  For a fixed value of GPA and IQ, the difference between female and male is given by:  Y_{female} - Y_{male} = \\beta_3 + \\beta_5 \\times GPA = 35 - 10 GPA Y_{female} - Y_{male} = \\beta_3 + \\beta_5 \\times GPA = 35 - 10 GPA ,   which depends on GPA. It is clear that in the normal range of the GPA (0 to 4.0), the difference in expected salary between female and male ranges linearly from 35 to -5. In particular, if GPA > 3.5, males earn on average more than females. Therefore,   the correct answer is (iii) .",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise3/#b",
            "text": "The  predicted salary is 137.1 (thousand dollars) . Given the coefficients from the fit, GPA = 4.0, IQ = 110 and Gender = 1, the model predicts:  Y = \\beta_0 + \\beta_1 GPA + \\beta_2 IQ + \\beta_3 Gender + \\beta_4 (GPA \\times IQ) + \\beta_5 (GPA \\times Gender) Y = \\beta_0 + \\beta_1 GPA + \\beta_2 IQ + \\beta_3 Gender + \\beta_4 (GPA \\times IQ) + \\beta_5 (GPA \\times Gender)  Y = 50 + 20 \\times 4.0 + 0.07 \\times 110 + 35 \\times 1  + 0.01 \\times 4.0 \\times 110 + (-10) \\times 4.0 \\times 1 Y = 50 + 20 \\times 4.0 + 0.07 \\times 110 + 35 \\times 1  + 0.01 \\times 4.0 \\times 110 + (-10) \\times 4.0 \\times 1  = 137.1",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise3/#c",
            "text": "False . Although the coefficient for the GPA/IQ interaction term is very small, specially when compared to the other coefficients, this does not indicate whether there is an interaction effect. First, this coefficient is multiplied by the product  of IQ and GPA which ranges from 0 to a few hundred, so that the contribution to the response would tipically add up to a value between 2 and 6, let's say. Secondly, and more importantly, evidence for the interaction effect has to be evaluated with a t-statistic or an F-statistic for a null hypothesis ( H_0: \\beta_4 = 0 H_0: \\beta_4 = 0 ), yielding a certain p-value. This requires the standard error of which we have no information, and therefore cannot conclude whether there is evidence for a interaction effect.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise3/#additional-calculations",
            "text": "50 + 20*4 + 0.07*110 + 35*1 + 0.01*4*110 + (-10)*4*1  137.1",
            "title": "Additional calculations"
        },
        {
            "location": "/sols/chapter3/exercise4/",
            "text": "Exercise 3.4\n\n\n(a) \nCubic regression will have lower Residual Sum of Squares (RSS).\n The cubic regression model is more flexible than the linear regression model. Accordingly, the cubic regression model can fit the data better and achieve a lower training RSS than the linear regression model.\n\n\n(b) \nLinear regression will have lower RSS. \n In general, more flexible models have less bias and higher variance. By contrast, more rigid models have high bias and lower variance. Since it is said that true relationship between the predictor and the response is linear, we know that, in this case, the linear regression model will have low bias. Consequently, this model will perform better than the cubic regression model, which is expected to have higher variance. \n\n\n(c) \n Cubic regression will have lower RSS. \n Same reason as in (a). Since the model is more flexible, it is able to fit the data better.\n\n\n(d) \nNot enough information to tell.\n Due to its flexibility, it is generally expected that the cubic regression model has lower bias and higher variance than the linear regression model. In this exercise, we know that the true relationship is non-linear, but we don't know how far it is from linear. This means that we don't have any idea about how high the bias of the linear regression model can be. If the model is just slightly non-linear, the linear regression will be able to model the data and achieve low bias. Thus, we would expect the linear model to have low bias and low variance. This could be enough (or not) to beat the cubic regression model, which is expected to have low bias and high variance. However, if the true relationship is substantially non-linear, the linear model will not be able to model the data and its bias will be high. With high bias and low variance, the linear regression model is beaten by a cubic model without \noverfitting\n problems. It will always depend on the bias-variance trade-off and, in general, on the size of the training set and the magnitude of the noise. We would need more information to know which model would have lower RSS.",
            "title": "3.4"
        },
        {
            "location": "/sols/chapter3/exercise4/#exercise-34",
            "text": "(a)  Cubic regression will have lower Residual Sum of Squares (RSS).  The cubic regression model is more flexible than the linear regression model. Accordingly, the cubic regression model can fit the data better and achieve a lower training RSS than the linear regression model.  (b)  Linear regression will have lower RSS.   In general, more flexible models have less bias and higher variance. By contrast, more rigid models have high bias and lower variance. Since it is said that true relationship between the predictor and the response is linear, we know that, in this case, the linear regression model will have low bias. Consequently, this model will perform better than the cubic regression model, which is expected to have higher variance.   (c)   Cubic regression will have lower RSS.   Same reason as in (a). Since the model is more flexible, it is able to fit the data better.  (d)  Not enough information to tell.  Due to its flexibility, it is generally expected that the cubic regression model has lower bias and higher variance than the linear regression model. In this exercise, we know that the true relationship is non-linear, but we don't know how far it is from linear. This means that we don't have any idea about how high the bias of the linear regression model can be. If the model is just slightly non-linear, the linear regression will be able to model the data and achieve low bias. Thus, we would expect the linear model to have low bias and low variance. This could be enough (or not) to beat the cubic regression model, which is expected to have low bias and high variance. However, if the true relationship is substantially non-linear, the linear model will not be able to model the data and its bias will be high. With high bias and low variance, the linear regression model is beaten by a cubic model without  overfitting  problems. It will always depend on the bias-variance trade-off and, in general, on the size of the training set and the magnitude of the noise. We would need more information to know which model would have lower RSS.",
            "title": "Exercise 3.4"
        },
        {
            "location": "/sols/chapter3/exercise5/",
            "text": "Exercise 3.5\n\n\nThis is a simple exercise of plugging the expression for \n\\hat{\\beta}\n\\hat{\\beta}\n in the formula of the ith fitted value:\n\n\n\n\n\n\\hat{y}_i = x_i \\hat{\\beta} = x_i \\left( \\sum_{i'=1}^n x_{i'} y_{i'} \\right) / \\left( \\sum_{j=1}^n x_j^2 \\right) = \\sum_{i'=1}^n \\left( \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2} y_{i'} \\right)\n\n\n\n\n\\hat{y}_i = x_i \\hat{\\beta} = x_i \\left( \\sum_{i'=1}^n x_{i'} y_{i'} \\right) / \\left( \\sum_{j=1}^n x_j^2 \\right) = \\sum_{i'=1}^n \\left( \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2} y_{i'} \\right)\n\n\n\n\n\nand comparing to the expression\n\n\n\n\n\n\\hat{y}_i = \\sum_{i'=1}^n a_{i'} y_{i'}\n\n\n\n\n\\hat{y}_i = \\sum_{i'=1}^n a_{i'} y_{i'}\n\n\n\n\n\nto obtain:\n\n\n\n\n\na_{i'} =  \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2}\n\n\n\n\na_{i'} =  \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2}",
            "title": "3.5"
        },
        {
            "location": "/sols/chapter3/exercise5/#exercise-35",
            "text": "This is a simple exercise of plugging the expression for  \\hat{\\beta} \\hat{\\beta}  in the formula of the ith fitted value:   \n\\hat{y}_i = x_i \\hat{\\beta} = x_i \\left( \\sum_{i'=1}^n x_{i'} y_{i'} \\right) / \\left( \\sum_{j=1}^n x_j^2 \\right) = \\sum_{i'=1}^n \\left( \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2} y_{i'} \\right)  \n\\hat{y}_i = x_i \\hat{\\beta} = x_i \\left( \\sum_{i'=1}^n x_{i'} y_{i'} \\right) / \\left( \\sum_{j=1}^n x_j^2 \\right) = \\sum_{i'=1}^n \\left( \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2} y_{i'} \\right)   and comparing to the expression   \n\\hat{y}_i = \\sum_{i'=1}^n a_{i'} y_{i'}  \n\\hat{y}_i = \\sum_{i'=1}^n a_{i'} y_{i'}   to obtain:   \na_{i'} =  \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2}  \na_{i'} =  \\frac{x_i x_{i'} }{\\sum_{j=1}^n x_j^2}",
            "title": "Exercise 3.5"
        },
        {
            "location": "/sols/chapter3/exercise6/",
            "text": "Exercise 3.6\n\n\nThe least squares line is given by:\n\n\n\n\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times x\n\n\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times x\n\n\n\n\nwhere \n\\hat{\\beta_0}\n\\hat{\\beta_0}\n and \n\\hat{\\beta_1}\n\\hat{\\beta_1}\n are the least squares coefficient estimates for simple linear regression.\n\n\nBy definition, \n\\hat{\\beta_0}\n\\hat{\\beta_0}\n is:\n\n\n\n\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x}\n\n\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x}\n\n\n\n\nwhere \n\\bar{y}\n\\bar{y}\n and \n\\bar{x}\n\\bar{x}\n are the average values of \ny\ny\n and \nx\nx\n, respectively.\n\n\nSince we want to know if the least squares line always passes through the point (\n\\bar{x}\n\\bar{x}\n, \n\\bar{y}\n\\bar{y}\n), all we have to do is to substitute (\n\\bar{x}\n\\bar{x}\n, \n\\bar{y}\n\\bar{y}\n) into the first equation above and see if the condition is satisfied. We get:\n\n\n\n\n\\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times \\bar{x}\n\n\n\\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times \\bar{x}\n\n\n\n\nand substituting the expression above for \n\\hat{\\beta_0}\n\\hat{\\beta_0}\n, we obtain:\n\n\n\n\n\\bar{y} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x} + \\hat{\\beta_1} \\times \\bar{x}\n\n\n\\bar{y} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x} + \\hat{\\beta_1} \\times \\bar{x}\n\n\n\n\nSince this is always true, we conclude that the least squares line always passes through the point (\n\\bar{x}\n\\bar{x}\n, \n\\bar{y}\n\\bar{y}\n).",
            "title": "3.6"
        },
        {
            "location": "/sols/chapter3/exercise6/#exercise-36",
            "text": "The least squares line is given by:   \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times x  \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times x   where  \\hat{\\beta_0} \\hat{\\beta_0}  and  \\hat{\\beta_1} \\hat{\\beta_1}  are the least squares coefficient estimates for simple linear regression.  By definition,  \\hat{\\beta_0} \\hat{\\beta_0}  is:   \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x}  \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x}   where  \\bar{y} \\bar{y}  and  \\bar{x} \\bar{x}  are the average values of  y y  and  x x , respectively.  Since we want to know if the least squares line always passes through the point ( \\bar{x} \\bar{x} ,  \\bar{y} \\bar{y} ), all we have to do is to substitute ( \\bar{x} \\bar{x} ,  \\bar{y} \\bar{y} ) into the first equation above and see if the condition is satisfied. We get:   \\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times \\bar{x}  \\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times \\bar{x}   and substituting the expression above for  \\hat{\\beta_0} \\hat{\\beta_0} , we obtain:   \\bar{y} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x} + \\hat{\\beta_1} \\times \\bar{x}  \\bar{y} = \\bar{y} - \\hat{\\beta_1} \\times \\bar{x} + \\hat{\\beta_1} \\times \\bar{x}   Since this is always true, we conclude that the least squares line always passes through the point ( \\bar{x} \\bar{x} ,  \\bar{y} \\bar{y} ).",
            "title": "Exercise 3.6"
        },
        {
            "location": "/sols/chapter3/exercise7/",
            "text": "Exercise 3.7\n\n\nTo show that  \nR^2=r^2 \\equiv cor^2(X,Y)\nR^2=r^2 \\equiv cor^2(X,Y)\n for simple linear regression of \nY\nY\n onto \nX\nX\n, we will use the following formulas from the text (where we have used the simplifying assumption that \n\\bar{x}=\\bar{y}=0\n\\bar{x}=\\bar{y}=0\n):\n\n\n\n\n\n\\begin{align}\ncor(X,Y) & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}} = \\frac{\\sum_i x_i y_i}{\\sqrt{\\sum_i x_i^2}\\sqrt{\\sum_i y_i^2}},\\\\\n\\hat{\\beta}_0 & = \\bar{y} -\\hat{\\beta}_1 \\bar{x} = 0, \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i(x_i - \\bar{x})^2} = \\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}, \\\\\nR^2 & = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_i(y_i - \\hat{y})^2}{\\sum_i(y_i - \\bar{y})^2}= 1 - \\frac{\\sum_i(y_i - \\hat{\\beta}_1 x_i)^2}{\\sum_i y_i^2}.\n\\end{align}\n\n\n\n\n\\begin{align}\ncor(X,Y) & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}} = \\frac{\\sum_i x_i y_i}{\\sqrt{\\sum_i x_i^2}\\sqrt{\\sum_i y_i^2}},\\\\\n\\hat{\\beta}_0 & = \\bar{y} -\\hat{\\beta}_1 \\bar{x} = 0, \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i(x_i - \\bar{x})^2} = \\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}, \\\\\nR^2 & = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_i(y_i - \\hat{y})^2}{\\sum_i(y_i - \\bar{y})^2}= 1 - \\frac{\\sum_i(y_i - \\hat{\\beta}_1 x_i)^2}{\\sum_i y_i^2}.\n\\end{align}\n\n\n\n\n\nExpanding the numerator in the last expression and plugging in the formula for \n\\hat{\\beta}_1\n\\hat{\\beta}_1\n, we obtain\n\n\n\n\n\n\\begin{align}\nR^2 & = 1 - \\frac{\\sum_i(y_i^2 - 2\\hat{\\beta}_1 x_i y_i + \\hat{\\beta}_1^2 x_i^2 )}{\\sum_i y_i^2} = \\frac{ 2\\hat{\\beta}_1 \\sum_ i x_i y_i - \\hat{\\beta}_1^2 \\sum_i x_i^2 }{\\sum_i y_i^2} = \\\\\n  & = \\frac{ 2 (\\sum_i x_i y_i) ( \\sum_ i x_i y_i)/ (\\sum_i x_i^2) -( \\sum_ i x_i y_i)^2 (\\sum_i x_i^2)/ (\\sum_i x_i^2)^2  }{\\sum_i y_i^2} = \\frac{(\\sum_i x_i y_i)^2}{\\sum_i x_i^2 \\sum_i y_i^2},\n\\end{align}\n\n\n\n\n\\begin{align}\nR^2 & = 1 - \\frac{\\sum_i(y_i^2 - 2\\hat{\\beta}_1 x_i y_i + \\hat{\\beta}_1^2 x_i^2 )}{\\sum_i y_i^2} = \\frac{ 2\\hat{\\beta}_1 \\sum_ i x_i y_i - \\hat{\\beta}_1^2 \\sum_i x_i^2 }{\\sum_i y_i^2} = \\\\\n  & = \\frac{ 2 (\\sum_i x_i y_i) ( \\sum_ i x_i y_i)/ (\\sum_i x_i^2) -( \\sum_ i x_i y_i)^2 (\\sum_i x_i^2)/ (\\sum_i x_i^2)^2  }{\\sum_i y_i^2} = \\frac{(\\sum_i x_i y_i)^2}{\\sum_i x_i^2 \\sum_i y_i^2},\n\\end{align}\n\n\n\n\n\nwhich we can see is equal to \ncor^2(X,Y)\ncor^2(X,Y)\n.",
            "title": "3.7"
        },
        {
            "location": "/sols/chapter3/exercise7/#exercise-37",
            "text": "To show that   R^2=r^2 \\equiv cor^2(X,Y) R^2=r^2 \\equiv cor^2(X,Y)  for simple linear regression of  Y Y  onto  X X , we will use the following formulas from the text (where we have used the simplifying assumption that  \\bar{x}=\\bar{y}=0 \\bar{x}=\\bar{y}=0 ):   \n\\begin{align}\ncor(X,Y) & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}} = \\frac{\\sum_i x_i y_i}{\\sqrt{\\sum_i x_i^2}\\sqrt{\\sum_i y_i^2}},\\\\\n\\hat{\\beta}_0 & = \\bar{y} -\\hat{\\beta}_1 \\bar{x} = 0, \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i(x_i - \\bar{x})^2} = \\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}, \\\\\nR^2 & = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_i(y_i - \\hat{y})^2}{\\sum_i(y_i - \\bar{y})^2}= 1 - \\frac{\\sum_i(y_i - \\hat{\\beta}_1 x_i)^2}{\\sum_i y_i^2}.\n\\end{align}  \n\\begin{align}\ncor(X,Y) & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}} = \\frac{\\sum_i x_i y_i}{\\sqrt{\\sum_i x_i^2}\\sqrt{\\sum_i y_i^2}},\\\\\n\\hat{\\beta}_0 & = \\bar{y} -\\hat{\\beta}_1 \\bar{x} = 0, \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i(x_i - \\bar{x})^2} = \\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}, \\\\\nR^2 & = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_i(y_i - \\hat{y})^2}{\\sum_i(y_i - \\bar{y})^2}= 1 - \\frac{\\sum_i(y_i - \\hat{\\beta}_1 x_i)^2}{\\sum_i y_i^2}.\n\\end{align}   Expanding the numerator in the last expression and plugging in the formula for  \\hat{\\beta}_1 \\hat{\\beta}_1 , we obtain   \n\\begin{align}\nR^2 & = 1 - \\frac{\\sum_i(y_i^2 - 2\\hat{\\beta}_1 x_i y_i + \\hat{\\beta}_1^2 x_i^2 )}{\\sum_i y_i^2} = \\frac{ 2\\hat{\\beta}_1 \\sum_ i x_i y_i - \\hat{\\beta}_1^2 \\sum_i x_i^2 }{\\sum_i y_i^2} = \\\\\n  & = \\frac{ 2 (\\sum_i x_i y_i) ( \\sum_ i x_i y_i)/ (\\sum_i x_i^2) -( \\sum_ i x_i y_i)^2 (\\sum_i x_i^2)/ (\\sum_i x_i^2)^2  }{\\sum_i y_i^2} = \\frac{(\\sum_i x_i y_i)^2}{\\sum_i x_i^2 \\sum_i y_i^2},\n\\end{align}  \n\\begin{align}\nR^2 & = 1 - \\frac{\\sum_i(y_i^2 - 2\\hat{\\beta}_1 x_i y_i + \\hat{\\beta}_1^2 x_i^2 )}{\\sum_i y_i^2} = \\frac{ 2\\hat{\\beta}_1 \\sum_ i x_i y_i - \\hat{\\beta}_1^2 \\sum_i x_i^2 }{\\sum_i y_i^2} = \\\\\n  & = \\frac{ 2 (\\sum_i x_i y_i) ( \\sum_ i x_i y_i)/ (\\sum_i x_i^2) -( \\sum_ i x_i y_i)^2 (\\sum_i x_i^2)/ (\\sum_i x_i^2)^2  }{\\sum_i y_i^2} = \\frac{(\\sum_i x_i y_i)^2}{\\sum_i x_i^2 \\sum_i y_i^2},\n\\end{align}   which we can see is equal to  cor^2(X,Y) cor^2(X,Y) .",
            "title": "Exercise 3.7"
        },
        {
            "location": "/sols/chapter3/exercise8/",
            "text": "Exercise 3.8\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf  # Statsmodels is a Python module for statistics\nimport statsmodels.api as sm\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/auto.csv') # import dataset\n\n\n\n\ndf.head() # just to have a look\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.0\n\n      \n8\n\n      \n307.0\n\n      \n130\n\n      \n3504\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.0\n\n      \n8\n\n      \n350.0\n\n      \n165\n\n      \n3693\n\n      \n11.5\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.0\n\n      \n8\n\n      \n318.0\n\n      \n150\n\n      \n3436\n\n      \n11.0\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.0\n\n      \n8\n\n      \n304.0\n\n      \n150\n\n      \n3433\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.0\n\n      \n8\n\n      \n302.0\n\n      \n140\n\n      \n3449\n\n      \n10.5\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\n# prepare data for modelling (training set)\nX_train = df['horsepower'] # horsepower as predictor\ny_train = df['mpg']        # mpg as response\n\n\n\n\nX_train.head() # just to check\n\n\n\n\n0    130\n1    165\n2    150\n3    150\n4    140\nName: horsepower, dtype: object\n\n\n\ny_train.head() # just to check\n\n\n\n\n0    18.0\n1    15.0\n2    18.0\n3    16.0\n4    17.0\nName: mpg, dtype: float64\n\n\n\nX_train.unique() # check for data quality (missing data, errors, etc.)\n\n\n\n\narray(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)\n\n\n\n\n\nThere is a strange value: \n'?'\n\n\n\n\n# handling with '?' observations (delete them)\ndroplist = X_train[X_train == '?'].index # get index of observations with '?' value\nX_train = X_train.drop(droplist)         # drop those observations\ny_train = y_train.drop(droplist)         # ensure dimensional compatibility between variables\nX_train.unique()                         # test\n\n\n\n\narray(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '100', '105', '175', '153', '180', '110', '72',\n       '86', '70', '76', '65', '69', '60', '80', '54', '208', '155', '112',\n       '92', '145', '137', '158', '167', '94', '107', '230', '49', '75',\n       '91', '122', '67', '83', '78', '52', '61', '93', '148', '129', '96',\n       '71', '98', '115', '53', '81', '79', '120', '152', '102', '108',\n       '68', '58', '149', '89', '63', '48', '66', '139', '103', '125',\n       '133', '138', '135', '142', '77', '62', '132', '84', '64', '74',\n       '116', '82'], dtype=object)\n\n\n\ny_train.unique() # check for data quality (missing data, errors, etc.)\n\n\n\n\narray([ 18. ,  15. ,  16. ,  17. ,  14. ,  24. ,  22. ,  21. ,  27. ,\n        26. ,  25. ,  10. ,  11. ,   9. ,  28. ,  19. ,  12. ,  13. ,\n        23. ,  30. ,  31. ,  35. ,  20. ,  29. ,  32. ,  33. ,  17.5,\n        15.5,  14.5,  22.5,  24.5,  18.5,  29.5,  26.5,  16.5,  31.5,\n        36. ,  25.5,  33.5,  20.5,  30.5,  21.5,  43.1,  36.1,  32.8,\n        39.4,  19.9,  19.4,  20.2,  19.2,  25.1,  20.6,  20.8,  18.6,\n        18.1,  17.7,  27.5,  27.2,  30.9,  21.1,  23.2,  23.8,  23.9,\n        20.3,  21.6,  16.2,  19.8,  22.3,  17.6,  18.2,  16.9,  31.9,\n        34.1,  35.7,  27.4,  25.4,  34.2,  34.5,  31.8,  37.3,  28.4,\n        28.8,  26.8,  41.5,  38.1,  32.1,  37.2,  26.4,  24.3,  19.1,\n        34.3,  29.8,  31.3,  37. ,  32.2,  46.6,  27.9,  40.8,  44.3,\n        43.4,  36.4,  44.6,  33.8,  32.7,  23.7,  32.4,  26.6,  25.8,\n        23.5,  39.1,  39. ,  35.1,  32.3,  37.7,  34.7,  34.4,  29.9,\n        33.7,  32.9,  31.6,  28.1,  30.7,  24.2,  22.4,  34. ,  38. ,  44. ])\n\n\n\n# create dataframe to use statsmodel\nd = {'horsepower':X_train.astype('float'), 'mpg':y_train}\ndf = pd.DataFrame(data=d)\ndf.head()\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nhorsepower\n\n      \nmpg\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n130.0\n\n      \n18.0\n\n    \n\n    \n\n      \n1\n\n      \n165.0\n\n      \n15.0\n\n    \n\n    \n\n      \n2\n\n      \n150.0\n\n      \n18.0\n\n    \n\n    \n\n      \n3\n\n      \n150.0\n\n      \n16.0\n\n    \n\n    \n\n      \n4\n\n      \n140.0\n\n      \n17.0\n\n    \n\n  \n\n\n\n\n\n\n\n# plot\nplt.scatter(X_train,y_train);\n\n\n\n\n\n\n# using statsmodel for linear regression (http://statsmodels.sourceforge.net/)\nmod = smf.ols(formula='mpg ~ horsepower', data = df)\nres = mod.fit()\nprint(res.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.606\nModel:                            OLS   Adj. R-squared:                  0.605\nMethod:                 Least Squares   F-statistic:                     599.7\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           7.03e-81\nTime:                        09:48:30   Log-Likelihood:                -1178.7\nNo. Observations:                 392   AIC:                             2361.\nDf Residuals:                     390   BIC:                             2369.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     39.9359      0.717     55.660      0.000      38.525      41.347\nhorsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n==============================================================================\nOmnibus:                       16.432   Durbin-Watson:                   0.920\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\nSkew:                           0.492   Prob(JB):                     0.000175\nKurtosis:                       3.299   Cond. No.                         322.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ni.\n\n\nYes, there is a relationship between the predictor and the response. We can reject the null hypothesis that the regression coefficients are zero, since the F-statistic is much larger than 1 and the p-value is close to zero.\n\n\nii\n\n\nSince the R-squared value is just about 0.606, we can say that approximately 60% of the variance in mpg is explained by horsepower.\n\n\niii\n\n\nThe relationship is negative because the coefficient corresponding to horsepower is equal to -0.1578.\n\n\niv\n\n\nThis exercise is a bit trickier to do in Python, since there's no standard out-of-the-box function for it (please let us know if you could find one). If we were solving this exercise in R, we would just need to use the functions confint() and predict() (see bottom of page 111 of ISLR). \n\n\nOn page 82 of ISLR, they describe the qualitative difference between a confidence and a prediction interval. However, the text does not explain how to calculate the intervals.\nFor a good description and derivation on how to calculate these intervals see Casella and Berger [1], section 11.3.5 (\"Estimation and Prediction at a Specified x=x0\"). \nThe distinction between the two types of interval is also well laid out in the answer [2], adapted from section 4.1 of Faraway [3].\n\n\nThe answer is calculated below. \nThe predicted 'mpg' is equal to 24.46708, with a 95% confidence interval of (23.97308, 24.96108) and a 95% prediction interval of (14.80940, 34.12476).\nThese are the same values as calculated with the R functions confint() and predict().\n\n\n# this code follows Chapter 11 of Casella and Berger [1]\n# below we comment the lines with the page numbers of the text where the corresponding formulas appear\n\nfrom scipy.stats import t\nfrom math import sqrt\n\ndef interval(x, y, x0,alpha = .05):\n    n = np.size(x)\n    x_bar = np.mean(x)\n    y_bar = np.mean(y)\n    S_xx = np.sum((x-x_bar)**2)         # page 541\n    S_xy = np.sum((x-x_bar)*(y-y_bar))  # page 541\n    b = S_xy/S_xx                       # page 542\n    a = y_bar - b*x_bar                 # page 542\n    S2 = np.sum((y-a-b*x)**2)/(n-2)     # page 552\n    S = sqrt(S2)\n    ts = t.ppf(1-alpha/2, n-2)\n    w_conf = ts*S*sqrt(1/n + (x0-x_bar)**2/S_xx)      # page 558\n    w_pred = ts*S*sqrt(1 + 1/n + (x0-x_bar)**2/S_xx)  # page 559\n    print(\"                fit \\t lwr \\t  upr\")\n    print(\"confidence %3.5f %3.5f %3.5f\" % (a+b*x0, a+b*x0 - w_conf, a+b*x0 + w_conf))\n    print(\"prediction %3.5f %3.5f %3.5f\" % (a+b*x0, a+b*x0 - w_pred, a+b*x0 + w_pred))\n\nx = df['horsepower']\ny = df['mpg']\nx0 = 98    \n\ninterval(x, y, x0)\n\n\n\n\n                fit      lwr      upr\nconfidence 24.46708 23.97308 24.96108\nprediction 24.46708 14.80940 34.12476\n\n\n\nReferences\n\n\n[1] Casella, George, and Roger L. Berger. Statistical inference. Vol. 2. Pacific Grove, CA: Duxbury, 2002. \namazon\n\n\n[2] \nhttps://stats.stackexchange.com/a/271232\n\n\n[3] Faraway, Julian J. Linear models with R. CRC press, 2014.  \namazon\n\n\n(b)\n\n\nplt.subplots(1,1) #to get both plots in the same figure \nplt.scatter(X_train.astype('float'), y_train);\nplt.plot(X_train.astype('float'), res.fittedvalues, color='red');\n\n\n\n\n\n\n(c)\n\n\nIn R [4], by default, plot() on a fit produces 4 plots: \n * a plot of residuals against fitted values,\n * a Scale-Location plot of sqrt(| residuals |) against fitted values,\n * a Normal Q-Q plot,\n * a plot of residuals against leverages.\n\n\nBelow, we plot each of these 4 plots. We used the code published by Emre Can [5] with a few adaptations.\n\n\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rc('font', size=14)\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\nmodel_f = 'mpg ~ horsepower'\n\ndf.reset_index(drop=True, inplace=True)\n\nmodel = smf.ols(formula=model_f, data=df)\n\nmodel_fit = model.fit()\n\n# fitted values (need a constant term for intercept)\nmodel_fitted_y = model_fit.fittedvalues\n\n# model residuals\nmodel_residuals = model_fit.resid\n\n# normalized residuals\nmodel_norm_residuals = model_fit.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n\n# leverage, from statsmodels internals\nmodel_leverage = model_fit.get_influence().hat_matrix_diag\n\n# cook's distance, from statsmodels internals\nmodel_cooks = model_fit.get_influence().cooks_distance[0]\n\n\n\n\nResiduals against fitted values\n\n\nplot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = sns.residplot(model_fitted_y, 'mpg', data=df,\n                                  lowess=True,\n                                  scatter_kws={'alpha': 0.5},\n                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\n\n\n# annotations\nabs_resid = model_abs_resid.sort_values(ascending=False)\nabs_resid_top_3 = abs_resid[:3]\n\nfor i in abs_resid_top_3.index:\n    plot_lm_1.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_residuals[i]));\n\n\n\n\n\n\nThe red line is a smooth fit to the residuals which helps to spot any trends. \nWe can clearly see a strong non-linearity indicated by the curve of the red line.\nFigure 3.9 of the text also comments on this and shows the same plot for a quadratical fit, where the this trend is now almost non-existant.\nThe funnel shape of the plotted residuals indicates that we are in the presence of heteroscedasticity. \n\n\nNormal Q-Q plot\n\n\nQQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));\n\n\n\n\n\n\nThe qq-plot indicates that the assumption of normality does hold, since we can fit a straight line quite well, although there seems to be a slight left skew.\n\n\nThe links below provide nice explanations and intuitions of the concept of qq-plots and pp-plots and how to interpret them.\n\n\n\n\nhttps://stats.stackexchange.com/questions/52212/qq-plot-does-not-match-histogram/52221#52221\n\n\nhttps://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot\n\n\nhttps://xiongge.shinyapps.io/QQplots/\n\n\nhttps://stats.stackexchange.com/questions/92141/pp-plots-vs-qq-plots/100383#100383\n\n\n\n\nScale-Location plot of sqrt(|residuals|) against fitted values\n\n\nplot_lm_3 = plt.figure(3)\nplot_lm_3.set_figheight(8)\nplot_lm_3.set_figwidth(12)\n\nplt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\nsns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_3.axes[0].set_title('Scale-Location')\nplot_lm_3.axes[0].set_xlabel('Fitted values')\nplot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n\n\nfor i in abs_norm_resid_top_3:\n    plot_lm_3.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_norm_residuals_abs_sqrt[i]));\n\n\n\n\n\n\nThis plot is similar to the first one - it plots the residuals against the fitted values, although here they are all made positive and normalized.\nIt is clear from the plot that the assumption of homoscedasticity is not held.\n\n\n\"The third plot is a scale-location plot (square rooted standardized residual vs. predicted value). This is useful for checking the assumption of homoscedasticity. In this particular plot we are checking to see if there is a pattern in the residuals.\"\n\n\n(source: http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression7.html)\n\n\nResiduals against leverages\n\n\nplot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(model_leverage, model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0, 0.20)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n\n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None, ls='-'):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls=ls, color='red')\n\np = len(model_fit.params) # number of model parameters\n\ngraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance = .5', ls='--') # 0.5 line\n\ngraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), 'Cook\\'s distance = 1', ls=':') # 1 line\n\nplt.legend(loc='upper right');\n\n\n\n\n\n\nThe higher leverage points concentrate on the upper half of this plot, which indicates a deviation from normality.\nOn the other hand, from the plot we can see that every point has a Cook's distance well below 1, which indicates that no point has much individual influence on the fit.\n\n\n\"The fourth plot is of \"Cook's distance\", which is a measure of the influence of each observation on the regression coefficients. The Cook's distance statistic is a measure, for each observation in turn, of the extent of change in model estimates when that particular observation is omitted. Any observation for which the Cook's distance is close to 1 or more, or that is substantially larger than other Cook's distances (highly influential data points), requires investigation.\"\n\n\n(source: http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression7.html)\n\n\nReferences\n\n\n[4] https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.lm.html\n\n\n[5] https://emredjan.github.io/blog/2017/07/11/emulating-r-plots-in-python/",
            "title": "3.8"
        },
        {
            "location": "/sols/chapter3/exercise8/#exercise-38",
            "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf  # Statsmodels is a Python module for statistics\nimport statsmodels.api as sm\n\n%matplotlib inline  df = pd.read_csv('../data/auto.csv') # import dataset  df.head() # just to have a look   \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n     \n   \n   \n     \n       0 \n       18.0 \n       8 \n       307.0 \n       130 \n       3504 \n       12.0 \n       70 \n       1 \n       chevrolet chevelle malibu \n     \n     \n       1 \n       15.0 \n       8 \n       350.0 \n       165 \n       3693 \n       11.5 \n       70 \n       1 \n       buick skylark 320 \n     \n     \n       2 \n       18.0 \n       8 \n       318.0 \n       150 \n       3436 \n       11.0 \n       70 \n       1 \n       plymouth satellite \n     \n     \n       3 \n       16.0 \n       8 \n       304.0 \n       150 \n       3433 \n       12.0 \n       70 \n       1 \n       amc rebel sst \n     \n     \n       4 \n       17.0 \n       8 \n       302.0 \n       140 \n       3449 \n       10.5 \n       70 \n       1 \n       ford torino",
            "title": "Exercise 3.8"
        },
        {
            "location": "/sols/chapter3/exercise8/#a",
            "text": "# prepare data for modelling (training set)\nX_train = df['horsepower'] # horsepower as predictor\ny_train = df['mpg']        # mpg as response  X_train.head() # just to check  0    130\n1    165\n2    150\n3    150\n4    140\nName: horsepower, dtype: object  y_train.head() # just to check  0    18.0\n1    15.0\n2    18.0\n3    16.0\n4    17.0\nName: mpg, dtype: float64  X_train.unique() # check for data quality (missing data, errors, etc.)  array(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)   There is a strange value:  '?'   # handling with '?' observations (delete them)\ndroplist = X_train[X_train == '?'].index # get index of observations with '?' value\nX_train = X_train.drop(droplist)         # drop those observations\ny_train = y_train.drop(droplist)         # ensure dimensional compatibility between variables\nX_train.unique()                         # test  array(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '100', '105', '175', '153', '180', '110', '72',\n       '86', '70', '76', '65', '69', '60', '80', '54', '208', '155', '112',\n       '92', '145', '137', '158', '167', '94', '107', '230', '49', '75',\n       '91', '122', '67', '83', '78', '52', '61', '93', '148', '129', '96',\n       '71', '98', '115', '53', '81', '79', '120', '152', '102', '108',\n       '68', '58', '149', '89', '63', '48', '66', '139', '103', '125',\n       '133', '138', '135', '142', '77', '62', '132', '84', '64', '74',\n       '116', '82'], dtype=object)  y_train.unique() # check for data quality (missing data, errors, etc.)  array([ 18. ,  15. ,  16. ,  17. ,  14. ,  24. ,  22. ,  21. ,  27. ,\n        26. ,  25. ,  10. ,  11. ,   9. ,  28. ,  19. ,  12. ,  13. ,\n        23. ,  30. ,  31. ,  35. ,  20. ,  29. ,  32. ,  33. ,  17.5,\n        15.5,  14.5,  22.5,  24.5,  18.5,  29.5,  26.5,  16.5,  31.5,\n        36. ,  25.5,  33.5,  20.5,  30.5,  21.5,  43.1,  36.1,  32.8,\n        39.4,  19.9,  19.4,  20.2,  19.2,  25.1,  20.6,  20.8,  18.6,\n        18.1,  17.7,  27.5,  27.2,  30.9,  21.1,  23.2,  23.8,  23.9,\n        20.3,  21.6,  16.2,  19.8,  22.3,  17.6,  18.2,  16.9,  31.9,\n        34.1,  35.7,  27.4,  25.4,  34.2,  34.5,  31.8,  37.3,  28.4,\n        28.8,  26.8,  41.5,  38.1,  32.1,  37.2,  26.4,  24.3,  19.1,\n        34.3,  29.8,  31.3,  37. ,  32.2,  46.6,  27.9,  40.8,  44.3,\n        43.4,  36.4,  44.6,  33.8,  32.7,  23.7,  32.4,  26.6,  25.8,\n        23.5,  39.1,  39. ,  35.1,  32.3,  37.7,  34.7,  34.4,  29.9,\n        33.7,  32.9,  31.6,  28.1,  30.7,  24.2,  22.4,  34. ,  38. ,  44. ])  # create dataframe to use statsmodel\nd = {'horsepower':X_train.astype('float'), 'mpg':y_train}\ndf = pd.DataFrame(data=d)\ndf.head()   \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       horsepower \n       mpg \n     \n   \n   \n     \n       0 \n       130.0 \n       18.0 \n     \n     \n       1 \n       165.0 \n       15.0 \n     \n     \n       2 \n       150.0 \n       18.0 \n     \n     \n       3 \n       150.0 \n       16.0 \n     \n     \n       4 \n       140.0 \n       17.0 \n     \n      # plot\nplt.scatter(X_train,y_train);   # using statsmodel for linear regression (http://statsmodels.sourceforge.net/)\nmod = smf.ols(formula='mpg ~ horsepower', data = df)\nres = mod.fit()\nprint(res.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.606\nModel:                            OLS   Adj. R-squared:                  0.605\nMethod:                 Least Squares   F-statistic:                     599.7\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           7.03e-81\nTime:                        09:48:30   Log-Likelihood:                -1178.7\nNo. Observations:                 392   AIC:                             2361.\nDf Residuals:                     390   BIC:                             2369.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     39.9359      0.717     55.660      0.000      38.525      41.347\nhorsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n==============================================================================\nOmnibus:                       16.432   Durbin-Watson:                   0.920\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\nSkew:                           0.492   Prob(JB):                     0.000175\nKurtosis:                       3.299   Cond. No.                         322.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise8/#i",
            "text": "Yes, there is a relationship between the predictor and the response. We can reject the null hypothesis that the regression coefficients are zero, since the F-statistic is much larger than 1 and the p-value is close to zero.",
            "title": "i."
        },
        {
            "location": "/sols/chapter3/exercise8/#ii",
            "text": "Since the R-squared value is just about 0.606, we can say that approximately 60% of the variance in mpg is explained by horsepower.",
            "title": "ii"
        },
        {
            "location": "/sols/chapter3/exercise8/#iii",
            "text": "The relationship is negative because the coefficient corresponding to horsepower is equal to -0.1578.",
            "title": "iii"
        },
        {
            "location": "/sols/chapter3/exercise8/#iv",
            "text": "This exercise is a bit trickier to do in Python, since there's no standard out-of-the-box function for it (please let us know if you could find one). If we were solving this exercise in R, we would just need to use the functions confint() and predict() (see bottom of page 111 of ISLR).   On page 82 of ISLR, they describe the qualitative difference between a confidence and a prediction interval. However, the text does not explain how to calculate the intervals.\nFor a good description and derivation on how to calculate these intervals see Casella and Berger [1], section 11.3.5 (\"Estimation and Prediction at a Specified x=x0\"). \nThe distinction between the two types of interval is also well laid out in the answer [2], adapted from section 4.1 of Faraway [3].  The answer is calculated below. \nThe predicted 'mpg' is equal to 24.46708, with a 95% confidence interval of (23.97308, 24.96108) and a 95% prediction interval of (14.80940, 34.12476).\nThese are the same values as calculated with the R functions confint() and predict().  # this code follows Chapter 11 of Casella and Berger [1]\n# below we comment the lines with the page numbers of the text where the corresponding formulas appear\n\nfrom scipy.stats import t\nfrom math import sqrt\n\ndef interval(x, y, x0,alpha = .05):\n    n = np.size(x)\n    x_bar = np.mean(x)\n    y_bar = np.mean(y)\n    S_xx = np.sum((x-x_bar)**2)         # page 541\n    S_xy = np.sum((x-x_bar)*(y-y_bar))  # page 541\n    b = S_xy/S_xx                       # page 542\n    a = y_bar - b*x_bar                 # page 542\n    S2 = np.sum((y-a-b*x)**2)/(n-2)     # page 552\n    S = sqrt(S2)\n    ts = t.ppf(1-alpha/2, n-2)\n    w_conf = ts*S*sqrt(1/n + (x0-x_bar)**2/S_xx)      # page 558\n    w_pred = ts*S*sqrt(1 + 1/n + (x0-x_bar)**2/S_xx)  # page 559\n    print(\"                fit \\t lwr \\t  upr\")\n    print(\"confidence %3.5f %3.5f %3.5f\" % (a+b*x0, a+b*x0 - w_conf, a+b*x0 + w_conf))\n    print(\"prediction %3.5f %3.5f %3.5f\" % (a+b*x0, a+b*x0 - w_pred, a+b*x0 + w_pred))\n\nx = df['horsepower']\ny = df['mpg']\nx0 = 98    \n\ninterval(x, y, x0)                  fit      lwr      upr\nconfidence 24.46708 23.97308 24.96108\nprediction 24.46708 14.80940 34.12476",
            "title": "iv"
        },
        {
            "location": "/sols/chapter3/exercise8/#references",
            "text": "[1] Casella, George, and Roger L. Berger. Statistical inference. Vol. 2. Pacific Grove, CA: Duxbury, 2002.  amazon  [2]  https://stats.stackexchange.com/a/271232  [3] Faraway, Julian J. Linear models with R. CRC press, 2014.   amazon",
            "title": "References"
        },
        {
            "location": "/sols/chapter3/exercise8/#b",
            "text": "plt.subplots(1,1) #to get both plots in the same figure \nplt.scatter(X_train.astype('float'), y_train);\nplt.plot(X_train.astype('float'), res.fittedvalues, color='red');",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise8/#c",
            "text": "In R [4], by default, plot() on a fit produces 4 plots: \n * a plot of residuals against fitted values,\n * a Scale-Location plot of sqrt(| residuals |) against fitted values,\n * a Normal Q-Q plot,\n * a plot of residuals against leverages.  Below, we plot each of these 4 plots. We used the code published by Emre Can [5] with a few adaptations.  import statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rc('font', size=14)\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\nmodel_f = 'mpg ~ horsepower'\n\ndf.reset_index(drop=True, inplace=True)\n\nmodel = smf.ols(formula=model_f, data=df)\n\nmodel_fit = model.fit()\n\n# fitted values (need a constant term for intercept)\nmodel_fitted_y = model_fit.fittedvalues\n\n# model residuals\nmodel_residuals = model_fit.resid\n\n# normalized residuals\nmodel_norm_residuals = model_fit.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n\n# leverage, from statsmodels internals\nmodel_leverage = model_fit.get_influence().hat_matrix_diag\n\n# cook's distance, from statsmodels internals\nmodel_cooks = model_fit.get_influence().cooks_distance[0]",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise8/#residuals-against-fitted-values",
            "text": "plot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = sns.residplot(model_fitted_y, 'mpg', data=df,\n                                  lowess=True,\n                                  scatter_kws={'alpha': 0.5},\n                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\n\n\n# annotations\nabs_resid = model_abs_resid.sort_values(ascending=False)\nabs_resid_top_3 = abs_resid[:3]\n\nfor i in abs_resid_top_3.index:\n    plot_lm_1.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_residuals[i]));   The red line is a smooth fit to the residuals which helps to spot any trends. \nWe can clearly see a strong non-linearity indicated by the curve of the red line.\nFigure 3.9 of the text also comments on this and shows the same plot for a quadratical fit, where the this trend is now almost non-existant.\nThe funnel shape of the plotted residuals indicates that we are in the presence of heteroscedasticity.",
            "title": "Residuals against fitted values"
        },
        {
            "location": "/sols/chapter3/exercise8/#normal-q-q-plot",
            "text": "QQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));   The qq-plot indicates that the assumption of normality does hold, since we can fit a straight line quite well, although there seems to be a slight left skew.  The links below provide nice explanations and intuitions of the concept of qq-plots and pp-plots and how to interpret them.   https://stats.stackexchange.com/questions/52212/qq-plot-does-not-match-histogram/52221#52221  https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot  https://xiongge.shinyapps.io/QQplots/  https://stats.stackexchange.com/questions/92141/pp-plots-vs-qq-plots/100383#100383",
            "title": "Normal Q-Q plot"
        },
        {
            "location": "/sols/chapter3/exercise8/#scale-location-plot-of-sqrtresiduals-against-fitted-values",
            "text": "plot_lm_3 = plt.figure(3)\nplot_lm_3.set_figheight(8)\nplot_lm_3.set_figwidth(12)\n\nplt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\nsns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_3.axes[0].set_title('Scale-Location')\nplot_lm_3.axes[0].set_xlabel('Fitted values')\nplot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n\n\nfor i in abs_norm_resid_top_3:\n    plot_lm_3.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_norm_residuals_abs_sqrt[i]));   This plot is similar to the first one - it plots the residuals against the fitted values, although here they are all made positive and normalized.\nIt is clear from the plot that the assumption of homoscedasticity is not held.  \"The third plot is a scale-location plot (square rooted standardized residual vs. predicted value). This is useful for checking the assumption of homoscedasticity. In this particular plot we are checking to see if there is a pattern in the residuals.\"  (source: http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression7.html)",
            "title": "Scale-Location plot of sqrt(|residuals|) against fitted values"
        },
        {
            "location": "/sols/chapter3/exercise8/#residuals-against-leverages",
            "text": "plot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(model_leverage, model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0, 0.20)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n\n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None, ls='-'):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls=ls, color='red')\n\np = len(model_fit.params) # number of model parameters\n\ngraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance = .5', ls='--') # 0.5 line\n\ngraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), 'Cook\\'s distance = 1', ls=':') # 1 line\n\nplt.legend(loc='upper right');   The higher leverage points concentrate on the upper half of this plot, which indicates a deviation from normality.\nOn the other hand, from the plot we can see that every point has a Cook's distance well below 1, which indicates that no point has much individual influence on the fit.  \"The fourth plot is of \"Cook's distance\", which is a measure of the influence of each observation on the regression coefficients. The Cook's distance statistic is a measure, for each observation in turn, of the extent of change in model estimates when that particular observation is omitted. Any observation for which the Cook's distance is close to 1 or more, or that is substantially larger than other Cook's distances (highly influential data points), requires investigation.\"  (source: http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression7.html)",
            "title": "Residuals against leverages"
        },
        {
            "location": "/sols/chapter3/exercise8/#references_1",
            "text": "[4] https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.lm.html  [5] https://emredjan.github.io/blog/2017/07/11/emulating-r-plots-in-python/",
            "title": "References"
        },
        {
            "location": "/sols/chapter3/exercise9/",
            "text": "Exercise 3.9\n\n\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nsns.set(style=\"white\")\nplt.style.use('seaborn-white')\n\n\n\n\ndf = pd.read_csv('../data/Auto.csv')\ndf = pd.read_csv('../data/Auto.csv', na_values='?').dropna()\ndf.head()\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.0\n\n      \n8\n\n      \n307.0\n\n      \n130.0\n\n      \n3504\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.0\n\n      \n8\n\n      \n350.0\n\n      \n165.0\n\n      \n3693\n\n      \n11.5\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.0\n\n      \n8\n\n      \n318.0\n\n      \n150.0\n\n      \n3436\n\n      \n11.0\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.0\n\n      \n8\n\n      \n304.0\n\n      \n150.0\n\n      \n3433\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.0\n\n      \n8\n\n      \n302.0\n\n      \n140.0\n\n      \n3449\n\n      \n10.5\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\n# http://seaborn.pydata.org/generated/seaborn.PairGrid.html\n\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3, legend=False);\n\n\n\n\n\n\n(b)\n\n\n# pandas' corr() function takes care of excluding non numeric data: \n# https://github.com/pandas-dev/pandas/blob/v0.19.2/pandas/core/frame.py#L4721\n\ndf.corr()\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n    \n\n  \n\n  \n\n    \n\n      \nmpg\n\n      \n1.000000\n\n      \n-0.777618\n\n      \n-0.805127\n\n      \n-0.778427\n\n      \n-0.832244\n\n      \n0.423329\n\n      \n0.580541\n\n      \n0.565209\n\n    \n\n    \n\n      \ncylinders\n\n      \n-0.777618\n\n      \n1.000000\n\n      \n0.950823\n\n      \n0.842983\n\n      \n0.897527\n\n      \n-0.504683\n\n      \n-0.345647\n\n      \n-0.568932\n\n    \n\n    \n\n      \ndisplacement\n\n      \n-0.805127\n\n      \n0.950823\n\n      \n1.000000\n\n      \n0.897257\n\n      \n0.932994\n\n      \n-0.543800\n\n      \n-0.369855\n\n      \n-0.614535\n\n    \n\n    \n\n      \nhorsepower\n\n      \n-0.778427\n\n      \n0.842983\n\n      \n0.897257\n\n      \n1.000000\n\n      \n0.864538\n\n      \n-0.689196\n\n      \n-0.416361\n\n      \n-0.455171\n\n    \n\n    \n\n      \nweight\n\n      \n-0.832244\n\n      \n0.897527\n\n      \n0.932994\n\n      \n0.864538\n\n      \n1.000000\n\n      \n-0.416839\n\n      \n-0.309120\n\n      \n-0.585005\n\n    \n\n    \n\n      \nacceleration\n\n      \n0.423329\n\n      \n-0.504683\n\n      \n-0.543800\n\n      \n-0.689196\n\n      \n-0.416839\n\n      \n1.000000\n\n      \n0.290316\n\n      \n0.212746\n\n    \n\n    \n\n      \nyear\n\n      \n0.580541\n\n      \n-0.345647\n\n      \n-0.369855\n\n      \n-0.416361\n\n      \n-0.309120\n\n      \n0.290316\n\n      \n1.000000\n\n      \n0.181528\n\n    \n\n    \n\n      \norigin\n\n      \n0.565209\n\n      \n-0.568932\n\n      \n-0.614535\n\n      \n-0.455171\n\n      \n-0.585005\n\n      \n0.212746\n\n      \n0.181528\n\n      \n1.000000\n\n    \n\n  \n\n\n\n\n\n\n\nExtra\n\n\nWhy not a correlation heatmap as well?\n\n\nhttp://seaborn.pydata.org/examples/network_correlations.html\n\n\ncorrmat = df.corr()\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 9))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=.8, square=True)\n\nf.tight_layout()\n\n\n\n\n\n\n(c)\n\n\nreg = smf.ols('mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n           \nmpg\n       \n  R-squared:         \n \n   0.821\n \n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.818\n \n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   252.4\n \n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n2.04e-139\n\n\n\n\n\n  \nTime:\n                 \n09:49:08\n     \n  Log-Likelihood:    \n \n -1023.5\n \n\n\n\n\n  \nNo. Observations:\n      \n   392\n      \n  AIC:               \n \n   2063.\n \n\n\n\n\n  \nDf Residuals:\n          \n   384\n      \n  BIC:               \n \n   2095.\n \n\n\n\n\n  \nDf Model:\n              \n     7\n      \n                     \n     \n \n    \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n    \n\n\n\n\n\n\n\n\n\n        \n          \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n    \n  -17.2184\n \n    4.644\n \n   -3.707\n \n 0.000\n \n  -26.350\n \n   -8.087\n\n\n\n\n\n  \ncylinders\n    \n   -0.4934\n \n    0.323\n \n   -1.526\n \n 0.128\n \n   -1.129\n \n    0.142\n\n\n\n\n\n  \ndisplacement\n \n    0.0199\n \n    0.008\n \n    2.647\n \n 0.008\n \n    0.005\n \n    0.035\n\n\n\n\n\n  \nhorsepower\n   \n   -0.0170\n \n    0.014\n \n   -1.230\n \n 0.220\n \n   -0.044\n \n    0.010\n\n\n\n\n\n  \nweight\n       \n   -0.0065\n \n    0.001\n \n   -9.929\n \n 0.000\n \n   -0.008\n \n   -0.005\n\n\n\n\n\n  \nacceleration\n \n    0.0806\n \n    0.099\n \n    0.815\n \n 0.415\n \n   -0.114\n \n    0.275\n\n\n\n\n\n  \nyear\n         \n    0.7508\n \n    0.051\n \n   14.729\n \n 0.000\n \n    0.651\n \n    0.851\n\n\n\n\n\n  \norigin\n       \n    1.4261\n \n    0.278\n \n    5.127\n \n 0.000\n \n    0.879\n \n    1.973\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n31.906\n \n  Durbin-Watson:     \n \n   1.309\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.000\n \n  Jarque-Bera (JB):  \n \n  53.100\n\n\n\n\n\n  \nSkew:\n          \n 0.529\n \n  Prob(JB):          \n \n2.95e-12\n\n\n\n\n\n  \nKurtosis:\n      \n 4.460\n \n  Cond. No.          \n \n8.59e+04\n\n\n\n\n\n\n\ni\n\n\nYes, there is a relationship between the predictors and the response. \nIn the table above we can see that the value of the F-statistic is 252 which is much larger than 1, so we can reject the corresponding null hypothesis:\n\n\n\n\n\nH_0 :\\beta_{cylinders} = \\beta_{displacement} = \\beta_{weight} = \\beta_{acceleration} = \\beta_{year} = \\beta_{origin}  =0.  \n\n\n\n\nH_0 :\\beta_{cylinders} = \\beta_{displacement} = \\beta_{weight} = \\beta_{acceleration} = \\beta_{year} = \\beta_{origin}  =0.  \n\n\n\n\n\nIn fact, the probability this data would be generated if \nH_0\nH_0\n was true is \nProb(F-Statistic) = 2 \\times 10^{-139}\nProb(F-Statistic) = 2 \\times 10^{-139}\n, a ridiculously low value.\n\n\nii\n\n\nWe can see which predictors have a statistically significant relationship with the response by looking at the p-values in the table above.\nThe predictors that have a statistically significant relationship to the response are definitely weight, year and origin, and we could say displacement as well; while cylinders, horsepower, and acceleration do not. \n\n\niii\n\n\nThe coefficient suggests that, on average, when the other variables are held constant, an increase of one year (of production) corresponds to an increase of 0.75 of mpg (so, the more recent the more efficient).\n\n\n(d)\n\n\nIn R [4], by default, plot() on a fit produces 4 plots: \n * a plot of residuals against fitted values,\n * a Scale-Location plot of sqrt(| residuals |) against fitted values,\n * a Normal Q-Q plot,\n * a plot of residuals against leverages.\n\n\nBelow, we plot each of these 4 plots. We use the code published by Emre Can [5] with a few adaptations.\n\n\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rc('font', size=14)\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\nmodel_f = 'mpg ~ cylinders + \\\n                 displacement + \\\n                 horsepower + \\\n                 weight + \\\n                 acceleration + \\\n                 year + \\\n                 origin'\n\ndf.reset_index(drop=True, inplace=True)\n\nmodel = smf.ols(formula=model_f, data=df)\n\nmodel_fit = model.fit()\n\n# fitted values (need a constant term for intercept)\nmodel_fitted_y = model_fit.fittedvalues\n\n# model residuals\nmodel_residuals = model_fit.resid\n\n# normalized residuals\nmodel_norm_residuals = model_fit.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n\n# leverage, from statsmodels internals\nmodel_leverage = model_fit.get_influence().hat_matrix_diag\n\n# cook's distance, from statsmodels internals\nmodel_cooks = model_fit.get_influence().cooks_distance[0]\n\n\n\n\nResiduals against fitted values\n\n\nplot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = sns.residplot(model_fitted_y, 'mpg', data=df,\n                                  lowess=True,\n                                  scatter_kws={'alpha': 0.5},\n                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\n\n\n# annotations\nabs_resid = model_abs_resid.sort_values(ascending=False)\nabs_resid_top_3 = abs_resid[:3]\n\nfor i in abs_resid_top_3.index:\n    plot_lm_1.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_residuals[i]));\n\n\n\n\n\n\nNormal Q-Q plot\n\n\nQQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));\n\n\n\n\n\n\nScale-Location plot of sqrt(|residuals|) against fitted values\n\n\nplot_lm_3 = plt.figure(3)\nplot_lm_3.set_figheight(8)\nplot_lm_3.set_figwidth(12)\n\nplt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\nsns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_3.axes[0].set_title('Scale-Location')\nplot_lm_3.axes[0].set_xlabel('Fitted values')\nplot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n\n\nfor i in abs_norm_resid_top_3:\n    plot_lm_3.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_norm_residuals_abs_sqrt[i]));\n\n\n\n\n\n\nResiduals against leverages\n\n\nplot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(model_leverage, model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0, 0.20)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n\n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None, ls='-'):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls=ls, color='red')\n\np = len(model_fit.params) # number of model parameters\n\ngraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance = .5', ls='--') # 0.5 line\n\ngraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), 'Cook\\'s distance = 1', ls=':') # 1 line\n\nplt.legend(loc='upper right');\n\n\n\n\n\n\nComments\n\n\nNo, there are no unusually large outliers, as per the the scale log location.\nThey are however skewedly distributed. The larger the fitted value, the larger the variance, since the spread of the residuals increases.\n\n\nNo, even though there is an observation (number 13) with higher leverage, it is still well within Cook's 0.5 distance.\n\n\nThe normal qq-plot deviates at one extreme, which could indicate that there are other explanatory predictors that we are not considering (quadratic terms, for example).\nAdditionally, the funnel shape of the residuals plot indicates heteroskedacity.\n\n\nReferences\n\n\n[1] https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.lm.html\n\n\n[2] https://emredjan.github.io/blog/2017/07/11/emulating-r-plots-in-python/\n\n\n(e)\n\n\nStatsmodels uses patsy which is a \"mini-language\" inspired by R and S to describe statistical models. The symbols ':' and '*' have the same meaning as in R. Namely, a:b includes an interaction term between a and b, while a*b is shorthand for a + b + a:b, that is, it includes a and b as well.\n\n\nReferences:\n\n\n\n\nhttp://patsy.readthedocs.io/en/latest/formulas.html\n\n\nhttp://stackoverflow.com/questions/33050104/difference-between-the-interaction-and-term-for-formulas-in-statsmodels-ols\n\n\nhttp://stackoverflow.com/questions/23672466/interaction-effects-in-patsy-with-patsy-dmatrices-giving-duplicate-columns-for\n\n\n\n\nSo, which pairs of variables would we expect to interact, both a priori (from our interpretation of the meaning of these variables) and from the pairs plot?\n\n\nPerhaps horsepower and year? What would this mean? It would mean that, for different years, varying horsepower has different effect on mpg. It seems plausible. We could also interpret it in the reverse order: for different values of horsepower, does varying year have a different effect on mpg? For example, does the change in mpg when varying year (i.e., the derivative dmpg/dyear), differ when holding horsepower at either 130 or 160? \n\n\nLet's find out.\n\n\nreg = smf.ols('mpg ~ horsepower*year + displacement + weight + origin', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n           \nmpg\n       \n  R-squared:         \n \n   0.851\n \n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.849\n \n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   367.0\n \n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n7.51e-156\n\n\n\n\n\n  \nTime:\n                 \n09:49:11\n     \n  Log-Likelihood:    \n \n -987.81\n \n\n\n\n\n  \nNo. Observations:\n      \n   392\n      \n  AIC:               \n \n   1990.\n \n\n\n\n\n  \nDf Residuals:\n          \n   385\n      \n  BIC:               \n \n   2017.\n \n\n\n\n\n  \nDf Model:\n              \n     6\n      \n                     \n     \n \n    \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n    \n\n\n\n\n\n\n\n\n\n         \n            \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n       \n  -96.6688\n \n    9.667\n \n  -10.000\n \n 0.000\n \n -115.675\n \n  -77.663\n\n\n\n\n\n  \nhorsepower\n      \n    0.7993\n \n    0.092\n \n    8.687\n \n 0.000\n \n    0.618\n \n    0.980\n\n\n\n\n\n  \nyear\n            \n    1.8179\n \n    0.128\n \n   14.221\n \n 0.000\n \n    1.567\n \n    2.069\n\n\n\n\n\n  \nhorsepower:year\n \n   -0.0113\n \n    0.001\n \n   -8.977\n \n 0.000\n \n   -0.014\n \n   -0.009\n\n\n\n\n\n  \ndisplacement\n    \n    0.0068\n \n    0.005\n \n    1.344\n \n 0.180\n \n   -0.003\n \n    0.017\n\n\n\n\n\n  \nweight\n          \n   -0.0054\n \n    0.001\n \n  -10.170\n \n 0.000\n \n   -0.006\n \n   -0.004\n\n\n\n\n\n  \norigin\n          \n    1.1866\n \n    0.253\n \n    4.684\n \n 0.000\n \n    0.688\n \n    1.685\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n21.932\n \n  Durbin-Watson:     \n \n   1.488\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.000\n \n  Jarque-Bera (JB):  \n \n  33.066\n\n\n\n\n\n  \nSkew:\n          \n 0.411\n \n  Prob(JB):          \n \n6.60e-08\n\n\n\n\n\n  \nKurtosis:\n      \n 4.161\n \n  Cond. No.          \n \n5.60e+05\n\n\n\n\n\n\n\nFrom the value of the p-value of the coefficient of the interaction term between horsepower and year, it does seem like there is a statistically significant relationship between the response and horsepower:year.\n\n\nWith 7 factors, there will be a total of 21 interaction terms.\nFor simplicity sake, we will exclude all the terms with cylinders and acceleration, leaving us with 10 interaction terms.\nLet's try to fit a model with these terms - a total of 15 terms.\n\n\nmodel = 'mpg ~ displacement + horsepower + origin + weight + year \\\n               + displacement:horsepower + displacement:origin + displacement:weight + displacement:year \\\n               + horsepower:origin + horsepower:weight + horsepower:year + origin:weight + origin:year + weight:year'\nreg = smf.ols(model, df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n           \nmpg\n       \n  R-squared:         \n \n   0.880\n \n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.875\n \n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   184.0\n \n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n1.09e-162\n\n\n\n\n\n  \nTime:\n                 \n09:49:11\n     \n  Log-Likelihood:    \n \n -945.49\n \n\n\n\n\n  \nNo. Observations:\n      \n   392\n      \n  AIC:               \n \n   1923.\n \n\n\n\n\n  \nDf Residuals:\n          \n   376\n      \n  BIC:               \n \n   1987.\n \n\n\n\n\n  \nDf Model:\n              \n    15\n      \n                     \n     \n \n    \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n    \n\n\n\n\n\n\n\n\n\n             \n                \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n               \n  -51.3746\n \n   26.175\n \n   -1.963\n \n 0.050\n \n -102.843\n \n    0.093\n\n\n\n\n\n  \ndisplacement\n            \n   -0.1818\n \n    0.120\n \n   -1.521\n \n 0.129\n \n   -0.417\n \n    0.053\n\n\n\n\n\n  \nhorsepower\n              \n    0.9485\n \n    0.232\n \n    4.089\n \n 0.000\n \n    0.492\n \n    1.405\n\n\n\n\n\n  \norigin\n                  \n   -3.0637\n \n    5.496\n \n   -0.557\n \n 0.578\n \n  -13.871\n \n    7.744\n\n\n\n\n\n  \nweight\n                  \n   -0.0174\n \n    0.016\n \n   -1.115\n \n 0.265\n \n   -0.048\n \n    0.013\n\n\n\n\n\n  \nyear\n                    \n    1.3975\n \n    0.328\n \n    4.267\n \n 0.000\n \n    0.754\n \n    2.042\n\n\n\n\n\n  \ndisplacement:horsepower\n \n   -0.0001\n \n    0.000\n \n   -0.815\n \n 0.416\n \n   -0.000\n \n    0.000\n\n\n\n\n\n  \ndisplacement:origin\n     \n    0.0282\n \n    0.013\n \n    2.172\n \n 0.030\n \n    0.003\n \n    0.054\n\n\n\n\n\n  \ndisplacement:weight\n     \n 2.792e-05\n \n 5.99e-06\n \n    4.663\n \n 0.000\n \n 1.61e-05\n \n 3.97e-05\n\n\n\n\n\n  \ndisplacement:year\n       \n    0.0010\n \n    0.001\n \n    0.710\n \n 0.478\n \n   -0.002\n \n    0.004\n\n\n\n\n\n  \nhorsepower:origin\n       \n   -0.0629\n \n    0.020\n \n   -3.104\n \n 0.002\n \n   -0.103\n \n   -0.023\n\n\n\n\n\n  \nhorsepower:weight\n       \n-1.175e-05\n \n 1.77e-05\n \n   -0.664\n \n 0.507\n \n-4.65e-05\n \n  2.3e-05\n\n\n\n\n\n  \nhorsepower:year\n         \n   -0.0114\n \n    0.003\n \n   -3.998\n \n 0.000\n \n   -0.017\n \n   -0.006\n\n\n\n\n\n  \norigin:weight\n           \n    0.0014\n \n    0.001\n \n    1.200\n \n 0.231\n \n   -0.001\n \n    0.004\n\n\n\n\n\n  \norigin:year\n             \n    0.0322\n \n    0.069\n \n    0.464\n \n 0.643\n \n   -0.104\n \n    0.169\n\n\n\n\n\n  \nweight:year\n             \n 7.438e-05\n \n    0.000\n \n    0.394\n \n 0.694\n \n   -0.000\n \n    0.000\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n52.658\n \n  Durbin-Watson:     \n \n   1.599\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.000\n \n  Jarque-Bera (JB):  \n \n 115.208\n\n\n\n\n\n  \nSkew:\n          \n 0.710\n \n  Prob(JB):          \n \n9.61e-26\n\n\n\n\n\n  \nKurtosis:\n      \n 5.244\n \n  Cond. No.          \n \n1.81e+08\n\n\n\n\n\n\n\nThese results indicate that the interactions that appear to be statistically significant are displacement:weight, horsepower:origin and horsepower:year.\nInterestingly, when these are considered the only first order terms that are statistically different are horsepower and year.\nBy the hierarchy principle (page 89), we should nonetheless include all of the main effects (for more on this, see \nthese\n answers).\n\n\nWe could also have a try at \ninteraction plots\n, which are not covered in the book, but we will leave it as a \nmention\n only.\n\n\n(f)\n\n\nAs an example, we fit the data with a model containing the transformations indicated for the variable horsepower, starting with \nX^2\nX^2\n. \n\n\nreg = smf.ols('mpg ~ horsepower + np.power(horsepower,2) + weight + year + origin', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n           \nmpg\n       \n  R-squared:         \n \n   0.851\n \n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.849\n \n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   439.5\n \n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n7.11e-157\n\n\n\n\n\n  \nTime:\n                 \n09:49:11\n     \n  Log-Likelihood:    \n \n -988.57\n \n\n\n\n\n  \nNo. Observations:\n      \n   392\n      \n  AIC:               \n \n   1989.\n \n\n\n\n\n  \nDf Residuals:\n          \n   386\n      \n  BIC:               \n \n   2013.\n \n\n\n\n\n  \nDf Model:\n              \n     5\n      \n                     \n     \n \n    \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n    \n\n\n\n\n\n\n\n\n\n             \n                \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n               \n   -6.6457\n \n    3.915\n \n   -1.698\n \n 0.090\n \n  -14.343\n \n    1.052\n\n\n\n\n\n  \nhorsepower\n              \n   -0.2441\n \n    0.027\n \n   -9.099\n \n 0.000\n \n   -0.297\n \n   -0.191\n\n\n\n\n\n  \nnp.power(horsepower, 2)\n \n    0.0008\n \n 9.13e-05\n \n    9.170\n \n 0.000\n \n    0.001\n \n    0.001\n\n\n\n\n\n  \nweight\n                  \n   -0.0044\n \n    0.000\n \n  -10.426\n \n 0.000\n \n   -0.005\n \n   -0.004\n\n\n\n\n\n  \nyear\n                    \n    0.7456\n \n    0.046\n \n   16.145\n \n 0.000\n \n    0.655\n \n    0.836\n\n\n\n\n\n  \norigin\n                  \n    1.0465\n \n    0.238\n \n    4.405\n \n 0.000\n \n    0.579\n \n    1.514\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n21.819\n \n  Durbin-Watson:     \n \n   1.500\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.000\n \n  Jarque-Bera (JB):  \n \n  32.447\n\n\n\n\n\n  \nSkew:\n          \n 0.414\n \n  Prob(JB):          \n \n9.00e-08\n\n\n\n\n\n  \nKurtosis:\n      \n 4.140\n \n  Cond. No.          \n \n4.10e+05\n\n\n\n\n\n\n\nfig = plt.figure()\nfitted = reg.fittedvalues\nsns.regplot(fitted, df.mpg - fitted,  lowess=True, line_kws={'color':'r', 'lw':1})\nax = fig.axes[0]\nax.axhline(color=\"grey\", ls=\"--\")\nax.set_title(\"Residuals vs Fitted Values\")\nax.set_xlabel(\"Fitted Values\")\nax.set_ylabel(\"Residuals\");\n\n\n\n\n\n\nIt is clear that this quadratic term is statistically significant. Let's try adding a logarithmic term as well.\n\n\nreg = smf.ols('mpg ~ horsepower + np.power(horsepower,2) + np.log(horsepower) + weight + year + origin', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n           \nmpg\n       \n  R-squared:         \n \n   0.855\n \n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.853\n \n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   378.6\n \n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n4.62e-158\n\n\n\n\n\n  \nTime:\n                 \n09:49:11\n     \n  Log-Likelihood:    \n \n -982.62\n \n\n\n\n\n  \nNo. Observations:\n      \n   392\n      \n  AIC:               \n \n   1979.\n \n\n\n\n\n  \nDf Residuals:\n          \n   385\n      \n  BIC:               \n \n   2007.\n \n\n\n\n\n  \nDf Model:\n              \n     6\n      \n                     \n     \n \n    \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n    \n\n\n\n\n\n\n\n\n\n             \n                \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n               \n   80.3681\n \n   25.568\n \n    3.143\n \n 0.002\n \n   30.098\n \n  130.638\n\n\n\n\n\n  \nhorsepower\n              \n    0.2570\n \n    0.148\n \n    1.737\n \n 0.083\n \n   -0.034\n \n    0.548\n\n\n\n\n\n  \nnp.power(horsepower, 2)\n \n   -0.0002\n \n    0.000\n \n   -0.571\n \n 0.568\n \n   -0.001\n \n    0.000\n\n\n\n\n\n  \nnp.log(horsepower)\n      \n  -27.5412\n \n    8.000\n \n   -3.443\n \n 0.001\n \n  -43.270\n \n  -11.812\n\n\n\n\n\n  \nweight\n                  \n   -0.0048\n \n    0.000\n \n  -11.098\n \n 0.000\n \n   -0.006\n \n   -0.004\n\n\n\n\n\n  \nyear\n                    \n    0.7561\n \n    0.046\n \n   16.565\n \n 0.000\n \n    0.666\n \n    0.846\n\n\n\n\n\n  \norigin\n                  \n    0.9480\n \n    0.236\n \n    4.016\n \n 0.000\n \n    0.484\n \n    1.412\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n17.905\n \n  Durbin-Watson:     \n \n   1.575\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.000\n \n  Jarque-Bera (JB):  \n \n  29.299\n\n\n\n\n\n  \nSkew:\n          \n 0.312\n \n  Prob(JB):          \n \n4.34e-07\n\n\n\n\n\n  \nKurtosis:\n      \n 4.185\n \n  Cond. No.          \n \n2.84e+06\n\n\n\n\n\n\n\nNow the p-value for the square term is very large.\nThis indicates that there is indeed a non-linearity but it seems to be better captured by the logarithm than the square. \nLet's try adding the square root term.\n\n\nreg = smf.ols('mpg ~ horsepower + np.power(horsepower,2) + np.log(horsepower) + np.sqrt(horsepower) + weight + year + origin', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n           \nmpg\n       \n  R-squared:         \n \n   0.859\n \n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.856\n \n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   332.9\n \n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n9.16e-159\n\n\n\n\n\n  \nTime:\n                 \n09:49:12\n     \n  Log-Likelihood:    \n \n -977.89\n \n\n\n\n\n  \nNo. Observations:\n      \n   392\n      \n  AIC:               \n \n   1972.\n \n\n\n\n\n  \nDf Residuals:\n          \n   384\n      \n  BIC:               \n \n   2004.\n \n\n\n\n\n  \nDf Model:\n              \n     7\n      \n                     \n     \n \n    \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n    \n\n\n\n\n\n\n\n\n\n             \n                \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n               \n -426.3991\n \n  167.288\n \n   -2.549\n \n 0.011\n \n -755.314\n \n  -97.485\n\n\n\n\n\n  \nhorsepower\n              \n    8.4452\n \n    2.676\n \n    3.156\n \n 0.002\n \n    3.184\n \n   13.706\n\n\n\n\n\n  \nnp.power(horsepower, 2)\n \n   -0.0060\n \n    0.002\n \n   -3.117\n \n 0.002\n \n   -0.010\n \n   -0.002\n\n\n\n\n\n  \nnp.log(horsepower)\n      \n  416.0064\n \n  144.951\n \n    2.870\n \n 0.004\n \n  131.009\n \n  701.004\n\n\n\n\n\n  \nnp.sqrt(horsepower)\n     \n -229.6161\n \n   74.927\n \n   -3.065\n \n 0.002\n \n -376.934\n \n  -82.298\n\n\n\n\n\n  \nweight\n                  \n   -0.0048\n \n    0.000\n \n  -11.229\n \n 0.000\n \n   -0.006\n \n   -0.004\n\n\n\n\n\n  \nyear\n                    \n    0.7475\n \n    0.045\n \n   16.522\n \n 0.000\n \n    0.659\n \n    0.836\n\n\n\n\n\n  \norigin\n                  \n    0.9088\n \n    0.234\n \n    3.886\n \n 0.000\n \n    0.449\n \n    1.369\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n23.395\n \n  Durbin-Watson:     \n \n   1.570\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.000\n \n  Jarque-Bera (JB):  \n \n  37.804\n\n\n\n\n\n  \nSkew:\n          \n 0.411\n \n  Prob(JB):          \n \n6.18e-09\n\n\n\n\n\n  \nKurtosis:\n      \n 4.281\n \n  Cond. No.          \n \n2.50e+07\n\n\n\n\n\n\n\nSo now the square term is back to a small p-value, indicating that it is statistically significant in the presence of the square root and the logarithm.",
            "title": "3.9"
        },
        {
            "location": "/sols/chapter3/exercise9/#exercise-39",
            "text": "%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nsns.set(style=\"white\")\nplt.style.use('seaborn-white')  df = pd.read_csv('../data/Auto.csv')\ndf = pd.read_csv('../data/Auto.csv', na_values='?').dropna()\ndf.head()   \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n     \n   \n   \n     \n       0 \n       18.0 \n       8 \n       307.0 \n       130.0 \n       3504 \n       12.0 \n       70 \n       1 \n       chevrolet chevelle malibu \n     \n     \n       1 \n       15.0 \n       8 \n       350.0 \n       165.0 \n       3693 \n       11.5 \n       70 \n       1 \n       buick skylark 320 \n     \n     \n       2 \n       18.0 \n       8 \n       318.0 \n       150.0 \n       3436 \n       11.0 \n       70 \n       1 \n       plymouth satellite \n     \n     \n       3 \n       16.0 \n       8 \n       304.0 \n       150.0 \n       3433 \n       12.0 \n       70 \n       1 \n       amc rebel sst \n     \n     \n       4 \n       17.0 \n       8 \n       302.0 \n       140.0 \n       3449 \n       10.5 \n       70 \n       1 \n       ford torino",
            "title": "Exercise 3.9"
        },
        {
            "location": "/sols/chapter3/exercise9/#a",
            "text": "# http://seaborn.pydata.org/generated/seaborn.PairGrid.html\n\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3, legend=False);",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise9/#b",
            "text": "# pandas' corr() function takes care of excluding non numeric data: \n# https://github.com/pandas-dev/pandas/blob/v0.19.2/pandas/core/frame.py#L4721\n\ndf.corr()   \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n     \n   \n   \n     \n       mpg \n       1.000000 \n       -0.777618 \n       -0.805127 \n       -0.778427 \n       -0.832244 \n       0.423329 \n       0.580541 \n       0.565209 \n     \n     \n       cylinders \n       -0.777618 \n       1.000000 \n       0.950823 \n       0.842983 \n       0.897527 \n       -0.504683 \n       -0.345647 \n       -0.568932 \n     \n     \n       displacement \n       -0.805127 \n       0.950823 \n       1.000000 \n       0.897257 \n       0.932994 \n       -0.543800 \n       -0.369855 \n       -0.614535 \n     \n     \n       horsepower \n       -0.778427 \n       0.842983 \n       0.897257 \n       1.000000 \n       0.864538 \n       -0.689196 \n       -0.416361 \n       -0.455171 \n     \n     \n       weight \n       -0.832244 \n       0.897527 \n       0.932994 \n       0.864538 \n       1.000000 \n       -0.416839 \n       -0.309120 \n       -0.585005 \n     \n     \n       acceleration \n       0.423329 \n       -0.504683 \n       -0.543800 \n       -0.689196 \n       -0.416839 \n       1.000000 \n       0.290316 \n       0.212746 \n     \n     \n       year \n       0.580541 \n       -0.345647 \n       -0.369855 \n       -0.416361 \n       -0.309120 \n       0.290316 \n       1.000000 \n       0.181528 \n     \n     \n       origin \n       0.565209 \n       -0.568932 \n       -0.614535 \n       -0.455171 \n       -0.585005 \n       0.212746 \n       0.181528 \n       1.000000 \n     \n      Extra  Why not a correlation heatmap as well?  http://seaborn.pydata.org/examples/network_correlations.html  corrmat = df.corr()\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 9))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=.8, square=True)\n\nf.tight_layout()",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise9/#c",
            "text": "reg = smf.ols('mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:             mpg           R-squared:               0.821    \n   Model:                     OLS           Adj. R-squared:          0.818    \n   Method:               Least Squares      F-statistic:             252.4    \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   2.04e-139   \n   Time:                   09:49:08         Log-Likelihood:        -1023.5    \n   No. Observations:           392          AIC:                     2063.    \n   Df Residuals:               384          BIC:                     2095.    \n   Df Model:                     7                                            \n   Covariance Type:        nonrobust                                            \n                    coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept        -17.2184       4.644      -3.707    0.000     -26.350      -8.087   \n   cylinders         -0.4934       0.323      -1.526    0.128      -1.129       0.142   \n   displacement       0.0199       0.008       2.647    0.008       0.005       0.035   \n   horsepower        -0.0170       0.014      -1.230    0.220      -0.044       0.010   \n   weight            -0.0065       0.001      -9.929    0.000      -0.008      -0.005   \n   acceleration       0.0806       0.099       0.815    0.415      -0.114       0.275   \n   year               0.7508       0.051      14.729    0.000       0.651       0.851   \n   origin             1.4261       0.278       5.127    0.000       0.879       1.973     \n   Omnibus:         31.906     Durbin-Watson:           1.309   \n   Prob(Omnibus):    0.000     Jarque-Bera (JB):       53.100   \n   Skew:             0.529     Prob(JB):             2.95e-12   \n   Kurtosis:         4.460     Cond. No.             8.59e+04",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise9/#i",
            "text": "Yes, there is a relationship between the predictors and the response. \nIn the table above we can see that the value of the F-statistic is 252 which is much larger than 1, so we can reject the corresponding null hypothesis:   \nH_0 :\\beta_{cylinders} = \\beta_{displacement} = \\beta_{weight} = \\beta_{acceleration} = \\beta_{year} = \\beta_{origin}  =0.    \nH_0 :\\beta_{cylinders} = \\beta_{displacement} = \\beta_{weight} = \\beta_{acceleration} = \\beta_{year} = \\beta_{origin}  =0.     In fact, the probability this data would be generated if  H_0 H_0  was true is  Prob(F-Statistic) = 2 \\times 10^{-139} Prob(F-Statistic) = 2 \\times 10^{-139} , a ridiculously low value.",
            "title": "i"
        },
        {
            "location": "/sols/chapter3/exercise9/#ii",
            "text": "We can see which predictors have a statistically significant relationship with the response by looking at the p-values in the table above.\nThe predictors that have a statistically significant relationship to the response are definitely weight, year and origin, and we could say displacement as well; while cylinders, horsepower, and acceleration do not.",
            "title": "ii"
        },
        {
            "location": "/sols/chapter3/exercise9/#iii",
            "text": "The coefficient suggests that, on average, when the other variables are held constant, an increase of one year (of production) corresponds to an increase of 0.75 of mpg (so, the more recent the more efficient).",
            "title": "iii"
        },
        {
            "location": "/sols/chapter3/exercise9/#d",
            "text": "In R [4], by default, plot() on a fit produces 4 plots: \n * a plot of residuals against fitted values,\n * a Scale-Location plot of sqrt(| residuals |) against fitted values,\n * a Normal Q-Q plot,\n * a plot of residuals against leverages.  Below, we plot each of these 4 plots. We use the code published by Emre Can [5] with a few adaptations.  import statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rc('font', size=14)\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\nmodel_f = 'mpg ~ cylinders + \\\n                 displacement + \\\n                 horsepower + \\\n                 weight + \\\n                 acceleration + \\\n                 year + \\\n                 origin'\n\ndf.reset_index(drop=True, inplace=True)\n\nmodel = smf.ols(formula=model_f, data=df)\n\nmodel_fit = model.fit()\n\n# fitted values (need a constant term for intercept)\nmodel_fitted_y = model_fit.fittedvalues\n\n# model residuals\nmodel_residuals = model_fit.resid\n\n# normalized residuals\nmodel_norm_residuals = model_fit.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n\n# leverage, from statsmodels internals\nmodel_leverage = model_fit.get_influence().hat_matrix_diag\n\n# cook's distance, from statsmodels internals\nmodel_cooks = model_fit.get_influence().cooks_distance[0]",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter3/exercise9/#residuals-against-fitted-values",
            "text": "plot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = sns.residplot(model_fitted_y, 'mpg', data=df,\n                                  lowess=True,\n                                  scatter_kws={'alpha': 0.5},\n                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\n\n\n# annotations\nabs_resid = model_abs_resid.sort_values(ascending=False)\nabs_resid_top_3 = abs_resid[:3]\n\nfor i in abs_resid_top_3.index:\n    plot_lm_1.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_residuals[i]));",
            "title": "Residuals against fitted values"
        },
        {
            "location": "/sols/chapter3/exercise9/#normal-q-q-plot",
            "text": "QQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));",
            "title": "Normal Q-Q plot"
        },
        {
            "location": "/sols/chapter3/exercise9/#scale-location-plot-of-sqrtresiduals-against-fitted-values",
            "text": "plot_lm_3 = plt.figure(3)\nplot_lm_3.set_figheight(8)\nplot_lm_3.set_figwidth(12)\n\nplt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\nsns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_3.axes[0].set_title('Scale-Location')\nplot_lm_3.axes[0].set_xlabel('Fitted values')\nplot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n\n\nfor i in abs_norm_resid_top_3:\n    plot_lm_3.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_norm_residuals_abs_sqrt[i]));",
            "title": "Scale-Location plot of sqrt(|residuals|) against fitted values"
        },
        {
            "location": "/sols/chapter3/exercise9/#residuals-against-leverages",
            "text": "plot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(model_leverage, model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0, 0.20)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n\n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None, ls='-'):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls=ls, color='red')\n\np = len(model_fit.params) # number of model parameters\n\ngraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance = .5', ls='--') # 0.5 line\n\ngraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), 'Cook\\'s distance = 1', ls=':') # 1 line\n\nplt.legend(loc='upper right');",
            "title": "Residuals against leverages"
        },
        {
            "location": "/sols/chapter3/exercise9/#comments",
            "text": "No, there are no unusually large outliers, as per the the scale log location.\nThey are however skewedly distributed. The larger the fitted value, the larger the variance, since the spread of the residuals increases.  No, even though there is an observation (number 13) with higher leverage, it is still well within Cook's 0.5 distance.  The normal qq-plot deviates at one extreme, which could indicate that there are other explanatory predictors that we are not considering (quadratic terms, for example).\nAdditionally, the funnel shape of the residuals plot indicates heteroskedacity.",
            "title": "Comments"
        },
        {
            "location": "/sols/chapter3/exercise9/#references",
            "text": "[1] https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.lm.html  [2] https://emredjan.github.io/blog/2017/07/11/emulating-r-plots-in-python/",
            "title": "References"
        },
        {
            "location": "/sols/chapter3/exercise9/#e",
            "text": "Statsmodels uses patsy which is a \"mini-language\" inspired by R and S to describe statistical models. The symbols ':' and '*' have the same meaning as in R. Namely, a:b includes an interaction term between a and b, while a*b is shorthand for a + b + a:b, that is, it includes a and b as well.  References:   http://patsy.readthedocs.io/en/latest/formulas.html  http://stackoverflow.com/questions/33050104/difference-between-the-interaction-and-term-for-formulas-in-statsmodels-ols  http://stackoverflow.com/questions/23672466/interaction-effects-in-patsy-with-patsy-dmatrices-giving-duplicate-columns-for   So, which pairs of variables would we expect to interact, both a priori (from our interpretation of the meaning of these variables) and from the pairs plot?  Perhaps horsepower and year? What would this mean? It would mean that, for different years, varying horsepower has different effect on mpg. It seems plausible. We could also interpret it in the reverse order: for different values of horsepower, does varying year have a different effect on mpg? For example, does the change in mpg when varying year (i.e., the derivative dmpg/dyear), differ when holding horsepower at either 130 or 160?   Let's find out.  reg = smf.ols('mpg ~ horsepower*year + displacement + weight + origin', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:             mpg           R-squared:               0.851    \n   Model:                     OLS           Adj. R-squared:          0.849    \n   Method:               Least Squares      F-statistic:             367.0    \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   7.51e-156   \n   Time:                   09:49:11         Log-Likelihood:        -987.81    \n   No. Observations:           392          AIC:                     1990.    \n   Df Residuals:               385          BIC:                     2017.    \n   Df Model:                     6                                            \n   Covariance Type:        nonrobust                                            \n                       coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept           -96.6688       9.667     -10.000    0.000    -115.675     -77.663   \n   horsepower            0.7993       0.092       8.687    0.000       0.618       0.980   \n   year                  1.8179       0.128      14.221    0.000       1.567       2.069   \n   horsepower:year      -0.0113       0.001      -8.977    0.000      -0.014      -0.009   \n   displacement          0.0068       0.005       1.344    0.180      -0.003       0.017   \n   weight               -0.0054       0.001     -10.170    0.000      -0.006      -0.004   \n   origin                1.1866       0.253       4.684    0.000       0.688       1.685     \n   Omnibus:         21.932     Durbin-Watson:           1.488   \n   Prob(Omnibus):    0.000     Jarque-Bera (JB):       33.066   \n   Skew:             0.411     Prob(JB):             6.60e-08   \n   Kurtosis:         4.161     Cond. No.             5.60e+05    From the value of the p-value of the coefficient of the interaction term between horsepower and year, it does seem like there is a statistically significant relationship between the response and horsepower:year.  With 7 factors, there will be a total of 21 interaction terms.\nFor simplicity sake, we will exclude all the terms with cylinders and acceleration, leaving us with 10 interaction terms.\nLet's try to fit a model with these terms - a total of 15 terms.  model = 'mpg ~ displacement + horsepower + origin + weight + year \\\n               + displacement:horsepower + displacement:origin + displacement:weight + displacement:year \\\n               + horsepower:origin + horsepower:weight + horsepower:year + origin:weight + origin:year + weight:year'\nreg = smf.ols(model, df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:             mpg           R-squared:               0.880    \n   Model:                     OLS           Adj. R-squared:          0.875    \n   Method:               Least Squares      F-statistic:             184.0    \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   1.09e-162   \n   Time:                   09:49:11         Log-Likelihood:        -945.49    \n   No. Observations:           392          AIC:                     1923.    \n   Df Residuals:               376          BIC:                     1987.    \n   Df Model:                    15                                            \n   Covariance Type:        nonrobust                                            \n                               coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept                   -51.3746      26.175      -1.963    0.050    -102.843       0.093   \n   displacement                 -0.1818       0.120      -1.521    0.129      -0.417       0.053   \n   horsepower                    0.9485       0.232       4.089    0.000       0.492       1.405   \n   origin                       -3.0637       5.496      -0.557    0.578     -13.871       7.744   \n   weight                       -0.0174       0.016      -1.115    0.265      -0.048       0.013   \n   year                          1.3975       0.328       4.267    0.000       0.754       2.042   \n   displacement:horsepower      -0.0001       0.000      -0.815    0.416      -0.000       0.000   \n   displacement:origin           0.0282       0.013       2.172    0.030       0.003       0.054   \n   displacement:weight        2.792e-05    5.99e-06       4.663    0.000    1.61e-05    3.97e-05   \n   displacement:year             0.0010       0.001       0.710    0.478      -0.002       0.004   \n   horsepower:origin            -0.0629       0.020      -3.104    0.002      -0.103      -0.023   \n   horsepower:weight         -1.175e-05    1.77e-05      -0.664    0.507   -4.65e-05     2.3e-05   \n   horsepower:year              -0.0114       0.003      -3.998    0.000      -0.017      -0.006   \n   origin:weight                 0.0014       0.001       1.200    0.231      -0.001       0.004   \n   origin:year                   0.0322       0.069       0.464    0.643      -0.104       0.169   \n   weight:year                7.438e-05       0.000       0.394    0.694      -0.000       0.000     \n   Omnibus:         52.658     Durbin-Watson:           1.599   \n   Prob(Omnibus):    0.000     Jarque-Bera (JB):      115.208   \n   Skew:             0.710     Prob(JB):             9.61e-26   \n   Kurtosis:         5.244     Cond. No.             1.81e+08    These results indicate that the interactions that appear to be statistically significant are displacement:weight, horsepower:origin and horsepower:year.\nInterestingly, when these are considered the only first order terms that are statistically different are horsepower and year.\nBy the hierarchy principle (page 89), we should nonetheless include all of the main effects (for more on this, see  these  answers).  We could also have a try at  interaction plots , which are not covered in the book, but we will leave it as a  mention  only.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter3/exercise9/#f",
            "text": "As an example, we fit the data with a model containing the transformations indicated for the variable horsepower, starting with  X^2 X^2 .   reg = smf.ols('mpg ~ horsepower + np.power(horsepower,2) + weight + year + origin', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:             mpg           R-squared:               0.851    \n   Model:                     OLS           Adj. R-squared:          0.849    \n   Method:               Least Squares      F-statistic:             439.5    \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   7.11e-157   \n   Time:                   09:49:11         Log-Likelihood:        -988.57    \n   No. Observations:           392          AIC:                     1989.    \n   Df Residuals:               386          BIC:                     2013.    \n   Df Model:                     5                                            \n   Covariance Type:        nonrobust                                            \n                               coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept                    -6.6457       3.915      -1.698    0.090     -14.343       1.052   \n   horsepower                   -0.2441       0.027      -9.099    0.000      -0.297      -0.191   \n   np.power(horsepower, 2)       0.0008    9.13e-05       9.170    0.000       0.001       0.001   \n   weight                       -0.0044       0.000     -10.426    0.000      -0.005      -0.004   \n   year                          0.7456       0.046      16.145    0.000       0.655       0.836   \n   origin                        1.0465       0.238       4.405    0.000       0.579       1.514     \n   Omnibus:         21.819     Durbin-Watson:           1.500   \n   Prob(Omnibus):    0.000     Jarque-Bera (JB):       32.447   \n   Skew:             0.414     Prob(JB):             9.00e-08   \n   Kurtosis:         4.140     Cond. No.             4.10e+05    fig = plt.figure()\nfitted = reg.fittedvalues\nsns.regplot(fitted, df.mpg - fitted,  lowess=True, line_kws={'color':'r', 'lw':1})\nax = fig.axes[0]\nax.axhline(color=\"grey\", ls=\"--\")\nax.set_title(\"Residuals vs Fitted Values\")\nax.set_xlabel(\"Fitted Values\")\nax.set_ylabel(\"Residuals\");   It is clear that this quadratic term is statistically significant. Let's try adding a logarithmic term as well.  reg = smf.ols('mpg ~ horsepower + np.power(horsepower,2) + np.log(horsepower) + weight + year + origin', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:             mpg           R-squared:               0.855    \n   Model:                     OLS           Adj. R-squared:          0.853    \n   Method:               Least Squares      F-statistic:             378.6    \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   4.62e-158   \n   Time:                   09:49:11         Log-Likelihood:        -982.62    \n   No. Observations:           392          AIC:                     1979.    \n   Df Residuals:               385          BIC:                     2007.    \n   Df Model:                     6                                            \n   Covariance Type:        nonrobust                                            \n                               coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept                    80.3681      25.568       3.143    0.002      30.098     130.638   \n   horsepower                    0.2570       0.148       1.737    0.083      -0.034       0.548   \n   np.power(horsepower, 2)      -0.0002       0.000      -0.571    0.568      -0.001       0.000   \n   np.log(horsepower)          -27.5412       8.000      -3.443    0.001     -43.270     -11.812   \n   weight                       -0.0048       0.000     -11.098    0.000      -0.006      -0.004   \n   year                          0.7561       0.046      16.565    0.000       0.666       0.846   \n   origin                        0.9480       0.236       4.016    0.000       0.484       1.412     \n   Omnibus:         17.905     Durbin-Watson:           1.575   \n   Prob(Omnibus):    0.000     Jarque-Bera (JB):       29.299   \n   Skew:             0.312     Prob(JB):             4.34e-07   \n   Kurtosis:         4.185     Cond. No.             2.84e+06    Now the p-value for the square term is very large.\nThis indicates that there is indeed a non-linearity but it seems to be better captured by the logarithm than the square. \nLet's try adding the square root term.  reg = smf.ols('mpg ~ horsepower + np.power(horsepower,2) + np.log(horsepower) + np.sqrt(horsepower) + weight + year + origin', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:             mpg           R-squared:               0.859    \n   Model:                     OLS           Adj. R-squared:          0.856    \n   Method:               Least Squares      F-statistic:             332.9    \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   9.16e-159   \n   Time:                   09:49:12         Log-Likelihood:        -977.89    \n   No. Observations:           392          AIC:                     1972.    \n   Df Residuals:               384          BIC:                     2004.    \n   Df Model:                     7                                            \n   Covariance Type:        nonrobust                                            \n                               coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept                  -426.3991     167.288      -2.549    0.011    -755.314     -97.485   \n   horsepower                    8.4452       2.676       3.156    0.002       3.184      13.706   \n   np.power(horsepower, 2)      -0.0060       0.002      -3.117    0.002      -0.010      -0.002   \n   np.log(horsepower)          416.0064     144.951       2.870    0.004     131.009     701.004   \n   np.sqrt(horsepower)        -229.6161      74.927      -3.065    0.002    -376.934     -82.298   \n   weight                       -0.0048       0.000     -11.229    0.000      -0.006      -0.004   \n   year                          0.7475       0.045      16.522    0.000       0.659       0.836   \n   origin                        0.9088       0.234       3.886    0.000       0.449       1.369     \n   Omnibus:         23.395     Durbin-Watson:           1.570   \n   Prob(Omnibus):    0.000     Jarque-Bera (JB):       37.804   \n   Skew:             0.411     Prob(JB):             6.18e-09   \n   Kurtosis:         4.281     Cond. No.             2.50e+07    So now the square term is back to a small p-value, indicating that it is statistically significant in the presence of the square root and the logarithm.",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter3/exercise10/",
            "text": "Exercise 3.10\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf #statsmodels is a Python module for statistics\n\n%matplotlib inline\n\n\n\n\nurl = 'https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/ISLR/Carseats.csv '\ndf = pd.read_csv(url,index_col=0)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nSales\n\n      \nCompPrice\n\n      \nIncome\n\n      \nAdvertising\n\n      \nPopulation\n\n      \nPrice\n\n      \nShelveLoc\n\n      \nAge\n\n      \nEducation\n\n      \nUrban\n\n      \nUS\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n9.50\n\n      \n138\n\n      \n73\n\n      \n11\n\n      \n276\n\n      \n120\n\n      \nBad\n\n      \n42\n\n      \n17\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n2\n\n      \n11.22\n\n      \n111\n\n      \n48\n\n      \n16\n\n      \n260\n\n      \n83\n\n      \nGood\n\n      \n65\n\n      \n10\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n3\n\n      \n10.06\n\n      \n113\n\n      \n35\n\n      \n10\n\n      \n269\n\n      \n80\n\n      \nMedium\n\n      \n59\n\n      \n12\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \n7.40\n\n      \n117\n\n      \n100\n\n      \n4\n\n      \n466\n\n      \n97\n\n      \nMedium\n\n      \n55\n\n      \n14\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n5\n\n      \n4.15\n\n      \n141\n\n      \n64\n\n      \n3\n\n      \n340\n\n      \n128\n\n      \nBad\n\n      \n38\n\n      \n13\n\n      \nYes\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\n# fit regression model\nmod = smf.ols(formula='Sales ~ Price + Urban + US',data=df)\nres = mod.fit()\nres.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n          \nSales\n      \n  R-squared:         \n \n   0.239\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.234\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   41.52\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n2.39e-23\n\n\n\n\n\n  \nTime:\n                 \n09:49:21\n     \n  Log-Likelihood:    \n \n -927.66\n\n\n\n\n\n  \nNo. Observations:\n      \n   400\n      \n  AIC:               \n \n   1863.\n\n\n\n\n\n  \nDf Residuals:\n          \n   396\n      \n  BIC:               \n \n   1879.\n\n\n\n\n\n  \nDf Model:\n              \n     3\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n        \n          \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n    \n   13.0435\n \n    0.651\n \n   20.036\n \n 0.000\n \n   11.764\n \n   14.323\n\n\n\n\n\n  \nUrban[T.Yes]\n \n   -0.0219\n \n    0.272\n \n   -0.081\n \n 0.936\n \n   -0.556\n \n    0.512\n\n\n\n\n\n  \nUS[T.Yes]\n    \n    1.2006\n \n    0.259\n \n    4.635\n \n 0.000\n \n    0.691\n \n    1.710\n\n\n\n\n\n  \nPrice\n        \n   -0.0545\n \n    0.005\n \n  -10.389\n \n 0.000\n \n   -0.065\n \n   -0.044\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 0.676\n \n  Durbin-Watson:     \n \n   1.912\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.713\n \n  Jarque-Bera (JB):  \n \n   0.758\n\n\n\n\n\n  \nSkew:\n          \n 0.093\n \n  Prob(JB):          \n \n   0.684\n\n\n\n\n\n  \nKurtosis:\n      \n 2.897\n \n  Cond. No.          \n \n    628.\n\n\n\n\n\n\n\n(b)\n\n\nInterpretation\n\n\n \nUrban.\n This coefficient is not statistically significant, suggesting that there is no relationship between this variable and the sales.\n\n \nUS.\n Qualitative variable with positive relationship. This means that when the observation is US, there will be a tendency for higher sales values. On average, if a store is located in the US, it will sell 1201 more units, approximately.\n* \nPrice.\n Quantitative variable with negative relationship. This means that the higher the prices, the lower the sales. On average, for every dollar that the price increases sales will drop by 55 units, approximately.\n\n\n# (c)\n\n\n\n\n\nSales = 13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price =\n\\begin{cases} \n      13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price & Urban=1, US=1 \\\\\n      13.0435-0.0219 \\times Urban + 1.2006 \\times Price & Urban=1, US=0 \\\\\n      13.0435+1.2006 \\times US - 0.0545 \\times Price & Urban=0, US=1 \\\\\n      13.0435- 0.0545 \\times Price & Urban=0, US=0\n\\end{cases}\n\n\n\n\nSales = 13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price =\n\\begin{cases} \n      13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price & Urban=1, US=1 \\\\\n      13.0435-0.0219 \\times Urban + 1.2006 \\times Price & Urban=1, US=0 \\\\\n      13.0435+1.2006 \\times US - 0.0545 \\times Price & Urban=0, US=1 \\\\\n      13.0435- 0.0545 \\times Price & Urban=0, US=0\n\\end{cases}\n\n\n\n\n\n(d)\n\n\nFrom the p-values in the summary above, we can reject the null hypothesis for the intercept, US and Price, but not for Urban.\n\n\n(e)\n\n\nmod = smf.ols(formula='Sales ~ Price + US',data=df)\nres = mod.fit()\nprint(res.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.239\nModel:                            OLS   Adj. R-squared:                  0.235\nMethod:                 Least Squares   F-statistic:                     62.43\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           2.66e-24\nTime:                        09:49:21   Log-Likelihood:                -927.66\nNo. Observations:                 400   AIC:                             1861.\nDf Residuals:                     397   BIC:                             1873.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     13.0308      0.631     20.652      0.000      11.790      14.271\nUS[T.Yes]      1.1996      0.258      4.641      0.000       0.692       1.708\nPrice         -0.0545      0.005    -10.416      0.000      -0.065      -0.044\n==============================================================================\nOmnibus:                        0.666   Durbin-Watson:                   1.912\nProb(Omnibus):                  0.717   Jarque-Bera (JB):                0.749\nSkew:                           0.092   Prob(JB):                        0.688\nKurtosis:                       2.895   Cond. No.                         607.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n(f)\n\n\nTo compare how well the models fit, we can consider the value of R-squared. R-squared is the coefficient of determination. The coefficient of determination measures how much of the variance can be explained by the independent variables considered in the model.\n\n\nSince R-squared has the same value, namely 0.239, for both models we can conclude that the strictly smaller model (e), is a better model since it uses less variables for the same value of R-squared. This can also be seen in the value of the adjusted R-squared which is smaller for (e). In any case neither model fits the data very well given the low value of R-squared.\n\n\n(g)\n\n\nFor linear regression, the 95% confidence interval for the coefficients approximately takes the form:\n\n\n\n\n \\hat{\\beta} \\pm 2 \\times SE(\\hat{\\beta})\n\n\n \\hat{\\beta} \\pm 2 \\times SE(\\hat{\\beta})\n\n\n\n\nwhere \nSE\nSE\n is the standard error of the coefficient \n\\hat{\\beta}\n\\hat{\\beta}\n.\n\n\n# confidence interval for intercept\nintercept_coef = 13.0308\nintercept_stderr = .631\n\nus_coef = 1.1996\nus_stderr = .258\n\nprice_coef = -.0545\nprice_stderr = .005\n\nprint('95%% confidence interval for Intercept: [ %2.4f; %2.4f] ' % (intercept_coef-2*intercept_stderr, intercept_coef+2*intercept_stderr))\nprint('95%% confidence interval for Intercept: [ %2.4f; %2.4f] ' % (us_coef-2*us_stderr, us_coef+2*us_stderr))\nprint('95%% confidence interval for Intercept: [ %2.4f; %2.4f] ' % (price_coef-2*price_stderr, price_coef+2*price_stderr))\n\n\n\n\n95% confidence interval for Intercept: [ 11.7688; 14.2928] \n95% confidence interval for Intercept: [ 0.6836; 1.7156] \n95% confidence interval for Intercept: [ -0.0645; -0.0445]\n\n\n\nAs we can check this does not precisely equal the values in the summary table above. This is because of the approximation mentioned in the footnote on page 66 of the text.\n\n\n(h)\n\n\nTo investigate the presence of outliers or high leverage points, we analyse a plot of standardized residuals against leverages.\n\n\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rc('font', size=14)\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\nmodel_f = 'Sales ~ Price + US'\n\ndf.reset_index(drop=True, inplace=True)\n\nmodel = smf.ols(formula=model_f, data=df)\n\nmodel_fit = model.fit()\n\n# fitted values (need a constant term for intercept)\nmodel_fitted_y = model_fit.fittedvalues\n\n# model residuals\nmodel_residuals = model_fit.resid\n\n# normalized residuals\nmodel_norm_residuals = model_fit.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n\n# leverage, from statsmodels internals\nmodel_leverage = model_fit.get_influence().hat_matrix_diag\n\n# cook's distance, from statsmodels internals\nmodel_cooks = model_fit.get_influence().cooks_distance[0]\n\n\n\n\nplot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(model_leverage, model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0, 0.20)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n\n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None, ls='-'):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls=ls, color='red')\n\np = len(model_fit.params) # number of model parameters\n\ngraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance = .5', ls='--') # 0.5 line\n\ngraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), 'Cook\\'s distance = 1', ls=':') # 1 line\n\nplt.legend(loc='upper right');\n\n\n\n\n\n\nFrom this plot we can that no outliers (all less than 3). Since \n(p+1)/n=3/400=0.0075\n(p+1)/n=3/400=0.0075\n, we can see that there a few candidates for high leverage points, although every point is well below a Cook's distance of 1.\n\n\nSo on the whole this indicates that there are no outliers, but that there are some high leverage points although likely not very influential.",
            "title": "3.10"
        },
        {
            "location": "/sols/chapter3/exercise10/#exercise-310",
            "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf #statsmodels is a Python module for statistics\n\n%matplotlib inline  url = 'https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/ISLR/Carseats.csv '\ndf = pd.read_csv(url,index_col=0)  df.head()   \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       Sales \n       CompPrice \n       Income \n       Advertising \n       Population \n       Price \n       ShelveLoc \n       Age \n       Education \n       Urban \n       US \n     \n   \n   \n     \n       1 \n       9.50 \n       138 \n       73 \n       11 \n       276 \n       120 \n       Bad \n       42 \n       17 \n       Yes \n       Yes \n     \n     \n       2 \n       11.22 \n       111 \n       48 \n       16 \n       260 \n       83 \n       Good \n       65 \n       10 \n       Yes \n       Yes \n     \n     \n       3 \n       10.06 \n       113 \n       35 \n       10 \n       269 \n       80 \n       Medium \n       59 \n       12 \n       Yes \n       Yes \n     \n     \n       4 \n       7.40 \n       117 \n       100 \n       4 \n       466 \n       97 \n       Medium \n       55 \n       14 \n       Yes \n       Yes \n     \n     \n       5 \n       4.15 \n       141 \n       64 \n       3 \n       340 \n       128 \n       Bad \n       38 \n       13 \n       Yes \n       No",
            "title": "Exercise 3.10"
        },
        {
            "location": "/sols/chapter3/exercise10/#a",
            "text": "# fit regression model\nmod = smf.ols(formula='Sales ~ Price + Urban + US',data=df)\nres = mod.fit()\nres.summary()   OLS Regression Results  \n   Dep. Variable:            Sales          R-squared:               0.239   \n   Model:                     OLS           Adj. R-squared:          0.234   \n   Method:               Least Squares      F-statistic:             41.52   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   2.39e-23   \n   Time:                   09:49:21         Log-Likelihood:        -927.66   \n   No. Observations:           400          AIC:                     1863.   \n   Df Residuals:               396          BIC:                     1879.   \n   Df Model:                     3                                           \n   Covariance Type:        nonrobust                                           \n                    coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept         13.0435       0.651      20.036    0.000      11.764      14.323   \n   Urban[T.Yes]      -0.0219       0.272      -0.081    0.936      -0.556       0.512   \n   US[T.Yes]          1.2006       0.259       4.635    0.000       0.691       1.710   \n   Price             -0.0545       0.005     -10.389    0.000      -0.065      -0.044     \n   Omnibus:          0.676     Durbin-Watson:           1.912   \n   Prob(Omnibus):    0.713     Jarque-Bera (JB):        0.758   \n   Skew:             0.093     Prob(JB):                0.684   \n   Kurtosis:         2.897     Cond. No.                 628.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise10/#b",
            "text": "Interpretation    Urban.  This coefficient is not statistically significant, suggesting that there is no relationship between this variable and the sales.   US.  Qualitative variable with positive relationship. This means that when the observation is US, there will be a tendency for higher sales values. On average, if a store is located in the US, it will sell 1201 more units, approximately.\n*  Price.  Quantitative variable with negative relationship. This means that the higher the prices, the lower the sales. On average, for every dollar that the price increases sales will drop by 55 units, approximately.  # (c)   \nSales = 13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price =\n\\begin{cases} \n      13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price & Urban=1, US=1 \\\\\n      13.0435-0.0219 \\times Urban + 1.2006 \\times Price & Urban=1, US=0 \\\\\n      13.0435+1.2006 \\times US - 0.0545 \\times Price & Urban=0, US=1 \\\\\n      13.0435- 0.0545 \\times Price & Urban=0, US=0\n\\end{cases}  \nSales = 13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price =\n\\begin{cases} \n      13.0435-0.0219 \\times Urban + 1.2006 \\times US - 0.0545 \\times Price & Urban=1, US=1 \\\\\n      13.0435-0.0219 \\times Urban + 1.2006 \\times Price & Urban=1, US=0 \\\\\n      13.0435+1.2006 \\times US - 0.0545 \\times Price & Urban=0, US=1 \\\\\n      13.0435- 0.0545 \\times Price & Urban=0, US=0\n\\end{cases}",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise10/#d",
            "text": "From the p-values in the summary above, we can reject the null hypothesis for the intercept, US and Price, but not for Urban.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter3/exercise10/#e",
            "text": "mod = smf.ols(formula='Sales ~ Price + US',data=df)\nres = mod.fit()\nprint(res.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.239\nModel:                            OLS   Adj. R-squared:                  0.235\nMethod:                 Least Squares   F-statistic:                     62.43\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           2.66e-24\nTime:                        09:49:21   Log-Likelihood:                -927.66\nNo. Observations:                 400   AIC:                             1861.\nDf Residuals:                     397   BIC:                             1873.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     13.0308      0.631     20.652      0.000      11.790      14.271\nUS[T.Yes]      1.1996      0.258      4.641      0.000       0.692       1.708\nPrice         -0.0545      0.005    -10.416      0.000      -0.065      -0.044\n==============================================================================\nOmnibus:                        0.666   Durbin-Watson:                   1.912\nProb(Omnibus):                  0.717   Jarque-Bera (JB):                0.749\nSkew:                           0.092   Prob(JB):                        0.688\nKurtosis:                       2.895   Cond. No.                         607.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter3/exercise10/#f",
            "text": "To compare how well the models fit, we can consider the value of R-squared. R-squared is the coefficient of determination. The coefficient of determination measures how much of the variance can be explained by the independent variables considered in the model.  Since R-squared has the same value, namely 0.239, for both models we can conclude that the strictly smaller model (e), is a better model since it uses less variables for the same value of R-squared. This can also be seen in the value of the adjusted R-squared which is smaller for (e). In any case neither model fits the data very well given the low value of R-squared.",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter3/exercise10/#g",
            "text": "For linear regression, the 95% confidence interval for the coefficients approximately takes the form:    \\hat{\\beta} \\pm 2 \\times SE(\\hat{\\beta})   \\hat{\\beta} \\pm 2 \\times SE(\\hat{\\beta})   where  SE SE  is the standard error of the coefficient  \\hat{\\beta} \\hat{\\beta} .  # confidence interval for intercept\nintercept_coef = 13.0308\nintercept_stderr = .631\n\nus_coef = 1.1996\nus_stderr = .258\n\nprice_coef = -.0545\nprice_stderr = .005\n\nprint('95%% confidence interval for Intercept: [ %2.4f; %2.4f] ' % (intercept_coef-2*intercept_stderr, intercept_coef+2*intercept_stderr))\nprint('95%% confidence interval for Intercept: [ %2.4f; %2.4f] ' % (us_coef-2*us_stderr, us_coef+2*us_stderr))\nprint('95%% confidence interval for Intercept: [ %2.4f; %2.4f] ' % (price_coef-2*price_stderr, price_coef+2*price_stderr))  95% confidence interval for Intercept: [ 11.7688; 14.2928] \n95% confidence interval for Intercept: [ 0.6836; 1.7156] \n95% confidence interval for Intercept: [ -0.0645; -0.0445]  As we can check this does not precisely equal the values in the summary table above. This is because of the approximation mentioned in the footnote on page 66 of the text.",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter3/exercise10/#h",
            "text": "To investigate the presence of outliers or high leverage points, we analyse a plot of standardized residuals against leverages.  import statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rc('font', size=14)\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\nmodel_f = 'Sales ~ Price + US'\n\ndf.reset_index(drop=True, inplace=True)\n\nmodel = smf.ols(formula=model_f, data=df)\n\nmodel_fit = model.fit()\n\n# fitted values (need a constant term for intercept)\nmodel_fitted_y = model_fit.fittedvalues\n\n# model residuals\nmodel_residuals = model_fit.resid\n\n# normalized residuals\nmodel_norm_residuals = model_fit.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n\n# leverage, from statsmodels internals\nmodel_leverage = model_fit.get_influence().hat_matrix_diag\n\n# cook's distance, from statsmodels internals\nmodel_cooks = model_fit.get_influence().cooks_distance[0]  plot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(model_leverage, model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0, 0.20)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n\n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None, ls='-'):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls=ls, color='red')\n\np = len(model_fit.params) # number of model parameters\n\ngraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance = .5', ls='--') # 0.5 line\n\ngraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), 'Cook\\'s distance = 1', ls=':') # 1 line\n\nplt.legend(loc='upper right');   From this plot we can that no outliers (all less than 3). Since  (p+1)/n=3/400=0.0075 (p+1)/n=3/400=0.0075 , we can see that there a few candidates for high leverage points, although every point is well below a Cook's distance of 1.  So on the whole this indicates that there are no outliers, but that there are some high leverage points although likely not very influential.",
            "title": "(h)"
        },
        {
            "location": "/sols/chapter3/exercise11/",
            "text": "Exercise 3.11\n\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(1)\n# does not generate the same sequences as R\n# it seems there's no easy, elegant way of getting Python and R to generate the same random sequences\n# http://stackoverflow.com/questions/22213298/creating-same-random-number-sequence-in-python-numpy-and-r\n\nx = np.random.normal(size=100)\ny = 2*x+np.random.normal(size=100)\n\ndf = pd.DataFrame({'x': x, 'y': y})\n\nfig, ax = plt.subplots()\nsns.regplot(x='x', y='y', data=df, scatter_kws={\"s\": 50, \"alpha\": 1}, ax=ax)\nax.axhline(color='gray')\nax.axvline(color='gray')\n\n\n\n\n\n\n<matplotlib.lines.Line2D at 0x118965588>\n\n\n\n\n\n(a)\n\n\nreg = smf.ols('y ~ x + 0', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n            \ny\n        \n  R-squared:         \n \n   0.798\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.796\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   391.7\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n3.46e-36\n\n\n\n\n\n  \nTime:\n                 \n09:50:01\n     \n  Log-Likelihood:    \n \n -135.67\n\n\n\n\n\n  \nNo. Observations:\n      \n   100\n      \n  AIC:               \n \n   273.3\n\n\n\n\n\n  \nDf Residuals:\n          \n    99\n      \n  BIC:               \n \n   275.9\n\n\n\n\n\n  \nDf Model:\n              \n     1\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n  \n     \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nx\n \n    2.1067\n \n    0.106\n \n   19.792\n \n 0.000\n \n    1.896\n \n    2.318\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 0.880\n \n  Durbin-Watson:     \n \n   2.106\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.644\n \n  Jarque-Bera (JB):  \n \n   0.554\n\n\n\n\n\n  \nSkew:\n          \n-0.172\n \n  Prob(JB):          \n \n   0.758\n\n\n\n\n\n  \nKurtosis:\n      \n 3.119\n \n  Cond. No.          \n \n    1.00\n\n\n\n\n\n\n\nFrom the table above, we see that the coefficient estimate \n\\hat{\\beta} = 2.1067\n\\hat{\\beta} = 2.1067\n and the standard error of this coefficient estimate is 0.106. The t-statistic is equal to 19.792 and the p-value is close to 0 (less than 0.0005). We can therefore reject the null hypothesis and conclude that there is evidence for a relationship between x and y.\n\n\n(b)\n\n\nreg = smf.ols('x ~ y + 0', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n            \nx\n        \n  R-squared:         \n \n   0.798\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.796\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   391.7\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n3.46e-36\n\n\n\n\n\n  \nTime:\n                 \n09:50:01\n     \n  Log-Likelihood:    \n \n -49.891\n\n\n\n\n\n  \nNo. Observations:\n      \n   100\n      \n  AIC:               \n \n   101.8\n\n\n\n\n\n  \nDf Residuals:\n          \n    99\n      \n  BIC:               \n \n   104.4\n\n\n\n\n\n  \nDf Model:\n              \n     1\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n  \n     \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \ny\n \n    0.3789\n \n    0.019\n \n   19.792\n \n 0.000\n \n    0.341\n \n    0.417\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 0.476\n \n  Durbin-Watson:     \n \n   2.166\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.788\n \n  Jarque-Bera (JB):  \n \n   0.631\n\n\n\n\n\n  \nSkew:\n          \n 0.115\n \n  Prob(JB):          \n \n   0.729\n\n\n\n\n\n  \nKurtosis:\n      \n 2.685\n \n  Cond. No.          \n \n    1.00\n\n\n\n\n\n\n\nFrom the table above, we see that the coefficient estimate \n\\hat{\\beta} = 0.3789\n\\hat{\\beta} = 0.3789\n and the standard error of this coefficient estimate is 0.019. The t-statistic is equal to 19.792 and the p-value is close to 0 (less than 0.0005). We can therefore reject the null hypothesis and conclude that there is evidence for a relationship between x and y.\n\n\n(c)\n\n\nAs we can see from the two tables above, the t-statistics are the same, which means that the p-values are the same. Why the t-statistics are the same is the subject of question (e) below.\nPerhaps surprisingly, the coefficient estimates are not the inverse of each other.\nThat is, \n\\hat{\\beta}_y \\neq 1/ \\hat{\\beta}_x\n\\hat{\\beta}_y \\neq 1/ \\hat{\\beta}_x\n, where \n\\hat{\\beta}_y\n\\hat{\\beta}_y\n and \n\\hat{\\beta}_x\n\\hat{\\beta}_x\n are the coefficients of the models \nY = \\hat{\\beta}_y X\nY = \\hat{\\beta}_y X\n and \nX = \\hat{\\beta}_x Y\nX = \\hat{\\beta}_x Y\n, respectively.\nWe know \n\\hat{\\beta}_x \\simeq 2\n\\hat{\\beta}_x \\simeq 2\n, but \n\\hat{\\beta}_y\n\\hat{\\beta}_y\n seems to be closer to 0.4 than 0.5.\nWhat is going on here?\n\n\nThe short answer: it has to do with the fitting method and with the non-symmetric loss function.\n\n\nThe visual insight is given in this \nanswer\n. As the two figures in this link suggest, we are not minimizing the same loss function when we find the estimate coefficient. \n\n\nThis can also be seen algebraically. The coefficient estimates are the values that minimize the loss functions of ordinary least squares for these linear models without intercept, which leads to:\n\n\n\\hat{\\beta}_y = \\underset{\\beta}{\\operatorname{argmin}} \\sum (y_i-\\beta x_i)^2 = \\frac{\\sum y_i x_i}{\\sum y_i^2} \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[Y^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(Y-\\operatorname{E}[Y])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(Y)}\\right)\n\\hat{\\beta}_y = \\underset{\\beta}{\\operatorname{argmin}} \\sum (y_i-\\beta x_i)^2 = \\frac{\\sum y_i x_i}{\\sum y_i^2} \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[Y^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(Y-\\operatorname{E}[Y])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(Y)}\\right)\n,\n\n\n\\hat{\\beta}_x = \\underset{\\beta}{\\operatorname{argmin}} \\sum (x_i-\\beta y_i)^2 = \\frac{\\sum y_i x_i}{\\sum x_i^2}  \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[X^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(X-\\operatorname{E}[X])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(X)}\\right)\n\\hat{\\beta}_x = \\underset{\\beta}{\\operatorname{argmin}} \\sum (x_i-\\beta y_i)^2 = \\frac{\\sum y_i x_i}{\\sum x_i^2}  \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[X^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(X-\\operatorname{E}[X])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(X)}\\right)\n, \n\n\nsince \n\\operatorname{E}[X]=\\operatorname{E}[Y]=0\n\\operatorname{E}[X]=\\operatorname{E}[Y]=0\n.\n\n\nSince the covariance is symmetric the coefficient estimates will depend on the respective variances.\nIn other words, the coefficients estimates will depend on and differ by the respectives spreads of the data, \n\\sum y_i^2\n\\sum y_i^2\n and \n\\sum x_i^2\n\\sum x_i^2\n.\n\n\nWe can see how the additional noise of Y breaks the symmetry.\nWe know that the \n\\operatorname{var}(X) = 1\n\\operatorname{var}(X) = 1\n, so that \n\\operatorname{var}(Y) = \\operatorname{var}(2X + \\epsilon ) = 4 \\operatorname{var}(X) + \\operatorname{var}(\\epsilon) = 4 (1) + 1 = 5\n\\operatorname{var}(Y) = \\operatorname{var}(2X + \\epsilon ) = 4 \\operatorname{var}(X) + \\operatorname{var}(\\epsilon) = 4 (1) + 1 = 5\n.\nSince \n\\sum y_i x_i =\\sum (2 x_i + \\epsilon_i) x_i = 2 \\sum x_i^2 + \\sum x_i \\epsilon_i \\sim 2\n\\sum y_i x_i =\\sum (2 x_i + \\epsilon_i) x_i = 2 \\sum x_i^2 + \\sum x_i \\epsilon_i \\sim 2\n. So  \n\\hat{\\beta}_x \\sim 2/1 = 2\n\\hat{\\beta}_x \\sim 2/1 = 2\n and \n\\hat{\\beta}_y \\sim 2/5 = .4\n\\hat{\\beta}_y \\sim 2/5 = .4\n\n\nIf we use a loss function symmetrical in x and y (as the rectangles or total least squares regression, or PCA, figured below), then the coeffiecients will be the same.\n\n\n(d)\n\n\nThis is a simple exercise of manipulating the formulas given.\n\n\nWe start with a simplification of a part of the formula for \nSE(\\hat{\\beta})\nSE(\\hat{\\beta})\n:\n\n\n$\\sum_{i=1}^{n} (y_i - x_i \\beta)^2 = \\sum_{i=1}^{n} (y_i^2 - 2 x_i \\beta + x_i^2 \\beta^2) =  \\sum_{i=1}^{n} y_i^2 - 2 \\beta  \\sum_{i=1}^{n} x_i  +  \\beta^2 \\sum_{i=1}^{n} x_i^2  $\n\n\nwhich using the formula (3.38) from the text \n\\hat{\\beta}=\\sum_{i=1}^{n} x_i y_i/\\sum_{j=1}^{n} x_j^2\n\\hat{\\beta}=\\sum_{i=1}^{n} x_i y_i/\\sum_{j=1}^{n} x_j^2\n becomes:\n\n\n$  \\sum_{i=1}^{n} y_i^2 - 2 \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{j=1}^{n} x_j^2 } + \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{j=1}^{n} x_j^2 } =  \\sum_{i=1}^{n} y_i^2 -  \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{i=1}^{n} x_i^2 }.$\n\n\nWith this, we restart from the formula for the t-statistic and simplify:\n\n\n$\\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}=   \\frac{(\\sum_{i=1}^{n} x_i y_i) \\sqrt{n-1} \\sqrt{\\sum_{i=1}^{n} x_i^2}  }{  ( \\sum_{i=1}^{n} x_i^2  )  \\sqrt{\\sum_{i=1}^{n} (y_i - x_i \\beta)^2}} = \\frac{(\\sum_{i=1}^{n} x_i y_i) \\sqrt{n-1}    }{ \\sqrt{ (\\sum_{i=1}^{n} x_i^2  ) \\sum_{i=1}^{n} (y_i - x_i \\beta)^2}} $\n\n\nwhere we now substitute our previous simplification in the denominator:\n\n\n$  \\frac{(\\sum_{i=1}^{n} x_i y_i) \\sqrt{n-1}    }{ \\sqrt{ (\\sum_{i=1}^{n} x_i^2  )   \\left( \\sum_{i=1}^{n} y_i^2 -  \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{i=1}^{n} x_i^2 } \\right)}} = \\frac{  (\\sqrt{n-1} )   \\sum_{i=1}^{n} x_i y_i    }{ \\sqrt{ (\\sum_{i=1}^{n} x_i^2  )( \\sum_{i=1}^{n} y_i^2 ) -  (\\sum_{i=1}^{n} x_i y_i)^2 }  }, $\n\n\nas we wanted.\n\n\n(e)\n\n\nSince the expression for the t-statistic is symmetric in \nx_i\nx_i\n and \ny_i\ny_i\n, it will have the same value whether we regress Y on X or X on Y.\n\n\n(f)\n\n\nregyx = smf.ols('y ~ x', df).fit()\nregxy = smf.ols('x ~ y', df).fit()\nprint(regyx.tvalues)\nprint(regxy.tvalues)\n\n\n\n\nIntercept     1.564347\nx            19.782585\ndtype: float64\nIntercept    -1.089559\ny            19.782585\ndtype: float64\n\n\n\nReferences:\n\n\n\n\nhttps://stats.stackexchange.com/questions/20553/effect-of-switching-response-and-explanatory-variable-in-simple-linear-regressio\n\n\nhttps://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y",
            "title": "3.11"
        },
        {
            "location": "/sols/chapter3/exercise11/#exercise-311",
            "text": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(1)\n# does not generate the same sequences as R\n# it seems there's no easy, elegant way of getting Python and R to generate the same random sequences\n# http://stackoverflow.com/questions/22213298/creating-same-random-number-sequence-in-python-numpy-and-r\n\nx = np.random.normal(size=100)\ny = 2*x+np.random.normal(size=100)\n\ndf = pd.DataFrame({'x': x, 'y': y})\n\nfig, ax = plt.subplots()\nsns.regplot(x='x', y='y', data=df, scatter_kws={\"s\": 50, \"alpha\": 1}, ax=ax)\nax.axhline(color='gray')\nax.axvline(color='gray')  <matplotlib.lines.Line2D at 0x118965588>",
            "title": "Exercise 3.11"
        },
        {
            "location": "/sols/chapter3/exercise11/#a",
            "text": "reg = smf.ols('y ~ x + 0', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:              y            R-squared:               0.798   \n   Model:                     OLS           Adj. R-squared:          0.796   \n   Method:               Least Squares      F-statistic:             391.7   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   3.46e-36   \n   Time:                   09:50:01         Log-Likelihood:        -135.67   \n   No. Observations:           100          AIC:                     273.3   \n   Df Residuals:                99          BIC:                     275.9   \n   Df Model:                     1                                           \n   Covariance Type:        nonrobust                                           \n         coef       std err        t        P>|t|    [0.025      0.975]     \n   x       2.1067       0.106      19.792    0.000       1.896       2.318     \n   Omnibus:          0.880     Durbin-Watson:           2.106   \n   Prob(Omnibus):    0.644     Jarque-Bera (JB):        0.554   \n   Skew:            -0.172     Prob(JB):                0.758   \n   Kurtosis:         3.119     Cond. No.                 1.00    From the table above, we see that the coefficient estimate  \\hat{\\beta} = 2.1067 \\hat{\\beta} = 2.1067  and the standard error of this coefficient estimate is 0.106. The t-statistic is equal to 19.792 and the p-value is close to 0 (less than 0.0005). We can therefore reject the null hypothesis and conclude that there is evidence for a relationship between x and y.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise11/#b",
            "text": "reg = smf.ols('x ~ y + 0', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:              x            R-squared:               0.798   \n   Model:                     OLS           Adj. R-squared:          0.796   \n   Method:               Least Squares      F-statistic:             391.7   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   3.46e-36   \n   Time:                   09:50:01         Log-Likelihood:        -49.891   \n   No. Observations:           100          AIC:                     101.8   \n   Df Residuals:                99          BIC:                     104.4   \n   Df Model:                     1                                           \n   Covariance Type:        nonrobust                                           \n         coef       std err        t        P>|t|    [0.025      0.975]     \n   y       0.3789       0.019      19.792    0.000       0.341       0.417     \n   Omnibus:          0.476     Durbin-Watson:           2.166   \n   Prob(Omnibus):    0.788     Jarque-Bera (JB):        0.631   \n   Skew:             0.115     Prob(JB):                0.729   \n   Kurtosis:         2.685     Cond. No.                 1.00    From the table above, we see that the coefficient estimate  \\hat{\\beta} = 0.3789 \\hat{\\beta} = 0.3789  and the standard error of this coefficient estimate is 0.019. The t-statistic is equal to 19.792 and the p-value is close to 0 (less than 0.0005). We can therefore reject the null hypothesis and conclude that there is evidence for a relationship between x and y.",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise11/#c",
            "text": "As we can see from the two tables above, the t-statistics are the same, which means that the p-values are the same. Why the t-statistics are the same is the subject of question (e) below.\nPerhaps surprisingly, the coefficient estimates are not the inverse of each other.\nThat is,  \\hat{\\beta}_y \\neq 1/ \\hat{\\beta}_x \\hat{\\beta}_y \\neq 1/ \\hat{\\beta}_x , where  \\hat{\\beta}_y \\hat{\\beta}_y  and  \\hat{\\beta}_x \\hat{\\beta}_x  are the coefficients of the models  Y = \\hat{\\beta}_y X Y = \\hat{\\beta}_y X  and  X = \\hat{\\beta}_x Y X = \\hat{\\beta}_x Y , respectively.\nWe know  \\hat{\\beta}_x \\simeq 2 \\hat{\\beta}_x \\simeq 2 , but  \\hat{\\beta}_y \\hat{\\beta}_y  seems to be closer to 0.4 than 0.5.\nWhat is going on here?  The short answer: it has to do with the fitting method and with the non-symmetric loss function.  The visual insight is given in this  answer . As the two figures in this link suggest, we are not minimizing the same loss function when we find the estimate coefficient.   This can also be seen algebraically. The coefficient estimates are the values that minimize the loss functions of ordinary least squares for these linear models without intercept, which leads to:  \\hat{\\beta}_y = \\underset{\\beta}{\\operatorname{argmin}} \\sum (y_i-\\beta x_i)^2 = \\frac{\\sum y_i x_i}{\\sum y_i^2} \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[Y^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(Y-\\operatorname{E}[Y])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(Y)}\\right) \\hat{\\beta}_y = \\underset{\\beta}{\\operatorname{argmin}} \\sum (y_i-\\beta x_i)^2 = \\frac{\\sum y_i x_i}{\\sum y_i^2} \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[Y^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(Y-\\operatorname{E}[Y])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(Y)}\\right) ,  \\hat{\\beta}_x = \\underset{\\beta}{\\operatorname{argmin}} \\sum (x_i-\\beta y_i)^2 = \\frac{\\sum y_i x_i}{\\sum x_i^2}  \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[X^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(X-\\operatorname{E}[X])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(X)}\\right) \\hat{\\beta}_x = \\underset{\\beta}{\\operatorname{argmin}} \\sum (x_i-\\beta y_i)^2 = \\frac{\\sum y_i x_i}{\\sum x_i^2}  \\left(= \\frac{\\operatorname{E}[X Y]}{\\operatorname{E}[X^2]} = \\frac{\\operatorname{E}[X Y] - \\operatorname{E}[X]\\operatorname{E}[Y]}{\\operatorname{E}[(X-\\operatorname{E}[X])^2]} = \\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}(X)}\\right) ,   since  \\operatorname{E}[X]=\\operatorname{E}[Y]=0 \\operatorname{E}[X]=\\operatorname{E}[Y]=0 .  Since the covariance is symmetric the coefficient estimates will depend on the respective variances.\nIn other words, the coefficients estimates will depend on and differ by the respectives spreads of the data,  \\sum y_i^2 \\sum y_i^2  and  \\sum x_i^2 \\sum x_i^2 .  We can see how the additional noise of Y breaks the symmetry.\nWe know that the  \\operatorname{var}(X) = 1 \\operatorname{var}(X) = 1 , so that  \\operatorname{var}(Y) = \\operatorname{var}(2X + \\epsilon ) = 4 \\operatorname{var}(X) + \\operatorname{var}(\\epsilon) = 4 (1) + 1 = 5 \\operatorname{var}(Y) = \\operatorname{var}(2X + \\epsilon ) = 4 \\operatorname{var}(X) + \\operatorname{var}(\\epsilon) = 4 (1) + 1 = 5 .\nSince  \\sum y_i x_i =\\sum (2 x_i + \\epsilon_i) x_i = 2 \\sum x_i^2 + \\sum x_i \\epsilon_i \\sim 2 \\sum y_i x_i =\\sum (2 x_i + \\epsilon_i) x_i = 2 \\sum x_i^2 + \\sum x_i \\epsilon_i \\sim 2 . So   \\hat{\\beta}_x \\sim 2/1 = 2 \\hat{\\beta}_x \\sim 2/1 = 2  and  \\hat{\\beta}_y \\sim 2/5 = .4 \\hat{\\beta}_y \\sim 2/5 = .4  If we use a loss function symmetrical in x and y (as the rectangles or total least squares regression, or PCA, figured below), then the coeffiecients will be the same.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise11/#d",
            "text": "This is a simple exercise of manipulating the formulas given.  We start with a simplification of a part of the formula for  SE(\\hat{\\beta}) SE(\\hat{\\beta}) :  $\\sum_{i=1}^{n} (y_i - x_i \\beta)^2 = \\sum_{i=1}^{n} (y_i^2 - 2 x_i \\beta + x_i^2 \\beta^2) =  \\sum_{i=1}^{n} y_i^2 - 2 \\beta  \\sum_{i=1}^{n} x_i  +  \\beta^2 \\sum_{i=1}^{n} x_i^2  $  which using the formula (3.38) from the text  \\hat{\\beta}=\\sum_{i=1}^{n} x_i y_i/\\sum_{j=1}^{n} x_j^2 \\hat{\\beta}=\\sum_{i=1}^{n} x_i y_i/\\sum_{j=1}^{n} x_j^2  becomes:  $  \\sum_{i=1}^{n} y_i^2 - 2 \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{j=1}^{n} x_j^2 } + \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{j=1}^{n} x_j^2 } =  \\sum_{i=1}^{n} y_i^2 -  \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{i=1}^{n} x_i^2 }.$  With this, we restart from the formula for the t-statistic and simplify:  $\\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}=   \\frac{(\\sum_{i=1}^{n} x_i y_i) \\sqrt{n-1} \\sqrt{\\sum_{i=1}^{n} x_i^2}  }{  ( \\sum_{i=1}^{n} x_i^2  )  \\sqrt{\\sum_{i=1}^{n} (y_i - x_i \\beta)^2}} = \\frac{(\\sum_{i=1}^{n} x_i y_i) \\sqrt{n-1}    }{ \\sqrt{ (\\sum_{i=1}^{n} x_i^2  ) \\sum_{i=1}^{n} (y_i - x_i \\beta)^2}} $  where we now substitute our previous simplification in the denominator:  $  \\frac{(\\sum_{i=1}^{n} x_i y_i) \\sqrt{n-1}    }{ \\sqrt{ (\\sum_{i=1}^{n} x_i^2  )   \\left( \\sum_{i=1}^{n} y_i^2 -  \\frac{(\\sum_{i=1}^{n} x_i y_i)^2}{\\sum_{i=1}^{n} x_i^2 } \\right)}} = \\frac{  (\\sqrt{n-1} )   \\sum_{i=1}^{n} x_i y_i    }{ \\sqrt{ (\\sum_{i=1}^{n} x_i^2  )( \\sum_{i=1}^{n} y_i^2 ) -  (\\sum_{i=1}^{n} x_i y_i)^2 }  }, $  as we wanted.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter3/exercise11/#e",
            "text": "Since the expression for the t-statistic is symmetric in  x_i x_i  and  y_i y_i , it will have the same value whether we regress Y on X or X on Y.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter3/exercise11/#f",
            "text": "regyx = smf.ols('y ~ x', df).fit()\nregxy = smf.ols('x ~ y', df).fit()\nprint(regyx.tvalues)\nprint(regxy.tvalues)  Intercept     1.564347\nx            19.782585\ndtype: float64\nIntercept    -1.089559\ny            19.782585\ndtype: float64",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter3/exercise11/#references",
            "text": "https://stats.stackexchange.com/questions/20553/effect-of-switching-response-and-explanatory-variable-in-simple-linear-regressio  https://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y",
            "title": "References:"
        },
        {
            "location": "/sols/chapter3/exercise12/",
            "text": "Exercise 3.12\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# to use linear regression models as an alternative to statsmodels\nfrom sklearn.linear_model import LinearRegression \n\n\n\n\n(a)\n\n\nWe have seen from exercise 3.11 that the formulas for the estimates for the linear regression of Y onto X and X onto Y are, respectively:\n\n\n\\hat{\\beta}_y = \\frac{\\sum y_i x_i}{\\sum y_i^2}\n\\hat{\\beta}_y = \\frac{\\sum y_i x_i}{\\sum y_i^2}\n,\n\n\n\\hat{\\beta}_x = \\frac{\\sum y_i x_i}{\\sum x_i^2}.\n\\hat{\\beta}_x = \\frac{\\sum y_i x_i}{\\sum x_i^2}.\n\n\nIt is clear than that the estimates will be the same whenever \n\\sum x_i^2 = \\sum y_i^2\n\\sum x_i^2 = \\sum y_i^2\n, which will not be the case in general.\n\n\n(b)\n\n\nIn general, the sum of the squares of \nx_i\nx_i\n and \ny_i\ny_i\n will be different, and nearly every sample of 100 observations will lead to different estimates of the coefficients of X onto Y and Y onto X, even when the underlying model is Y = X as long as there is noise. We do an example below.\n\n\nx = np.arange(100)\n\n\n\n\ny = x + np.random.normal(size=100)\n\n\n\n\n# reshape to avoid problems with LinearRegression\n# sklearn requires the data shape of (row number, column number), which means shape can't be (X,); it must be (X,1)\nx = x.reshape(np.shape(x)[0],1)\ny = y.reshape(np.shape(y)[0],1)\n\n\n\n\n# linear regression\nlr = LinearRegression(fit_intercept=False) #without intercept\nlr.fit(x,y)\nlr.coef_\n\n\n\n\narray([[ 0.99689073]])\n\n\n\nlr.fit(y,x)\nlr.coef_\n\n\n\n\narray([[ 1.00282725]])\n\n\n\n(c)\n\n\nTo garantee we have $\\sum x_i^2 = \\sum y_i^2 $, we can have \ny_i = x_i\ny_i = x_i\n, for every i, or have them shuffled, for example.\n\n\n# linear regression\nlr = LinearRegression(fit_intercept=False) #without intercept\nlr.fit(x,y)\nlr.coef_\n\n\n\n\narray([[ 0.99689073]])\n\n\n\nlr.fit(y,x)\nlr.coef_\n\n\n\n\narray([[ 1.00282725]])\n\n\n\nx = np.random.randint(200, size=100)\ny = np.random.permutation(x)\n\n# same as in (b)\nx = x.reshape(np.shape(y)[0],1)\ny = y.reshape(np.shape(y)[0],1)\n\nlr = LinearRegression(fit_intercept=False) #without intercept\nlr.fit(x,y)\ncoef_beta_x = lr.coef_\nplt.subplot(1,2,1)\nplt.scatter(x, y)\nplt.plot(x, lr.predict(x), color='blue', linewidth=3)\n\nlr.fit(y,x)\ncoef_beta_y = lr.coef_\nplt.subplot(1,2,2)\nplt.scatter(y, x)\nplt.plot(y, lr.predict(y), color='blue', linewidth=3)\nplt.tight_layout()\nplt.show()\n\nprint(\"beta_x = \", coef_beta_x, \" ; beta_y = \", coef_beta_y)\n\n\n\n\n\n\nbeta_x =  [[ 0.8158522]]  ; beta_y =  [[ 0.8158522]]",
            "title": "3.12"
        },
        {
            "location": "/sols/chapter3/exercise12/#exercise-312",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# to use linear regression models as an alternative to statsmodels\nfrom sklearn.linear_model import LinearRegression",
            "title": "Exercise 3.12"
        },
        {
            "location": "/sols/chapter3/exercise12/#a",
            "text": "We have seen from exercise 3.11 that the formulas for the estimates for the linear regression of Y onto X and X onto Y are, respectively:  \\hat{\\beta}_y = \\frac{\\sum y_i x_i}{\\sum y_i^2} \\hat{\\beta}_y = \\frac{\\sum y_i x_i}{\\sum y_i^2} ,  \\hat{\\beta}_x = \\frac{\\sum y_i x_i}{\\sum x_i^2}. \\hat{\\beta}_x = \\frac{\\sum y_i x_i}{\\sum x_i^2}.  It is clear than that the estimates will be the same whenever  \\sum x_i^2 = \\sum y_i^2 \\sum x_i^2 = \\sum y_i^2 , which will not be the case in general.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise12/#b",
            "text": "In general, the sum of the squares of  x_i x_i  and  y_i y_i  will be different, and nearly every sample of 100 observations will lead to different estimates of the coefficients of X onto Y and Y onto X, even when the underlying model is Y = X as long as there is noise. We do an example below.  x = np.arange(100)  y = x + np.random.normal(size=100)  # reshape to avoid problems with LinearRegression\n# sklearn requires the data shape of (row number, column number), which means shape can't be (X,); it must be (X,1)\nx = x.reshape(np.shape(x)[0],1)\ny = y.reshape(np.shape(y)[0],1)  # linear regression\nlr = LinearRegression(fit_intercept=False) #without intercept\nlr.fit(x,y)\nlr.coef_  array([[ 0.99689073]])  lr.fit(y,x)\nlr.coef_  array([[ 1.00282725]])",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise12/#c",
            "text": "To garantee we have $\\sum x_i^2 = \\sum y_i^2 $, we can have  y_i = x_i y_i = x_i , for every i, or have them shuffled, for example.  # linear regression\nlr = LinearRegression(fit_intercept=False) #without intercept\nlr.fit(x,y)\nlr.coef_  array([[ 0.99689073]])  lr.fit(y,x)\nlr.coef_  array([[ 1.00282725]])  x = np.random.randint(200, size=100)\ny = np.random.permutation(x)\n\n# same as in (b)\nx = x.reshape(np.shape(y)[0],1)\ny = y.reshape(np.shape(y)[0],1)\n\nlr = LinearRegression(fit_intercept=False) #without intercept\nlr.fit(x,y)\ncoef_beta_x = lr.coef_\nplt.subplot(1,2,1)\nplt.scatter(x, y)\nplt.plot(x, lr.predict(x), color='blue', linewidth=3)\n\nlr.fit(y,x)\ncoef_beta_y = lr.coef_\nplt.subplot(1,2,2)\nplt.scatter(y, x)\nplt.plot(y, lr.predict(y), color='blue', linewidth=3)\nplt.tight_layout()\nplt.show()\n\nprint(\"beta_x = \", coef_beta_x, \" ; beta_y = \", coef_beta_y)   beta_x =  [[ 0.8158522]]  ; beta_y =  [[ 0.8158522]]",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise13/",
            "text": "Exercise 3.13\n\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LinearRegression\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nnp.random.seed(1)\n# does not generate the same sequences as R\n# it seems there's no easy, elegant way of getting Python and R to generate the same random sequences\n# http://stackoverflow.com/questions/22213298/creating-same-random-number-sequence-in-python-numpy-and-r\n\n\n\n\n(a)\n\n\nx = np.random.normal(size=100)\n\n# http://seaborn.pydata.org/examples/distplot_options.html\nfig, ax = plt.subplots()\nsns.despine(left=True)\n\n# Plot a kernel density estimate and rug plot\nsns.distplot(x, hist=False, rug=True, color=\"r\", ax=ax);\n\n\n\n\n\n\n(b)\n\n\neps = np.random.normal(scale = .25, size=100)\n\nfig, ax = plt.subplots()\nsns.despine(left=True)\nsns.distplot(eps, hist=False, rug=True, color=\"r\", ax=ax);\n\n\n\n\n\n\n(c)\n\n\ny = -1 + .5*x + eps\nprint('Length of y = ' + str(len(y)))\n\n\n\n\nLength of y = 100\n\n\n\nNaturally, vector y has length 100, since it is a linear combination of 2 vectors of length 100.\n\n\nIn this model \n\\beta_0 = -1\n\\beta_0 = -1\n and \n\\beta_1=1/2\n\\beta_1=1/2\n.\n\n\n(d)\n\n\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.jointplot(x='x', y='y', data=df);\n\n\n\n\n\n\nAs expected, it does seem linear with a variance on the order of 0.5.\nNo alarming indication of high leverage points or outliers.\n\n\n(e)\n\n\nreg = smf.ols('y ~ x', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n            \ny\n        \n  R-squared:         \n \n   0.800\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.798\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   391.4\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n5.39e-36\n\n\n\n\n\n  \nTime:\n                 \n13:05:05\n     \n  Log-Likelihood:    \n \n  4.1908\n\n\n\n\n\n  \nNo. Observations:\n      \n   100\n      \n  AIC:               \n \n  -4.382\n\n\n\n\n\n  \nDf Residuals:\n          \n    98\n      \n  BIC:               \n \n  0.8288\n\n\n\n\n\n  \nDf Model:\n              \n     1\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n      \n         \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n \n   -0.9632\n \n    0.023\n \n  -40.999\n \n 0.000\n \n   -1.010\n \n   -0.917\n\n\n\n\n\n  \nx\n         \n    0.5239\n \n    0.026\n \n   19.783\n \n 0.000\n \n    0.471\n \n    0.576\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 0.898\n \n  Durbin-Watson:     \n \n   2.157\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.638\n \n  Jarque-Bera (JB):  \n \n   0.561\n\n\n\n\n\n  \nSkew:\n          \n-0.172\n \n  Prob(JB):          \n \n   0.755\n\n\n\n\n\n  \nKurtosis:\n      \n 3.127\n \n  Cond. No.          \n \n    1.15\n\n\n\n\n\n\n\nThe coefficients of the regression are similar to the \"true\" values, although they are not equal (naturally). They fall within the 95% confidence interval in both cases.\n\n\n(f)\n\n\nmodel = LinearRegression(fit_intercept= True)\nmodel.fit(x[:, np.newaxis],y)\n\n\nplt.scatter(df.x, df.y);\n\nxfit = np.linspace(df.x.min(), df.x.max(), 100)\nyfit = model.predict(xfit[:, np.newaxis])\nfit, = plt.plot(xfit, yfit, color='r');\n\nxpop = np.linspace(df.x.min(), df.x.max(), 100)\nypop = -1 + .5*xpop\npop, = plt.plot(xpop, ypop, color='g');\nplt.legend([fit, pop],['Fit line','Model line'])\n\n\n\n\n<matplotlib.legend.Legend at 0x10c324eb8>\n\n\n\n\n\n(g)\n\n\nreg = smf.ols('y ~ x + I(x**2)', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n            \ny\n        \n  R-squared:         \n \n   0.800\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.796\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   193.8\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n1.32e-34\n\n\n\n\n\n  \nTime:\n                 \n13:05:14\n     \n  Log-Likelihood:    \n \n  4.2077\n\n\n\n\n\n  \nNo. Observations:\n      \n   100\n      \n  AIC:               \n \n  -2.415\n\n\n\n\n\n  \nDf Residuals:\n          \n    97\n      \n  BIC:               \n \n   5.400\n\n\n\n\n\n  \nDf Model:\n              \n     2\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n      \n         \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n \n   -0.9663\n \n    0.029\n \n  -33.486\n \n 0.000\n \n   -1.024\n \n   -0.909\n\n\n\n\n\n  \nx\n         \n    0.5234\n \n    0.027\n \n   19.582\n \n 0.000\n \n    0.470\n \n    0.576\n\n\n\n\n\n  \nI(x ** 2)\n \n    0.0039\n \n    0.021\n \n    0.181\n \n 0.856\n \n   -0.038\n \n    0.046\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 0.893\n \n  Durbin-Watson:     \n \n   2.152\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.640\n \n  Jarque-Bera (JB):  \n \n   0.552\n\n\n\n\n\n  \nSkew:\n          \n-0.170\n \n  Prob(JB):          \n \n   0.759\n\n\n\n\n\n  \nKurtosis:\n      \n 3.132\n \n  Cond. No.          \n \n    2.10\n\n\n\n\n\n\n\nAfter fitting a model with basis functions \nX\nX\n and \nX^2\nX^2\n, looking at the table above we can see that the p-value for the squared term is 0.856, so we can conclude that there is no evidence to reject the null hypothesis asserting the coefficient of the second order term is zero. In other words, there is no statistically significant relationship between \nX^2\nX^2\n and \nY\nY\n - the quadratic term does not improve the model fit. This is also evident when comparing the R-squared of both fits. They are equal, even though the second model has an additional variable.\n\n\n(h)\n\n\nx = np.random.normal(size=100)\neps = np.random.normal(scale = .05, size=100)\ny = -1 + .5*x + eps\n\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.jointplot(x='x', y='y', data=df)\n\nmodel = LinearRegression(fit_intercept= True)\nmodel.fit(x[:, np.newaxis],y)\n\nplt.subplots()\nplt.scatter(df.x, df.y);\n\n\nxfit = np.linspace(df.x.min(), df.x.max(), 100)\nyfit = model.predict(xfit[:, np.newaxis])\nfit, = plt.plot(xfit, yfit, color='r');\n\nxpop = np.linspace(df.x.min(), df.x.max(), 100)\nypop = -1 + .5*xpop\npop, = plt.plot(xpop, ypop, color='g');\nplt.legend([fit, pop],['Fit line','Model line'])\n\n\n\n\n\n<matplotlib.legend.Legend at 0x10c742b38>\n\n\n\n\n\n\n\nreg = smf.ols('y ~ x', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n            \ny\n        \n  R-squared:         \n \n   0.989\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.989\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   8662.\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n1.97e-97\n\n\n\n\n\n  \nTime:\n                 \n13:05:22\n     \n  Log-Likelihood:    \n \n  151.58\n\n\n\n\n\n  \nNo. Observations:\n      \n   100\n      \n  AIC:               \n \n  -299.2\n\n\n\n\n\n  \nDf Residuals:\n          \n    98\n      \n  BIC:               \n \n  -293.9\n\n\n\n\n\n  \nDf Model:\n              \n     1\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n      \n         \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n \n   -1.0010\n \n    0.005\n \n -186.443\n \n 0.000\n \n   -1.012\n \n   -0.990\n\n\n\n\n\n  \nx\n         \n    0.4972\n \n    0.005\n \n   93.071\n \n 0.000\n \n    0.487\n \n    0.508\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 0.426\n \n  Durbin-Watson:     \n \n   1.977\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.808\n \n  Jarque-Bera (JB):  \n \n   0.121\n\n\n\n\n\n  \nSkew:\n          \n-0.045\n \n  Prob(JB):          \n \n   0.941\n\n\n\n\n\n  \nKurtosis:\n      \n 3.145\n \n  Cond. No.          \n \n    1.01\n\n\n\n\n\n\n\nAs expected, we have a better fit, a better confidence intervals, and a higher R-squared. \n\n\nOur model fits this data set better than the previous one that was generated from a noisier distribution.\n\n\n(i)\n\n\nx = np.random.normal(size=100)\neps = np.random.normal(scale = 1, size=100)\ny = -1 + .5*x + eps\n\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.jointplot(x='x', y='y', data=df)\n\nmodel = LinearRegression(fit_intercept= True)\nmodel.fit(x[:, np.newaxis],y)\n\nplt.subplots()\nplt.scatter(df.x, df.y);\n\n\nxfit = np.linspace(df.x.min(), df.x.max(), 100)\nyfit = model.predict(xfit[:, np.newaxis])\nfit, = plt.plot(xfit, yfit, color='r');\n\nxpop = np.linspace(df.x.min(), df.x.max(), 100)\nypop = -1 + .5*xpop\npop, = plt.plot(xpop, ypop, color='g');\nplt.legend([fit, pop],['Fit line','Model line'])\n\n\n\n\n\n<matplotlib.legend.Legend at 0x10bc05b00>\n\n\n\n\n\n\n\n(i)\n\n\nreg = smf.ols('y ~ x', df).fit()\nreg.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n            \ny\n        \n  R-squared:         \n \n   0.257\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.250\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   33.96\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n7.19e-08\n\n\n\n\n\n  \nTime:\n                 \n13:05:27\n     \n  Log-Likelihood:    \n \n -145.66\n\n\n\n\n\n  \nNo. Observations:\n      \n   100\n      \n  AIC:               \n \n   295.3\n\n\n\n\n\n  \nDf Residuals:\n          \n    98\n      \n  BIC:               \n \n   300.5\n\n\n\n\n\n  \nDf Model:\n              \n     1\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n      \n         \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n \n   -0.8802\n \n    0.105\n \n   -8.375\n \n 0.000\n \n   -1.089\n \n   -0.672\n\n\n\n\n\n  \nx\n         \n    0.5903\n \n    0.101\n \n    5.828\n \n 0.000\n \n    0.389\n \n    0.791\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 3.633\n \n  Durbin-Watson:     \n \n   1.972\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.163\n \n  Jarque-Bera (JB):  \n \n   3.566\n\n\n\n\n\n  \nSkew:\n          \n 0.192\n \n  Prob(JB):          \n \n   0.168\n\n\n\n\n\n  \nKurtosis:\n      \n 3.842\n \n  Cond. No.          \n \n    1.07\n\n\n\n\n\n\n\nNow, on the other hand, we have a much worse fit. The R-squared is just .178 and the confidence intervals for the coefficients are much wider. Still there's no doubt we are in the presence of a statistically significant relationship, with very low p-values.\n\n\n(j)\n\n\nHere's a table with the various confidence intervals:\n\n\n\n\n\n\n\n\nNoise\n\n\nlower \n\\beta_0\n\\beta_0\n\n\n\\beta_0\n\\beta_0\n\n\nupper \n\\beta_0\n\\beta_0\n\n\nlower \n\\beta_1\n\\beta_1\n\n\n\\beta_1\n\\beta_1\n\n\nupper \n\\beta_1\n\\beta_1\n\n\n\n\n\n\n\n\n\n\n\\varepsilon = .05\n\\varepsilon = .05\n\n\n-1.009\n\n\n0.9984\n\n\n-0.987\n\n\n0.489\n\n\n0.5002\n\n\n0.512\n\n\n\n\n\n\n\\varepsilon=.25\n\\varepsilon=.25\n\n\n-1.010\n\n\n-0.9632\n\n\n-0.917\n\n\n0.471\n\n\n0.5239\n\n\n0.576\n\n\n\n\n\n\n\\varepsilon = 1\n\\varepsilon = 1\n\n\n-1.098\n\n\n-0.8869\n\n\n-0.675\n\n\n0.267\n\n\n0.4697\n\n\n0.672\n\n\n\n\n\n\n\n\nAs expected the width of the intervals increases as the level of noise increases.",
            "title": "3.13"
        },
        {
            "location": "/sols/chapter3/exercise13/#exercise-313",
            "text": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LinearRegression\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nnp.random.seed(1)\n# does not generate the same sequences as R\n# it seems there's no easy, elegant way of getting Python and R to generate the same random sequences\n# http://stackoverflow.com/questions/22213298/creating-same-random-number-sequence-in-python-numpy-and-r",
            "title": "Exercise 3.13"
        },
        {
            "location": "/sols/chapter3/exercise13/#a",
            "text": "x = np.random.normal(size=100)\n\n# http://seaborn.pydata.org/examples/distplot_options.html\nfig, ax = plt.subplots()\nsns.despine(left=True)\n\n# Plot a kernel density estimate and rug plot\nsns.distplot(x, hist=False, rug=True, color=\"r\", ax=ax);",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise13/#b",
            "text": "eps = np.random.normal(scale = .25, size=100)\n\nfig, ax = plt.subplots()\nsns.despine(left=True)\nsns.distplot(eps, hist=False, rug=True, color=\"r\", ax=ax);",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise13/#c",
            "text": "y = -1 + .5*x + eps\nprint('Length of y = ' + str(len(y)))  Length of y = 100  Naturally, vector y has length 100, since it is a linear combination of 2 vectors of length 100.  In this model  \\beta_0 = -1 \\beta_0 = -1  and  \\beta_1=1/2 \\beta_1=1/2 .",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise13/#d",
            "text": "df = pd.DataFrame({'x': x, 'y': y})\nsns.jointplot(x='x', y='y', data=df);   As expected, it does seem linear with a variance on the order of 0.5.\nNo alarming indication of high leverage points or outliers.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter3/exercise13/#e",
            "text": "reg = smf.ols('y ~ x', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:              y            R-squared:               0.800   \n   Model:                     OLS           Adj. R-squared:          0.798   \n   Method:               Least Squares      F-statistic:             391.4   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   5.39e-36   \n   Time:                   13:05:05         Log-Likelihood:         4.1908   \n   No. Observations:           100          AIC:                    -4.382   \n   Df Residuals:                98          BIC:                    0.8288   \n   Df Model:                     1                                           \n   Covariance Type:        nonrobust                                           \n                 coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept      -0.9632       0.023     -40.999    0.000      -1.010      -0.917   \n   x               0.5239       0.026      19.783    0.000       0.471       0.576     \n   Omnibus:          0.898     Durbin-Watson:           2.157   \n   Prob(Omnibus):    0.638     Jarque-Bera (JB):        0.561   \n   Skew:            -0.172     Prob(JB):                0.755   \n   Kurtosis:         3.127     Cond. No.                 1.15    The coefficients of the regression are similar to the \"true\" values, although they are not equal (naturally). They fall within the 95% confidence interval in both cases.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter3/exercise13/#f",
            "text": "model = LinearRegression(fit_intercept= True)\nmodel.fit(x[:, np.newaxis],y)\n\n\nplt.scatter(df.x, df.y);\n\nxfit = np.linspace(df.x.min(), df.x.max(), 100)\nyfit = model.predict(xfit[:, np.newaxis])\nfit, = plt.plot(xfit, yfit, color='r');\n\nxpop = np.linspace(df.x.min(), df.x.max(), 100)\nypop = -1 + .5*xpop\npop, = plt.plot(xpop, ypop, color='g');\nplt.legend([fit, pop],['Fit line','Model line'])  <matplotlib.legend.Legend at 0x10c324eb8>",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter3/exercise13/#g",
            "text": "reg = smf.ols('y ~ x + I(x**2)', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:              y            R-squared:               0.800   \n   Model:                     OLS           Adj. R-squared:          0.796   \n   Method:               Least Squares      F-statistic:             193.8   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   1.32e-34   \n   Time:                   13:05:14         Log-Likelihood:         4.2077   \n   No. Observations:           100          AIC:                    -2.415   \n   Df Residuals:                97          BIC:                     5.400   \n   Df Model:                     2                                           \n   Covariance Type:        nonrobust                                           \n                 coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept      -0.9663       0.029     -33.486    0.000      -1.024      -0.909   \n   x               0.5234       0.027      19.582    0.000       0.470       0.576   \n   I(x ** 2)       0.0039       0.021       0.181    0.856      -0.038       0.046     \n   Omnibus:          0.893     Durbin-Watson:           2.152   \n   Prob(Omnibus):    0.640     Jarque-Bera (JB):        0.552   \n   Skew:            -0.170     Prob(JB):                0.759   \n   Kurtosis:         3.132     Cond. No.                 2.10    After fitting a model with basis functions  X X  and  X^2 X^2 , looking at the table above we can see that the p-value for the squared term is 0.856, so we can conclude that there is no evidence to reject the null hypothesis asserting the coefficient of the second order term is zero. In other words, there is no statistically significant relationship between  X^2 X^2  and  Y Y  - the quadratic term does not improve the model fit. This is also evident when comparing the R-squared of both fits. They are equal, even though the second model has an additional variable.",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter3/exercise13/#h",
            "text": "x = np.random.normal(size=100)\neps = np.random.normal(scale = .05, size=100)\ny = -1 + .5*x + eps\n\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.jointplot(x='x', y='y', data=df)\n\nmodel = LinearRegression(fit_intercept= True)\nmodel.fit(x[:, np.newaxis],y)\n\nplt.subplots()\nplt.scatter(df.x, df.y);\n\n\nxfit = np.linspace(df.x.min(), df.x.max(), 100)\nyfit = model.predict(xfit[:, np.newaxis])\nfit, = plt.plot(xfit, yfit, color='r');\n\nxpop = np.linspace(df.x.min(), df.x.max(), 100)\nypop = -1 + .5*xpop\npop, = plt.plot(xpop, ypop, color='g');\nplt.legend([fit, pop],['Fit line','Model line'])  <matplotlib.legend.Legend at 0x10c742b38>    reg = smf.ols('y ~ x', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:              y            R-squared:               0.989   \n   Model:                     OLS           Adj. R-squared:          0.989   \n   Method:               Least Squares      F-statistic:             8662.   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   1.97e-97   \n   Time:                   13:05:22         Log-Likelihood:         151.58   \n   No. Observations:           100          AIC:                    -299.2   \n   Df Residuals:                98          BIC:                    -293.9   \n   Df Model:                     1                                           \n   Covariance Type:        nonrobust                                           \n                 coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept      -1.0010       0.005    -186.443    0.000      -1.012      -0.990   \n   x               0.4972       0.005      93.071    0.000       0.487       0.508     \n   Omnibus:          0.426     Durbin-Watson:           1.977   \n   Prob(Omnibus):    0.808     Jarque-Bera (JB):        0.121   \n   Skew:            -0.045     Prob(JB):                0.941   \n   Kurtosis:         3.145     Cond. No.                 1.01    As expected, we have a better fit, a better confidence intervals, and a higher R-squared.   Our model fits this data set better than the previous one that was generated from a noisier distribution.",
            "title": "(h)"
        },
        {
            "location": "/sols/chapter3/exercise13/#i",
            "text": "x = np.random.normal(size=100)\neps = np.random.normal(scale = 1, size=100)\ny = -1 + .5*x + eps\n\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.jointplot(x='x', y='y', data=df)\n\nmodel = LinearRegression(fit_intercept= True)\nmodel.fit(x[:, np.newaxis],y)\n\nplt.subplots()\nplt.scatter(df.x, df.y);\n\n\nxfit = np.linspace(df.x.min(), df.x.max(), 100)\nyfit = model.predict(xfit[:, np.newaxis])\nfit, = plt.plot(xfit, yfit, color='r');\n\nxpop = np.linspace(df.x.min(), df.x.max(), 100)\nypop = -1 + .5*xpop\npop, = plt.plot(xpop, ypop, color='g');\nplt.legend([fit, pop],['Fit line','Model line'])  <matplotlib.legend.Legend at 0x10bc05b00>",
            "title": "(i)"
        },
        {
            "location": "/sols/chapter3/exercise13/#i_1",
            "text": "reg = smf.ols('y ~ x', df).fit()\nreg.summary()   OLS Regression Results  \n   Dep. Variable:              y            R-squared:               0.257   \n   Model:                     OLS           Adj. R-squared:          0.250   \n   Method:               Least Squares      F-statistic:             33.96   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   7.19e-08   \n   Time:                   13:05:27         Log-Likelihood:        -145.66   \n   No. Observations:           100          AIC:                     295.3   \n   Df Residuals:                98          BIC:                     300.5   \n   Df Model:                     1                                           \n   Covariance Type:        nonrobust                                           \n                 coef       std err        t        P>|t|    [0.025      0.975]     \n   Intercept      -0.8802       0.105      -8.375    0.000      -1.089      -0.672   \n   x               0.5903       0.101       5.828    0.000       0.389       0.791     \n   Omnibus:          3.633     Durbin-Watson:           1.972   \n   Prob(Omnibus):    0.163     Jarque-Bera (JB):        3.566   \n   Skew:             0.192     Prob(JB):                0.168   \n   Kurtosis:         3.842     Cond. No.                 1.07    Now, on the other hand, we have a much worse fit. The R-squared is just .178 and the confidence intervals for the coefficients are much wider. Still there's no doubt we are in the presence of a statistically significant relationship, with very low p-values.",
            "title": "(i)"
        },
        {
            "location": "/sols/chapter3/exercise13/#j",
            "text": "Here's a table with the various confidence intervals:     Noise  lower  \\beta_0 \\beta_0  \\beta_0 \\beta_0  upper  \\beta_0 \\beta_0  lower  \\beta_1 \\beta_1  \\beta_1 \\beta_1  upper  \\beta_1 \\beta_1      \\varepsilon = .05 \\varepsilon = .05  -1.009  0.9984  -0.987  0.489  0.5002  0.512    \\varepsilon=.25 \\varepsilon=.25  -1.010  -0.9632  -0.917  0.471  0.5239  0.576    \\varepsilon = 1 \\varepsilon = 1  -1.098  -0.8869  -0.675  0.267  0.4697  0.672     As expected the width of the intervals increases as the level of noise increases.",
            "title": "(j)"
        },
        {
            "location": "/sols/chapter3/exercise14/",
            "text": "Exercise 3.14\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm # To use statsmodel\nimport statsmodels.formula.api as smf # To use statsmodel with R-style formulas\nfrom statsmodels.stats import outliers_influence\nfrom sklearn.linear_model import LinearRegression\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\nnp.random.seed(5) # Python and R random generators give different values\nx1 = np.random.uniform(size=100)\nx2 = 0.5 * x1 + np.random.normal(size=100) / 10\ny = 2 + 2 * x1 + 0.3 * x2 + np.random.normal(size=100)\n\n# http://stackoverflow.com/questions/22213298/creating-same-random-number-sequence-in-python-numpy-and-r\n\n\n\n\nModel form:\n\n\n\n\n\nY = \\beta_0 + \\beta_1  X_1 + \\beta_2  X_2 + \\epsilon = 2 + 2 X_1 + 0.3 X_2 + \\epsilon.\n\n\n\n\nY = \\beta_0 + \\beta_1  X_1 + \\beta_2  X_2 + \\epsilon = 2 + 2 X_1 + 0.3 X_2 + \\epsilon.\n\n\n\n\n\n(b)\n\n\n# Get correlations\nnp.corrcoef(x1,x2)\n\n\n\n\narray([[ 1.        ,  0.81936924],\n       [ 0.81936924,  1.        ]])\n\n\n\nThe correlation coefficient between \nX_1\nX_1\n and \nX_2\nX_2\n is \n0.819\n.\n\n\n# Draw scatterplot\nplt.scatter(x1,x2);\n\n\n\n\n\n\n(c)\n\n\n# Define data\nX = pd.DataFrame({'x1':x1, 'x2':x2})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\n# Create model\nmodel = sm.OLS(y, X)\n\n# Fit regression model\nresults = model.fit()\n\n# Print results\nresults.summary()\n\n\n\n\n\n\nOLS Regression Results\n\n\n\n  \nDep. Variable:\n            \ny\n        \n  R-squared:         \n \n   0.444\n\n\n\n\n\n  \nModel:\n                   \nOLS\n       \n  Adj. R-squared:    \n \n   0.433\n\n\n\n\n\n  \nMethod:\n             \nLeast Squares\n  \n  F-statistic:       \n \n   38.74\n\n\n\n\n\n  \nDate:\n             \nFri, 08 Dec 2017\n \n  Prob (F-statistic):\n \n4.31e-13\n\n\n\n\n\n  \nTime:\n                 \n09:50:56\n     \n  Log-Likelihood:    \n \n -123.67\n\n\n\n\n\n  \nNo. Observations:\n      \n   100\n      \n  AIC:               \n \n   253.3\n\n\n\n\n\n  \nDf Residuals:\n          \n    97\n      \n  BIC:               \n \n   261.1\n\n\n\n\n\n  \nDf Model:\n              \n     2\n      \n                     \n     \n \n   \n\n\n\n\n  \nCovariance Type:\n      \nnonrobust\n    \n                     \n     \n \n   \n\n\n\n\n\n\n\n\n\n    \n       \ncoef\n     \nstd err\n      \nt\n      \nP>|t|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nconst\n \n    1.8158\n \n    0.162\n \n   11.231\n \n 0.000\n \n    1.495\n \n    2.137\n\n\n\n\n\n  \nx1\n    \n    2.0758\n \n    0.488\n \n    4.257\n \n 0.000\n \n    1.108\n \n    3.044\n\n\n\n\n\n  \nx2\n    \n    0.7584\n \n    0.817\n \n    0.929\n \n 0.355\n \n   -0.862\n \n    2.379\n\n\n\n\n\n\n\n\n\n\n  \nOmnibus:\n       \n 0.718\n \n  Durbin-Watson:     \n \n   1.960\n\n\n\n\n\n  \nProb(Omnibus):\n \n 0.698\n \n  Jarque-Bera (JB):  \n \n   0.574\n\n\n\n\n\n  \nSkew:\n          \n-0.185\n \n  Prob(JB):          \n \n   0.750\n\n\n\n\n\n  \nKurtosis:\n      \n 2.981\n \n  Cond. No.          \n \n    12.5\n\n\n\n\n\n\n\nAccording to the results we have:\n\n\n\n\n\\hat{\\beta_0} = 1.8158\n\\hat{\\beta_0} = 1.8158\n \n\n\n\\hat{\\beta_1} = 2.0758\n\\hat{\\beta_1} = 2.0758\n \n\n\n\\hat{\\beta_2} = 0.7584\n\\hat{\\beta_2} = 0.7584\n \n\n\n\n\nThese values are estimators of the true coefficients, which have the following values:\n\n\n\n\n\\beta_0 = 2\n\\beta_0 = 2\n\n\n\\beta_1 = 2\n\\beta_1 = 2\n\n\n\\beta_2 = 0.3\n\\beta_2 = 0.3\n\n\n\n\nAs we can see, there are some differences between the coefficients, especially in the case of \n\\hat{\\beta_2}\n\\hat{\\beta_2}\n (0.7584 vs. 0.3). \n\n\n \nH_0 : \\beta_1 = 0\nH_0 : \\beta_1 = 0\n \n. The rejection of the null hypothesis depends on the t-statistic (t). In the case of \n\\beta_1\n\\beta_1\n, this value is high. If the t-statistics is high, the p-value will be low. The p-value is the probability of observing any value equal to |t| or larger than (P>|t|), assuming that the coefficient is zero. Thus, if the p-value is low we should \nreject the null hypothesis and accept the alternative hypothesis\n.\n\n\n \nH_0 : \\beta_2 = 0\nH_0 : \\beta_2 = 0\n \n. In this case the t-statistic is low (0.929) and the p-value is high (0.355). Accordingly, the \nnull hypothesis can't be rejected\n.\n\n\nAlternative solution\n\n\nA different way to approximate the equation using R-style formula in StatsModel.\n\n\n# Define data\ndf = pd.DataFrame({'x1':x1, 'x2':x2, 'y':y})  # We don't need to add constant because we will use formulas\n\n# Create model\nmod = smf.ols(formula='y ~ x1 + x2', data=df)  # R-style command\n\n# Fit model\nres = mod.fit()\n\n# Print results\nprint (res.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.444\nModel:                            OLS   Adj. R-squared:                  0.433\nMethod:                 Least Squares   F-statistic:                     38.74\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           4.31e-13\nTime:                        09:50:56   Log-Likelihood:                -123.67\nNo. Observations:                 100   AIC:                             253.3\nDf Residuals:                      97   BIC:                             261.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.8158      0.162     11.231      0.000       1.495       2.137\nx1             2.0758      0.488      4.257      0.000       1.108       3.044\nx2             0.7584      0.817      0.929      0.355      -0.862       2.379\n==============================================================================\nOmnibus:                        0.718   Durbin-Watson:                   1.960\nProb(Omnibus):                  0.698   Jarque-Bera (JB):                0.574\nSkew:                          -0.185   Prob(JB):                        0.750\nKurtosis:                       2.981   Cond. No.                         12.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nAlternative solution\n\n\nThis is an alternative solution using Scikit instead of StatsModel.\n\n\n# Create model\nlr = LinearRegression()\n\n# Fit model\nmod = lr.fit(X,y)\n\n# Get coefficients\nmod.coef_\n\n\n\n\narray([ 0.        ,  2.0758066 ,  0.75840009])\n\n\n\n(d)\n\n\nX = pd.DataFrame({'x1':x1})\nX = sm.add_constant(X)\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.439\nModel:                            OLS   Adj. R-squared:                  0.433\nMethod:                 Least Squares   F-statistic:                     76.72\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           5.93e-14\nTime:                        09:50:56   Log-Likelihood:                -124.11\nNo. Observations:                 100   AIC:                             252.2\nDf Residuals:                      98   BIC:                             257.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.8229      0.161     11.295      0.000       1.503       2.143\nx1             2.4468      0.279      8.759      0.000       1.892       3.001\n==============================================================================\nOmnibus:                        0.357   Durbin-Watson:                   1.986\nProb(Omnibus):                  0.836   Jarque-Bera (JB):                0.272\nSkew:                          -0.127   Prob(JB):                        0.873\nKurtosis:                       2.963   Cond. No.                         4.17\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe coefficient value \nincreased\n to 2.4468 and the \nnull hypothesis can be rejected and the alternative hypothesis accepted\n because p-value is zero. It can be said that this results are in line with our expectations from (c).\n\n\n(e)\n\n\nX = pd.DataFrame({'x2':x2})\nX = sm.add_constant(X)\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.340\nModel:                            OLS   Adj. R-squared:                  0.333\nMethod:                 Least Squares   F-statistic:                     50.53\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           1.92e-10\nTime:                        09:50:56   Log-Likelihood:                -132.23\nNo. Observations:                 100   AIC:                             268.5\nDf Residuals:                      98   BIC:                             273.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.1250      0.157     13.572      0.000       1.814       2.436\nx2             3.6070      0.507      7.108      0.000       2.600       4.614\n==============================================================================\nOmnibus:                        1.537   Durbin-Watson:                   1.828\nProb(Omnibus):                  0.464   Jarque-Bera (JB):                1.597\nSkew:                          -0.272   Prob(JB):                        0.450\nKurtosis:                       2.704   Cond. No.                         5.89\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe coefficient value \nincreased\n to 3.6070 and the \nnull hypothesis can be rejected and the alternative hypothesis accepted\n because p-value is zero.\n\n\nThese results are significantly different from (c). In (c) the coefficient associated with \nx_2\nx_2\n had a lower value and the p-value suggested that the null hypothesis couldn't be rejected (coefficient value could be zero). Now, the coefficient value is higher (even higher than the coefficient value resulting from the case where only \nx_1\nx_1\n is used) and the null hypothesis can be rejected and the alternative hypothesis accepted.\n\n\n(f)\n\n\nThe results \ndo not contradict\n.\nWhat's happening here is a \ncollinearity\n phenomenon.\nAs suggested by the high correlation values and by the scatter plot (and, of course, from the generation of Y), we can linearly predict \nx_1\nx_1\n from \nx_2\nx_2\n (and vice-versa) with a substantial degree of accuracy. This is a clue of collinearity that is confirmed by the regression model. When both variables are combined in the same linear model, one of them loses explanatory power because the variance it explains is already being explained by the other variable. Accordingly, if considered individually, both variables lead to the rejection of the null hypothesis but, if considered together, one of the variables is dismissable.\n\n\nFinally, the values of the coefficients agree with what we know from the underlying model. If one writes \nX2\nX2\n in terms of \nX1\nX1\n, substitutes it in the model and adds both coefficients of \nX1\nX1\n, we get 2.15. This value is well within the confidence interval calculated in (d), namely [1.892; 3.001]. Likewise, for \nX2\nX2\n the expected value of the coefficient is 4.3 which is inside the [2.600; 4.614] interval calculated in (e).\n\n\n(g)\n\n\n# Add observation \nx1 = np.append(x1, 0.1)  # To x1\nx2 = np.append(x2, 0.8)  # To x2\ny = np.append(y, 6)  # To y\n\n# Add to dataframe (easier for outlier analysis plots)\nsample = {'x1': .1, 'x2': .8, 'y': 6}  # Create point\ndf = df.append(sample, ignore_index=True)  # Append sample to existing dataframe\n\n\n\n\nModels analysis\n\n\n# Model (c)\nX = pd.DataFrame({'x1':x1, 'x2':x2})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nprint(results.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.425\nModel:                            OLS   Adj. R-squared:                  0.414\nMethod:                 Least Squares   F-statistic:                     36.26\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           1.64e-12\nTime:                        09:50:56   Log-Likelihood:                -129.50\nNo. Observations:                 101   AIC:                             265.0\nDf Residuals:                      98   BIC:                             272.8\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.8697      0.168     11.111      0.000       1.536       2.204\nx1             1.2421      0.432      2.876      0.005       0.385       2.099\nx2             2.2711      0.698      3.254      0.002       0.886       3.656\n==============================================================================\nOmnibus:                        0.673   Durbin-Watson:                   1.803\nProb(Omnibus):                  0.714   Jarque-Bera (JB):                0.465\nSkew:                          -0.165   Prob(JB):                        0.792\nKurtosis:                       3.035   Cond. No.                         10.2\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Model (d)\nX = pd.DataFrame({'x1':x1})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nprint(results.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.363\nModel:                            OLS   Adj. R-squared:                  0.357\nMethod:                 Least Squares   F-statistic:                     56.46\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           2.60e-11\nTime:                        09:50:56   Log-Likelihood:                -134.68\nNo. Observations:                 101   AIC:                             273.4\nDf Residuals:                      99   BIC:                             278.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.9419      0.175     11.116      0.000       1.595       2.288\nx1             2.2829      0.304      7.514      0.000       1.680       2.886\n==============================================================================\nOmnibus:                       12.832   Durbin-Watson:                   1.773\nProb(Omnibus):                  0.002   Jarque-Bera (JB):               21.470\nSkew:                           0.522   Prob(JB):                     2.18e-05\nKurtosis:                       5.003   Cond. No.                         4.14\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Model (e)\nX = pd.DataFrame({'x2':x2})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nprint(results.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.377\nModel:                            OLS   Adj. R-squared:                  0.370\nMethod:                 Least Squares   F-statistic:                     59.84\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           8.79e-12\nTime:                        09:50:56   Log-Likelihood:                -133.59\nNo. Observations:                 101   AIC:                             271.2\nDf Residuals:                      99   BIC:                             276.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0962      0.154     13.604      0.000       1.790       2.402\nx2             3.7581      0.486      7.736      0.000       2.794       4.722\n==============================================================================\nOmnibus:                        1.784   Durbin-Watson:                   1.803\nProb(Omnibus):                  0.410   Jarque-Bera (JB):                1.826\nSkew:                          -0.299   Prob(JB):                        0.401\nKurtosis:                       2.726   Cond. No.                         5.68\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nEffect on models:\n\n \nModel (c)\n. R-squared decreased, which means that the predictive capacity of the model was reduced. The value of the regression coefficients changed: x1 coefficient decreased and x2 coefficient increased. As a consequence, x2 became the coefficient with higher value. The null hypothesis is now rejected in both variables.\n\n \nModel (d)\n. The only significant change was the reduction of R-squared.\n* \nModel (e)\n. The only significant change was a small increase of R-squared.\n\n\nOutliers and high leverage points analysis\n\n\nOutliers\n\n\nAn outlier is a point for which \ny_i\ny_i\n is far from the expected range predicted by the fit of the model. This raises the question of whether it is representative of the population.\n\n\nOutliers can be identified from a univariate, bivariate, or multivariate perspective based on the number of variables (characteristics) considered. We should use as many perspectives as possible, looking for a consistent pattern across them to identify outliers.\n\n\nCases that fall markedly outside the range of other observations will be seen as isolated points in the scatterplot. A drawback of the bivariate method in general is the potentially large number of scatterplots that arise as the number of variables increases. For 3 variables, it is only 3 graphs for all pairwise comparisons. But for 5 variables, it takes 10 graphs, and for 10 variables it takes 45 scatterplots! Since this analysis doesn't involve more than 2 variables, we can perform all pairwise comparisons.\n\n\nHigh leverage points\n\n\nWe just saw that outliers are observations for which the response \ny_i\ny_i\n is unusual given the predictor \nx_i\nx_i\n. In contrast, observations with high leverage have an unusual value for \nx_i\nx_i\n. In statistics, particularly in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations.\n\n\nIn a simple linear regression, high leverage observations are fairly easy to\nidentify, since we can simply look for observations for which the predictor\nvalue is outside of the normal range of the observations. With a single predictor, an extreme x value is simply one that is particularly high or low. \n\n\nBut in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor\u2019s values, but that is unusual in terms of the full set of predictors. With multiple predictors, extreme x values may be particularly high or low for one or more predictors, or may be \"unusual\" combinations of predictor values (e.g., with two predictors that are positively correlated, an unusual combination of predictor values might be a high value of one predictor paired with a low value of the other predictor).\n\n\n# Bivariate analysis (x1,x2)\nsample = df.iloc[-1:]  # To get the last observation\nother = df.iloc[:-1]  # To get all the observations but the last\nax = other.plot(kind='scatter',x='x1',y='x2', color='blue');  # Plot all observations but the last in blue\nsample.plot(ax=ax, kind='scatter',x='x1',y='x2', color='red');  # Plot last observation added in red\n\n\n\n\n\n\n\n\nThere is an unusual combination of predictor values, so it looks like an high leverage point.\n\n\n\n\nNote:\n we are not comparing predictors with responses nor evaluating if \ny_i\ny_i\n is far from the value predicted by the model. Therefore, it doesn't make sense to discuss if it's an outlier or not based on this plot.\n\n\n# Bivariate analysis (x1,y)\nsample = df.iloc[-1:]  # To get the last observation\nother = df.iloc[:-1]  # To get all the observations but the last\nax = other.plot(kind='scatter',x='x1',y='y', color='blue');  # Plot all observations but the last in blue\nsample.plot(ax=ax, kind='scatter',x='x1',y='y', color='red');  # Plot last observation added in red\n\n\n\n\n\n\n\n\nThe red point does not follow the trend, so it looks like an outlier.\n\n\nThe red point doesn't have an unusual \nx_1\nx_1\n value, so it doesn't look like an high leverage point.\n\n\n\n\n# Bivariate analysis (x2,y)\nsample = df.iloc[-1:]  # To get the last observation\nother = df.iloc[:-1]  # To get all the observations but the last\nax = other.plot(kind='scatter',x='x2',y='y', color='blue');  # Plot all observations but the last in blue\nsample.plot(ax=ax, kind='scatter',x='x2',y='y', color='red');  # Plot last observation added in red\n\n\n\n\n\n\n\n\nThe red point follows the trend, so it doesn't look like an outlier.\n\n\nThe red point has an extreme \nx_2\nx_2\n value, so it looks like an high leverage point.\n\n\n\n\nIn summary:\n the observation added influences significantly the model, in particular if we consider the regression model that includes \nx_1\nx_1\n and \nx_2\nx_2\n. In this case, \nx_2\nx_2\n passed from a neglected variable to a significant variable. This means that even being just 1  observation in 100, this observation reduced the existing phenomenon of collinearity. Also, the R-squared of the model reduced, which signifies a decrease in the model predicition capacity.\n\n\nAccording to the scatter plots, the observation added seems to be both an outlier and and an high leverage point. This conclusion can be taken from the visual observation of the observation added when confronted with the remaining observations. The added observations shows an unusual combination of predictor values, extreme predictor values and a substantial different behaviour when compared with other observations in several cases.\n\n\nReferences\n\n\n\n\nhttps://onlinecourses.science.psu.edu/stat501/node/337\n\n\nHair, J. F., Black, B., Babin, B., Anderson, R. E., & Tatham, R. L. (2010). \nMultivariate Data Analysis\n (7th\ned.). Upper Saddle River, NJ: Prentice-Hall",
            "title": "3.14"
        },
        {
            "location": "/sols/chapter3/exercise14/#exercise-314",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm # To use statsmodel\nimport statsmodels.formula.api as smf # To use statsmodel with R-style formulas\nfrom statsmodels.stats import outliers_influence\nfrom sklearn.linear_model import LinearRegression\n\n%matplotlib inline",
            "title": "Exercise 3.14"
        },
        {
            "location": "/sols/chapter3/exercise14/#a",
            "text": "np.random.seed(5) # Python and R random generators give different values\nx1 = np.random.uniform(size=100)\nx2 = 0.5 * x1 + np.random.normal(size=100) / 10\ny = 2 + 2 * x1 + 0.3 * x2 + np.random.normal(size=100)\n\n# http://stackoverflow.com/questions/22213298/creating-same-random-number-sequence-in-python-numpy-and-r  Model form:   \nY = \\beta_0 + \\beta_1  X_1 + \\beta_2  X_2 + \\epsilon = 2 + 2 X_1 + 0.3 X_2 + \\epsilon.  \nY = \\beta_0 + \\beta_1  X_1 + \\beta_2  X_2 + \\epsilon = 2 + 2 X_1 + 0.3 X_2 + \\epsilon.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise14/#b",
            "text": "# Get correlations\nnp.corrcoef(x1,x2)  array([[ 1.        ,  0.81936924],\n       [ 0.81936924,  1.        ]])  The correlation coefficient between  X_1 X_1  and  X_2 X_2  is  0.819 .  # Draw scatterplot\nplt.scatter(x1,x2);",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise14/#c",
            "text": "# Define data\nX = pd.DataFrame({'x1':x1, 'x2':x2})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\n# Create model\nmodel = sm.OLS(y, X)\n\n# Fit regression model\nresults = model.fit()\n\n# Print results\nresults.summary()   OLS Regression Results  \n   Dep. Variable:              y            R-squared:               0.444   \n   Model:                     OLS           Adj. R-squared:          0.433   \n   Method:               Least Squares      F-statistic:             38.74   \n   Date:               Fri, 08 Dec 2017     Prob (F-statistic):   4.31e-13   \n   Time:                   09:50:56         Log-Likelihood:        -123.67   \n   No. Observations:           100          AIC:                     253.3   \n   Df Residuals:                97          BIC:                     261.1   \n   Df Model:                     2                                           \n   Covariance Type:        nonrobust                                           \n             coef       std err        t        P>|t|    [0.025      0.975]     \n   const       1.8158       0.162      11.231    0.000       1.495       2.137   \n   x1          2.0758       0.488       4.257    0.000       1.108       3.044   \n   x2          0.7584       0.817       0.929    0.355      -0.862       2.379     \n   Omnibus:          0.718     Durbin-Watson:           1.960   \n   Prob(Omnibus):    0.698     Jarque-Bera (JB):        0.574   \n   Skew:            -0.185     Prob(JB):                0.750   \n   Kurtosis:         2.981     Cond. No.                 12.5    According to the results we have:   \\hat{\\beta_0} = 1.8158 \\hat{\\beta_0} = 1.8158    \\hat{\\beta_1} = 2.0758 \\hat{\\beta_1} = 2.0758    \\hat{\\beta_2} = 0.7584 \\hat{\\beta_2} = 0.7584     These values are estimators of the true coefficients, which have the following values:   \\beta_0 = 2 \\beta_0 = 2  \\beta_1 = 2 \\beta_1 = 2  \\beta_2 = 0.3 \\beta_2 = 0.3   As we can see, there are some differences between the coefficients, especially in the case of  \\hat{\\beta_2} \\hat{\\beta_2}  (0.7584 vs. 0.3).     H_0 : \\beta_1 = 0 H_0 : \\beta_1 = 0   . The rejection of the null hypothesis depends on the t-statistic (t). In the case of  \\beta_1 \\beta_1 , this value is high. If the t-statistics is high, the p-value will be low. The p-value is the probability of observing any value equal to |t| or larger than (P>|t|), assuming that the coefficient is zero. Thus, if the p-value is low we should  reject the null hypothesis and accept the alternative hypothesis .    H_0 : \\beta_2 = 0 H_0 : \\beta_2 = 0   . In this case the t-statistic is low (0.929) and the p-value is high (0.355). Accordingly, the  null hypothesis can't be rejected .  Alternative solution  A different way to approximate the equation using R-style formula in StatsModel.  # Define data\ndf = pd.DataFrame({'x1':x1, 'x2':x2, 'y':y})  # We don't need to add constant because we will use formulas\n\n# Create model\nmod = smf.ols(formula='y ~ x1 + x2', data=df)  # R-style command\n\n# Fit model\nres = mod.fit()\n\n# Print results\nprint (res.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.444\nModel:                            OLS   Adj. R-squared:                  0.433\nMethod:                 Least Squares   F-statistic:                     38.74\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           4.31e-13\nTime:                        09:50:56   Log-Likelihood:                -123.67\nNo. Observations:                 100   AIC:                             253.3\nDf Residuals:                      97   BIC:                             261.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.8158      0.162     11.231      0.000       1.495       2.137\nx1             2.0758      0.488      4.257      0.000       1.108       3.044\nx2             0.7584      0.817      0.929      0.355      -0.862       2.379\n==============================================================================\nOmnibus:                        0.718   Durbin-Watson:                   1.960\nProb(Omnibus):                  0.698   Jarque-Bera (JB):                0.574\nSkew:                          -0.185   Prob(JB):                        0.750\nKurtosis:                       2.981   Cond. No.                         12.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  Alternative solution  This is an alternative solution using Scikit instead of StatsModel.  # Create model\nlr = LinearRegression()\n\n# Fit model\nmod = lr.fit(X,y)\n\n# Get coefficients\nmod.coef_  array([ 0.        ,  2.0758066 ,  0.75840009])",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise14/#d",
            "text": "X = pd.DataFrame({'x1':x1})\nX = sm.add_constant(X)\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.439\nModel:                            OLS   Adj. R-squared:                  0.433\nMethod:                 Least Squares   F-statistic:                     76.72\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           5.93e-14\nTime:                        09:50:56   Log-Likelihood:                -124.11\nNo. Observations:                 100   AIC:                             252.2\nDf Residuals:                      98   BIC:                             257.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.8229      0.161     11.295      0.000       1.503       2.143\nx1             2.4468      0.279      8.759      0.000       1.892       3.001\n==============================================================================\nOmnibus:                        0.357   Durbin-Watson:                   1.986\nProb(Omnibus):                  0.836   Jarque-Bera (JB):                0.272\nSkew:                          -0.127   Prob(JB):                        0.873\nKurtosis:                       2.963   Cond. No.                         4.17\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  The coefficient value  increased  to 2.4468 and the  null hypothesis can be rejected and the alternative hypothesis accepted  because p-value is zero. It can be said that this results are in line with our expectations from (c).",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter3/exercise14/#e",
            "text": "X = pd.DataFrame({'x2':x2})\nX = sm.add_constant(X)\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.340\nModel:                            OLS   Adj. R-squared:                  0.333\nMethod:                 Least Squares   F-statistic:                     50.53\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           1.92e-10\nTime:                        09:50:56   Log-Likelihood:                -132.23\nNo. Observations:                 100   AIC:                             268.5\nDf Residuals:                      98   BIC:                             273.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.1250      0.157     13.572      0.000       1.814       2.436\nx2             3.6070      0.507      7.108      0.000       2.600       4.614\n==============================================================================\nOmnibus:                        1.537   Durbin-Watson:                   1.828\nProb(Omnibus):                  0.464   Jarque-Bera (JB):                1.597\nSkew:                          -0.272   Prob(JB):                        0.450\nKurtosis:                       2.704   Cond. No.                         5.89\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  The coefficient value  increased  to 3.6070 and the  null hypothesis can be rejected and the alternative hypothesis accepted  because p-value is zero.  These results are significantly different from (c). In (c) the coefficient associated with  x_2 x_2  had a lower value and the p-value suggested that the null hypothesis couldn't be rejected (coefficient value could be zero). Now, the coefficient value is higher (even higher than the coefficient value resulting from the case where only  x_1 x_1  is used) and the null hypothesis can be rejected and the alternative hypothesis accepted.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter3/exercise14/#f",
            "text": "The results  do not contradict .\nWhat's happening here is a  collinearity  phenomenon.\nAs suggested by the high correlation values and by the scatter plot (and, of course, from the generation of Y), we can linearly predict  x_1 x_1  from  x_2 x_2  (and vice-versa) with a substantial degree of accuracy. This is a clue of collinearity that is confirmed by the regression model. When both variables are combined in the same linear model, one of them loses explanatory power because the variance it explains is already being explained by the other variable. Accordingly, if considered individually, both variables lead to the rejection of the null hypothesis but, if considered together, one of the variables is dismissable.  Finally, the values of the coefficients agree with what we know from the underlying model. If one writes  X2 X2  in terms of  X1 X1 , substitutes it in the model and adds both coefficients of  X1 X1 , we get 2.15. This value is well within the confidence interval calculated in (d), namely [1.892; 3.001]. Likewise, for  X2 X2  the expected value of the coefficient is 4.3 which is inside the [2.600; 4.614] interval calculated in (e).",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter3/exercise14/#g",
            "text": "# Add observation \nx1 = np.append(x1, 0.1)  # To x1\nx2 = np.append(x2, 0.8)  # To x2\ny = np.append(y, 6)  # To y\n\n# Add to dataframe (easier for outlier analysis plots)\nsample = {'x1': .1, 'x2': .8, 'y': 6}  # Create point\ndf = df.append(sample, ignore_index=True)  # Append sample to existing dataframe",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter3/exercise14/#models-analysis",
            "text": "# Model (c)\nX = pd.DataFrame({'x1':x1, 'x2':x2})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nprint(results.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.425\nModel:                            OLS   Adj. R-squared:                  0.414\nMethod:                 Least Squares   F-statistic:                     36.26\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           1.64e-12\nTime:                        09:50:56   Log-Likelihood:                -129.50\nNo. Observations:                 101   AIC:                             265.0\nDf Residuals:                      98   BIC:                             272.8\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.8697      0.168     11.111      0.000       1.536       2.204\nx1             1.2421      0.432      2.876      0.005       0.385       2.099\nx2             2.2711      0.698      3.254      0.002       0.886       3.656\n==============================================================================\nOmnibus:                        0.673   Durbin-Watson:                   1.803\nProb(Omnibus):                  0.714   Jarque-Bera (JB):                0.465\nSkew:                          -0.165   Prob(JB):                        0.792\nKurtosis:                       3.035   Cond. No.                         10.2\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  # Model (d)\nX = pd.DataFrame({'x1':x1})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nprint(results.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.363\nModel:                            OLS   Adj. R-squared:                  0.357\nMethod:                 Least Squares   F-statistic:                     56.46\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           2.60e-11\nTime:                        09:50:56   Log-Likelihood:                -134.68\nNo. Observations:                 101   AIC:                             273.4\nDf Residuals:                      99   BIC:                             278.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.9419      0.175     11.116      0.000       1.595       2.288\nx1             2.2829      0.304      7.514      0.000       1.680       2.886\n==============================================================================\nOmnibus:                       12.832   Durbin-Watson:                   1.773\nProb(Omnibus):                  0.002   Jarque-Bera (JB):               21.470\nSkew:                           0.522   Prob(JB):                     2.18e-05\nKurtosis:                       5.003   Cond. No.                         4.14\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  # Model (e)\nX = pd.DataFrame({'x2':x2})\nX = sm.add_constant(X)  # No constant is added by the model unless we're using formulas, so we have to add it\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nprint(results.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.377\nModel:                            OLS   Adj. R-squared:                  0.370\nMethod:                 Least Squares   F-statistic:                     59.84\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           8.79e-12\nTime:                        09:50:56   Log-Likelihood:                -133.59\nNo. Observations:                 101   AIC:                             271.2\nDf Residuals:                      99   BIC:                             276.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0962      0.154     13.604      0.000       1.790       2.402\nx2             3.7581      0.486      7.736      0.000       2.794       4.722\n==============================================================================\nOmnibus:                        1.784   Durbin-Watson:                   1.803\nProb(Omnibus):                  0.410   Jarque-Bera (JB):                1.826\nSkew:                          -0.299   Prob(JB):                        0.401\nKurtosis:                       2.726   Cond. No.                         5.68\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  Effect on models:   Model (c) . R-squared decreased, which means that the predictive capacity of the model was reduced. The value of the regression coefficients changed: x1 coefficient decreased and x2 coefficient increased. As a consequence, x2 became the coefficient with higher value. The null hypothesis is now rejected in both variables.   Model (d) . The only significant change was the reduction of R-squared.\n*  Model (e) . The only significant change was a small increase of R-squared.",
            "title": "Models analysis"
        },
        {
            "location": "/sols/chapter3/exercise14/#outliers-and-high-leverage-points-analysis",
            "text": "Outliers  An outlier is a point for which  y_i y_i  is far from the expected range predicted by the fit of the model. This raises the question of whether it is representative of the population.  Outliers can be identified from a univariate, bivariate, or multivariate perspective based on the number of variables (characteristics) considered. We should use as many perspectives as possible, looking for a consistent pattern across them to identify outliers.  Cases that fall markedly outside the range of other observations will be seen as isolated points in the scatterplot. A drawback of the bivariate method in general is the potentially large number of scatterplots that arise as the number of variables increases. For 3 variables, it is only 3 graphs for all pairwise comparisons. But for 5 variables, it takes 10 graphs, and for 10 variables it takes 45 scatterplots! Since this analysis doesn't involve more than 2 variables, we can perform all pairwise comparisons.  High leverage points  We just saw that outliers are observations for which the response  y_i y_i  is unusual given the predictor  x_i x_i . In contrast, observations with high leverage have an unusual value for  x_i x_i . In statistics, particularly in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations.  In a simple linear regression, high leverage observations are fairly easy to\nidentify, since we can simply look for observations for which the predictor\nvalue is outside of the normal range of the observations. With a single predictor, an extreme x value is simply one that is particularly high or low.   But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor\u2019s values, but that is unusual in terms of the full set of predictors. With multiple predictors, extreme x values may be particularly high or low for one or more predictors, or may be \"unusual\" combinations of predictor values (e.g., with two predictors that are positively correlated, an unusual combination of predictor values might be a high value of one predictor paired with a low value of the other predictor).  # Bivariate analysis (x1,x2)\nsample = df.iloc[-1:]  # To get the last observation\nother = df.iloc[:-1]  # To get all the observations but the last\nax = other.plot(kind='scatter',x='x1',y='x2', color='blue');  # Plot all observations but the last in blue\nsample.plot(ax=ax, kind='scatter',x='x1',y='x2', color='red');  # Plot last observation added in red    There is an unusual combination of predictor values, so it looks like an high leverage point.   Note:  we are not comparing predictors with responses nor evaluating if  y_i y_i  is far from the value predicted by the model. Therefore, it doesn't make sense to discuss if it's an outlier or not based on this plot.  # Bivariate analysis (x1,y)\nsample = df.iloc[-1:]  # To get the last observation\nother = df.iloc[:-1]  # To get all the observations but the last\nax = other.plot(kind='scatter',x='x1',y='y', color='blue');  # Plot all observations but the last in blue\nsample.plot(ax=ax, kind='scatter',x='x1',y='y', color='red');  # Plot last observation added in red    The red point does not follow the trend, so it looks like an outlier.  The red point doesn't have an unusual  x_1 x_1  value, so it doesn't look like an high leverage point.   # Bivariate analysis (x2,y)\nsample = df.iloc[-1:]  # To get the last observation\nother = df.iloc[:-1]  # To get all the observations but the last\nax = other.plot(kind='scatter',x='x2',y='y', color='blue');  # Plot all observations but the last in blue\nsample.plot(ax=ax, kind='scatter',x='x2',y='y', color='red');  # Plot last observation added in red    The red point follows the trend, so it doesn't look like an outlier.  The red point has an extreme  x_2 x_2  value, so it looks like an high leverage point.   In summary:  the observation added influences significantly the model, in particular if we consider the regression model that includes  x_1 x_1  and  x_2 x_2 . In this case,  x_2 x_2  passed from a neglected variable to a significant variable. This means that even being just 1  observation in 100, this observation reduced the existing phenomenon of collinearity. Also, the R-squared of the model reduced, which signifies a decrease in the model predicition capacity.  According to the scatter plots, the observation added seems to be both an outlier and and an high leverage point. This conclusion can be taken from the visual observation of the observation added when confronted with the remaining observations. The added observations shows an unusual combination of predictor values, extreme predictor values and a substantial different behaviour when compared with other observations in several cases.",
            "title": "Outliers and high leverage points analysis"
        },
        {
            "location": "/sols/chapter3/exercise14/#references",
            "text": "https://onlinecourses.science.psu.edu/stat501/node/337  Hair, J. F., Black, B., Babin, B., Anderson, R. E., & Tatham, R. L. (2010).  Multivariate Data Analysis  (7th\ned.). Upper Saddle River, NJ: Prentice-Hall",
            "title": "References"
        },
        {
            "location": "/sols/chapter3/exercise15/",
            "text": "Exercise 3.15\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf #statsmodels is a Python module for statistics\nimport statsmodels.api as sm\nfrom sklearn.datasets import load_boston\n\n%matplotlib inline\n\n\n\n\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['MEDV'] = pd.Series(boston.target)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nMEDV\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\nprint(\"{:>9} {:>22} {:>24}\".format(\"predictor\", \"coef\",\"pvalue\"))\ncoefs = {}\n\npredictors = [c for c in list(df) if c not in [\"CRIM\"]]\nfor predictor in predictors:\n    model = 'CRIM ~ ' + predictor\n    res = smf.ols(formula = model, data=df).fit()\n    # http://www.statsmodels.org/devel/generated/statsmodels.regression.linear_model.RegressionResults.html\n    print(\"{:>9} {:>22} {:>24}\".format(predictor, res.params[predictor],res.pvalues[predictor]))\n    coefs[predictor] = [res.params[predictor]] \n\n\n\n\n\n\npredictor                   coef                   pvalue\n       ZN   -0.07352128504760275    6.151721643267655e-06\n    INDUS     0.5068466125328721    2.444137454620807e-21\n     CHAS    -1.8715451282984525      0.21434357527851233\n      NOX       30.9752586128881    9.159490025915888e-23\n       RM    -2.6910453263732346    5.838093667798685e-07\n      AGE    0.10713083068208369   4.2590641745370265e-16\n      DIS     -1.542831118235415   1.2688320361261509e-18\n      RAD     0.6141366715916436   1.6206052887449367e-55\n      TAX     0.0295625570653893    9.759521193159848e-47\n  PTRATIO     1.1446126207906333   3.8751218902071097e-11\n        B   -0.03553454597446588   1.4320876785176315e-18\n    LSTAT     0.5444063736854577    7.124777983462517e-27\n     MEDV    -0.3606473433413291   2.0835501108140565e-19\n\n\n\nThe list above indicates that every predictor except CHAS has a statistically significant association with CRIM at the 1% level. We now plot every predictor against the response with the regression line from the fit. \n\n\nplt.figure(figsize=(20, 20))\n\nfor i, predictor in enumerate(predictors):\n    model = 'CRIM ~ ' + predictor\n    res = smf.ols(formula = model, data=df).fit()\n    plt.subplot(5,3,i+1)\n    plt.xlabel(predictor)\n    plt.ylabel(\"CRIM\")\n    plt.scatter(df[predictor], df['CRIM'])\n    plt.plot(df[predictor], res.fittedvalues, color='red')\n\n\n\n\n\n\n(b)\n\n\nall_columns = \"+\".join([c for c in list(df) if c not in [\"CRIM\"]])\nmodel = \" CRIM ~ \" + all_columns\nres = smf.ols(formula = model, data=df).fit()\nprint(res.summary())\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   CRIM   R-squared:                       0.448\nModel:                            OLS   Adj. R-squared:                  0.434\nMethod:                 Least Squares   F-statistic:                     30.73\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           2.04e-55\nTime:                        09:51:22   Log-Likelihood:                -1655.7\nNo. Observations:                 506   AIC:                             3339.\nDf Residuals:                     492   BIC:                             3399.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     17.4184      7.270      2.396      0.017       3.135      31.702\nZN             0.0449      0.019      2.386      0.017       0.008       0.082\nINDUS         -0.0616      0.084     -0.735      0.463      -0.226       0.103\nCHAS          -0.7414      1.186     -0.625      0.532      -3.071       1.588\nNOX          -10.6455      5.301     -2.008      0.045     -21.061      -0.230\nRM             0.3811      0.616      0.619      0.536      -0.829       1.591\nAGE            0.0020      0.018      0.112      0.911      -0.033       0.037\nDIS           -0.9950      0.283     -3.514      0.000      -1.551      -0.439\nRAD            0.5888      0.088      6.656      0.000       0.415       0.763\nTAX           -0.0037      0.005     -0.723      0.470      -0.014       0.006\nPTRATIO       -0.2787      0.187     -1.488      0.137      -0.647       0.089\nB             -0.0069      0.004     -1.857      0.064      -0.014       0.000\nLSTAT          0.1213      0.076      1.594      0.112      -0.028       0.271\nMEDV          -0.1992      0.061     -3.276      0.001      -0.319      -0.080\n==============================================================================\nOmnibus:                      662.271   Durbin-Watson:                   1.515\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            82701.666\nSkew:                           6.544   Prob(JB):                         0.00\nKurtosis:                      64.248   Cond. No.                     1.58e+04\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.58e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nInspecting the t-statistics and p-values from the table above indicates that we can reject the null-hypothesis at the 1% level for every predictor except DIS and RAD.\n\n\n(c)\n\n\nfor pred in coefs:\n    coefs[pred].append(res.params[pred])\n\n\n\n\nplt.scatter([coefs[pred][0] for pred in coefs], [coefs[pred][1] for pred in coefs])\nplt.plot([-5,35],[-5,35]) # plot y=x\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\nplt.scatter([coefs[pred][0] for pred in coefs if pred != \"NOX\"], [coefs[pred][1] for pred in coefs if pred != \"NOX\"])\nplt.plot([-3,1], [-3,1]) # plot y=x\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\nfor pred in coefs:\n    print(\"{:>9} {:>22} {:>24}\".format(pred, coefs[pred][0], coefs[pred][1]))\n\n\n\n\n       ZN   -0.07352128504760275      0.04491938833833456\n    INDUS     0.5068466125328721     -0.06157595914315758\n     CHAS    -1.8715451282984525      -0.7414350725371193\n      NOX       30.9752586128881      -10.645499846398824\n       RM    -2.6910453263732346       0.3810702287184484\n      AGE    0.10713083068208369    0.0020113635247448122\n      DIS     -1.542831118235415       -0.994991753906636\n      RAD     0.6141366715916436       0.5888381693758202\n      TAX     0.0295625570653893   -0.0037457234762569955\n  PTRATIO     1.1446126207906333      -0.2787310489008303\n        B   -0.03553454597446588   -0.0068551485297145675\n    LSTAT     0.5444063736854577      0.12126930458422257\n     MEDV    -0.3606473433413291      -0.1992178026131782\n\n\n\nAs can be seen from the two plots and table above there's a reasonable correlation between the coefficients of the individual and multiple regressions, except for a couple of outliers (namely, NOX, and perhaps PTRATIO if we try to \"fit\" the line y=x).\n\n\nHowever, since we can only reject the null hypothesis for DIS and RAD, the other values are not very meaningful. We would also expect them to differ significantly between the individual and multiple regression case, since in the former the coefficient is the average change in the response from a unit change in the predictor completely ignoring the other predictors. In the latter case, the coefficient is the average change in the response from a unit change in the predictor while holding the other predictor fixed. Because of possible correlations, non-linearities and collinearities between the predictors, there is no expectancy that, in general, the coefficients in each case will be of the same magnitude or sign.\n\n\n(d)\n\n\nprint(\"{:>9} {:>22} {:>24}\".format(\"predictor\", \"coef\",\"pvalue\"))\ncoefs = {}\n\nplt.figure(figsize=(20, 20))\n\nfor i, predictor in enumerate(predictors):\n    model = 'CRIM ~ ' + predictor + \" + np.power(\" + predictor + \", 2) + np.power(\" + predictor + \", 3)\"\n    res = smf.ols(formula = model, data=df).fit()\n    plt.subplot(5,3,i+1)\n    plt.xlabel(predictor)\n    plt.ylabel(\"CRIM\")\n    plt.scatter(df[predictor], df['CRIM'])\n    x = np.linspace(min(df[predictor]),max(df[predictor]), 100)\n    y = res.params[0] + x*res.params[1]+ res.params[2]*(x**2)+ res.params[3]*(x**3)\n    plt.plot(x, y, color='red')    \n\n\n\n\npredictor                   coef                   pvalue\n\n\n\n\n\nprint(\"{:>13} {:>22} {:>22} {:>22} {:>22} {:>22}\".format(\"Pvalues for\", \"beta_0\", \"beta_1\", \"beta_2\", \"beta_3\", \"f_pvalue\"))\n\nfor predictor in predictors:\n    model = 'CRIM ~ ' + predictor + \" + np.power(\" + predictor + \", 2) + np.power(\" + predictor + \", 3)\"\n    res = smf.ols(formula = model, data=df).fit()\n    # http://www.statsmodels.org/devel/generated/statsmodels.regression.linear_model.RegressionResults.html\n    print(\"{:>13} {:>22} {:>22} {:>22} {:>22} {:>22}\".format(predictor, res.pvalues[0], res.pvalues[1], res.pvalues[2], res.pvalues[3], res.f_pvalue))\n    coefs[predictor] = [res.params[predictor]] \n\n\n\n\n  Pvalues for                 beta_0                 beta_1                 beta_2                 beta_3               f_pvalue\n           ZN  7.007549102153049e-26   0.002759025816137138    0.09562861334020208    0.23222420001929456  1.493835486494624e-06\n        INDUS    0.02127779341724847  5.996850336550802e-05  4.530067332031857e-10 1.7044411827492229e-12  3.883757105301397e-32\n         CHAS 1.8392584982986124e-19     0.2143435752785134     0.2143435752785135     0.2143435752785135    0.21434357527854317\n          NOX 2.5889952588127288e-11  5.832574000017562e-13  1.522887409265701e-14 1.5877781649655969e-15  1.944006533283078e-37\n           RM    0.08318387284841824    0.21659351269645624     0.3727454652123774     0.5206758617415794  9.064938181874174e-08\n          AGE    0.35608818782287965    0.14200846484879873    0.04742487317735903   0.006784649759092411 1.7617378955702614e-20\n          DIS  2.625993995501171e-30  7.973990285059572e-18   5.55230636339025e-12 1.1612920278009457e-08   6.20393032552023e-35\n          RAD     0.7687397958579762     0.6248690273355282     0.6147821175450365    0.48510593419963344 1.4667061463067613e-54\n          TAX    0.10749505285358406    0.11270703155044363    0.14066729321605811    0.24747620662298378 3.6866247874075365e-49\n      PTRATIO    0.00263349568053505  0.0032336441833622862   0.004384600558546921   0.006674970321795337  5.989427223419224e-13\n            B  4.087085804886006e-14    0.13513295148754378     0.4474840389744088     0.5086733122770439  7.828704859359701e-17\n        LSTAT     0.5940911637166147     0.3752691902431662    0.07929438940026551    0.15533172089582745  4.128065320901066e-26\n         MEDV 1.3488354131610002e-45  5.306129587965229e-28 4.8367172078814855e-18 1.3247686426755871e-12 2.6524355817735603e-58\n\n\n\nFrom the plots and table above, we can find evidence of a non-linear association, cubic type, between INDUS, NOX, AGE, DIS, PTRATIO and MEDV. In general, to get a sense if a non-linear association is present, we can plot the residuals of the linear fit against the fitted values and see if there is a non-linear trend.",
            "title": "3.15"
        },
        {
            "location": "/sols/chapter3/exercise15/#exercise-315",
            "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf #statsmodels is a Python module for statistics\nimport statsmodels.api as sm\nfrom sklearn.datasets import load_boston\n\n%matplotlib inline  boston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['MEDV'] = pd.Series(boston.target)  df.head()   \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       MEDV \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2",
            "title": "Exercise 3.15"
        },
        {
            "location": "/sols/chapter3/exercise15/#a",
            "text": "print(\"{:>9} {:>22} {:>24}\".format(\"predictor\", \"coef\",\"pvalue\"))\ncoefs = {}\n\npredictors = [c for c in list(df) if c not in [\"CRIM\"]]\nfor predictor in predictors:\n    model = 'CRIM ~ ' + predictor\n    res = smf.ols(formula = model, data=df).fit()\n    # http://www.statsmodels.org/devel/generated/statsmodels.regression.linear_model.RegressionResults.html\n    print(\"{:>9} {:>22} {:>24}\".format(predictor, res.params[predictor],res.pvalues[predictor]))\n    coefs[predictor] = [res.params[predictor]]   predictor                   coef                   pvalue\n       ZN   -0.07352128504760275    6.151721643267655e-06\n    INDUS     0.5068466125328721    2.444137454620807e-21\n     CHAS    -1.8715451282984525      0.21434357527851233\n      NOX       30.9752586128881    9.159490025915888e-23\n       RM    -2.6910453263732346    5.838093667798685e-07\n      AGE    0.10713083068208369   4.2590641745370265e-16\n      DIS     -1.542831118235415   1.2688320361261509e-18\n      RAD     0.6141366715916436   1.6206052887449367e-55\n      TAX     0.0295625570653893    9.759521193159848e-47\n  PTRATIO     1.1446126207906333   3.8751218902071097e-11\n        B   -0.03553454597446588   1.4320876785176315e-18\n    LSTAT     0.5444063736854577    7.124777983462517e-27\n     MEDV    -0.3606473433413291   2.0835501108140565e-19  The list above indicates that every predictor except CHAS has a statistically significant association with CRIM at the 1% level. We now plot every predictor against the response with the regression line from the fit.   plt.figure(figsize=(20, 20))\n\nfor i, predictor in enumerate(predictors):\n    model = 'CRIM ~ ' + predictor\n    res = smf.ols(formula = model, data=df).fit()\n    plt.subplot(5,3,i+1)\n    plt.xlabel(predictor)\n    plt.ylabel(\"CRIM\")\n    plt.scatter(df[predictor], df['CRIM'])\n    plt.plot(df[predictor], res.fittedvalues, color='red')",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter3/exercise15/#b",
            "text": "all_columns = \"+\".join([c for c in list(df) if c not in [\"CRIM\"]])\nmodel = \" CRIM ~ \" + all_columns\nres = smf.ols(formula = model, data=df).fit()\nprint(res.summary())                              OLS Regression Results                            \n==============================================================================\nDep. Variable:                   CRIM   R-squared:                       0.448\nModel:                            OLS   Adj. R-squared:                  0.434\nMethod:                 Least Squares   F-statistic:                     30.73\nDate:                Fri, 08 Dec 2017   Prob (F-statistic):           2.04e-55\nTime:                        09:51:22   Log-Likelihood:                -1655.7\nNo. Observations:                 506   AIC:                             3339.\nDf Residuals:                     492   BIC:                             3399.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     17.4184      7.270      2.396      0.017       3.135      31.702\nZN             0.0449      0.019      2.386      0.017       0.008       0.082\nINDUS         -0.0616      0.084     -0.735      0.463      -0.226       0.103\nCHAS          -0.7414      1.186     -0.625      0.532      -3.071       1.588\nNOX          -10.6455      5.301     -2.008      0.045     -21.061      -0.230\nRM             0.3811      0.616      0.619      0.536      -0.829       1.591\nAGE            0.0020      0.018      0.112      0.911      -0.033       0.037\nDIS           -0.9950      0.283     -3.514      0.000      -1.551      -0.439\nRAD            0.5888      0.088      6.656      0.000       0.415       0.763\nTAX           -0.0037      0.005     -0.723      0.470      -0.014       0.006\nPTRATIO       -0.2787      0.187     -1.488      0.137      -0.647       0.089\nB             -0.0069      0.004     -1.857      0.064      -0.014       0.000\nLSTAT          0.1213      0.076      1.594      0.112      -0.028       0.271\nMEDV          -0.1992      0.061     -3.276      0.001      -0.319      -0.080\n==============================================================================\nOmnibus:                      662.271   Durbin-Watson:                   1.515\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            82701.666\nSkew:                           6.544   Prob(JB):                         0.00\nKurtosis:                      64.248   Cond. No.                     1.58e+04\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.58e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.  Inspecting the t-statistics and p-values from the table above indicates that we can reject the null-hypothesis at the 1% level for every predictor except DIS and RAD.",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter3/exercise15/#c",
            "text": "for pred in coefs:\n    coefs[pred].append(res.params[pred])  plt.scatter([coefs[pred][0] for pred in coefs], [coefs[pred][1] for pred in coefs])\nplt.plot([-5,35],[-5,35]) # plot y=x\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()   plt.scatter([coefs[pred][0] for pred in coefs if pred != \"NOX\"], [coefs[pred][1] for pred in coefs if pred != \"NOX\"])\nplt.plot([-3,1], [-3,1]) # plot y=x\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()   for pred in coefs:\n    print(\"{:>9} {:>22} {:>24}\".format(pred, coefs[pred][0], coefs[pred][1]))         ZN   -0.07352128504760275      0.04491938833833456\n    INDUS     0.5068466125328721     -0.06157595914315758\n     CHAS    -1.8715451282984525      -0.7414350725371193\n      NOX       30.9752586128881      -10.645499846398824\n       RM    -2.6910453263732346       0.3810702287184484\n      AGE    0.10713083068208369    0.0020113635247448122\n      DIS     -1.542831118235415       -0.994991753906636\n      RAD     0.6141366715916436       0.5888381693758202\n      TAX     0.0295625570653893   -0.0037457234762569955\n  PTRATIO     1.1446126207906333      -0.2787310489008303\n        B   -0.03553454597446588   -0.0068551485297145675\n    LSTAT     0.5444063736854577      0.12126930458422257\n     MEDV    -0.3606473433413291      -0.1992178026131782  As can be seen from the two plots and table above there's a reasonable correlation between the coefficients of the individual and multiple regressions, except for a couple of outliers (namely, NOX, and perhaps PTRATIO if we try to \"fit\" the line y=x).  However, since we can only reject the null hypothesis for DIS and RAD, the other values are not very meaningful. We would also expect them to differ significantly between the individual and multiple regression case, since in the former the coefficient is the average change in the response from a unit change in the predictor completely ignoring the other predictors. In the latter case, the coefficient is the average change in the response from a unit change in the predictor while holding the other predictor fixed. Because of possible correlations, non-linearities and collinearities between the predictors, there is no expectancy that, in general, the coefficients in each case will be of the same magnitude or sign.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter3/exercise15/#d",
            "text": "print(\"{:>9} {:>22} {:>24}\".format(\"predictor\", \"coef\",\"pvalue\"))\ncoefs = {}\n\nplt.figure(figsize=(20, 20))\n\nfor i, predictor in enumerate(predictors):\n    model = 'CRIM ~ ' + predictor + \" + np.power(\" + predictor + \", 2) + np.power(\" + predictor + \", 3)\"\n    res = smf.ols(formula = model, data=df).fit()\n    plt.subplot(5,3,i+1)\n    plt.xlabel(predictor)\n    plt.ylabel(\"CRIM\")\n    plt.scatter(df[predictor], df['CRIM'])\n    x = np.linspace(min(df[predictor]),max(df[predictor]), 100)\n    y = res.params[0] + x*res.params[1]+ res.params[2]*(x**2)+ res.params[3]*(x**3)\n    plt.plot(x, y, color='red')      predictor                   coef                   pvalue   print(\"{:>13} {:>22} {:>22} {:>22} {:>22} {:>22}\".format(\"Pvalues for\", \"beta_0\", \"beta_1\", \"beta_2\", \"beta_3\", \"f_pvalue\"))\n\nfor predictor in predictors:\n    model = 'CRIM ~ ' + predictor + \" + np.power(\" + predictor + \", 2) + np.power(\" + predictor + \", 3)\"\n    res = smf.ols(formula = model, data=df).fit()\n    # http://www.statsmodels.org/devel/generated/statsmodels.regression.linear_model.RegressionResults.html\n    print(\"{:>13} {:>22} {:>22} {:>22} {:>22} {:>22}\".format(predictor, res.pvalues[0], res.pvalues[1], res.pvalues[2], res.pvalues[3], res.f_pvalue))\n    coefs[predictor] = [res.params[predictor]]     Pvalues for                 beta_0                 beta_1                 beta_2                 beta_3               f_pvalue\n           ZN  7.007549102153049e-26   0.002759025816137138    0.09562861334020208    0.23222420001929456  1.493835486494624e-06\n        INDUS    0.02127779341724847  5.996850336550802e-05  4.530067332031857e-10 1.7044411827492229e-12  3.883757105301397e-32\n         CHAS 1.8392584982986124e-19     0.2143435752785134     0.2143435752785135     0.2143435752785135    0.21434357527854317\n          NOX 2.5889952588127288e-11  5.832574000017562e-13  1.522887409265701e-14 1.5877781649655969e-15  1.944006533283078e-37\n           RM    0.08318387284841824    0.21659351269645624     0.3727454652123774     0.5206758617415794  9.064938181874174e-08\n          AGE    0.35608818782287965    0.14200846484879873    0.04742487317735903   0.006784649759092411 1.7617378955702614e-20\n          DIS  2.625993995501171e-30  7.973990285059572e-18   5.55230636339025e-12 1.1612920278009457e-08   6.20393032552023e-35\n          RAD     0.7687397958579762     0.6248690273355282     0.6147821175450365    0.48510593419963344 1.4667061463067613e-54\n          TAX    0.10749505285358406    0.11270703155044363    0.14066729321605811    0.24747620662298378 3.6866247874075365e-49\n      PTRATIO    0.00263349568053505  0.0032336441833622862   0.004384600558546921   0.006674970321795337  5.989427223419224e-13\n            B  4.087085804886006e-14    0.13513295148754378     0.4474840389744088     0.5086733122770439  7.828704859359701e-17\n        LSTAT     0.5940911637166147     0.3752691902431662    0.07929438940026551    0.15533172089582745  4.128065320901066e-26\n         MEDV 1.3488354131610002e-45  5.306129587965229e-28 4.8367172078814855e-18 1.3247686426755871e-12 2.6524355817735603e-58  From the plots and table above, we can find evidence of a non-linear association, cubic type, between INDUS, NOX, AGE, DIS, PTRATIO and MEDV. In general, to get a sense if a non-linear association is present, we can plot the residuals of the linear fit against the fitted values and see if there is a non-linear trend.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter4/exercise1/",
            "text": "Exercise 4.1\n\n\nWriting \nY=\\beta_0 + \\beta_1 X\nY=\\beta_0 + \\beta_1 X\n for clarity, we start with\n\n\n\n\n\\begin{align}\nP(X) & = \\frac{e^Y}{1+e^Y}, \n\\end{align}\n\n\n\\begin{align}\nP(X) & = \\frac{e^Y}{1+e^Y}, \n\\end{align}\n\n\n\n\nand \"using a little bit of algebra\", we get\n\n\n\n\n\\begin{align}\n\\frac{P(X)}{1-P(X)} & = \\frac{e^Y}{1+e^Y} \\left( 1 - \\frac{e^Y}{1+e^Y}  \\right)^{-1} = \\left( \\frac{(1+e^Y)-e^Y}{1+e^Y}  \\right)^{-1} = \\frac{e^Y}{1+e^Y} \\frac{1+e^Y}{1} = e^Y = e^{\\beta_0 + \\beta_1 X}.\n\\end{align}\n\n\n\\begin{align}\n\\frac{P(X)}{1-P(X)} & = \\frac{e^Y}{1+e^Y} \\left( 1 - \\frac{e^Y}{1+e^Y}  \\right)^{-1} = \\left( \\frac{(1+e^Y)-e^Y}{1+e^Y}  \\right)^{-1} = \\frac{e^Y}{1+e^Y} \\frac{1+e^Y}{1} = e^Y = e^{\\beta_0 + \\beta_1 X}.\n\\end{align}",
            "title": "4.1"
        },
        {
            "location": "/sols/chapter4/exercise1/#exercise-41",
            "text": "Writing  Y=\\beta_0 + \\beta_1 X Y=\\beta_0 + \\beta_1 X  for clarity, we start with   \\begin{align}\nP(X) & = \\frac{e^Y}{1+e^Y}, \n\\end{align}  \\begin{align}\nP(X) & = \\frac{e^Y}{1+e^Y}, \n\\end{align}   and \"using a little bit of algebra\", we get   \\begin{align}\n\\frac{P(X)}{1-P(X)} & = \\frac{e^Y}{1+e^Y} \\left( 1 - \\frac{e^Y}{1+e^Y}  \\right)^{-1} = \\left( \\frac{(1+e^Y)-e^Y}{1+e^Y}  \\right)^{-1} = \\frac{e^Y}{1+e^Y} \\frac{1+e^Y}{1} = e^Y = e^{\\beta_0 + \\beta_1 X}.\n\\end{align}  \\begin{align}\n\\frac{P(X)}{1-P(X)} & = \\frac{e^Y}{1+e^Y} \\left( 1 - \\frac{e^Y}{1+e^Y}  \\right)^{-1} = \\left( \\frac{(1+e^Y)-e^Y}{1+e^Y}  \\right)^{-1} = \\frac{e^Y}{1+e^Y} \\frac{1+e^Y}{1} = e^Y = e^{\\beta_0 + \\beta_1 X}.\n\\end{align}",
            "title": "Exercise 4.1"
        },
        {
            "location": "/sols/chapter4/exercise2/",
            "text": "Exercise 4.2\n\n\nThis is equation (4.12) from the book, expressing the probability that \nY = k\nY = k\n, given \nX = x\nX = x\n:\n\n\n\n\n\\begin{align}\np_k(x) & = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)}.\n\\end{align}\n\n\n\\begin{align}\np_k(x) & = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)}.\n\\end{align}\n\n\n\n\nWe want to show that the class \nk\nk\n for which this probability is largest is the same as the one that is largest for equation (4.13) from the book:\n\n\n\n\n\\begin{align}\n\\delta_k(x) & = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k).\n\\end{align}\n\n\n\\begin{align}\n\\delta_k(x) & = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k).\n\\end{align}\n\n\n\n\nIn short, this involves 3 simple steps: noticing the denominator is the same for all classes (so it can be ignored when trying to find the maximum), taking the logarithm of the whole expression (since the logarithm is monotonically increasing, the maximum will be preserved), and expanding the square of the remaining expression.\n\n\nHere we go:\n\n\n\n\n\\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma)  - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2 } +  \\log(\\pi_k)= \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}\n\n\n\\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma)  - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2 } +  \\log(\\pi_k)= \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}\n\n\n\n\nwhere again we have dropped every expression that does not depend on the class \nk\nk\n.",
            "title": "4.2"
        },
        {
            "location": "/sols/chapter4/exercise2/#exercise-42",
            "text": "This is equation (4.12) from the book, expressing the probability that  Y = k Y = k , given  X = x X = x :   \\begin{align}\np_k(x) & = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)}.\n\\end{align}  \\begin{align}\np_k(x) & = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)}.\n\\end{align}   We want to show that the class  k k  for which this probability is largest is the same as the one that is largest for equation (4.13) from the book:   \\begin{align}\n\\delta_k(x) & = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k).\n\\end{align}  \\begin{align}\n\\delta_k(x) & = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k).\n\\end{align}   In short, this involves 3 simple steps: noticing the denominator is the same for all classes (so it can be ignored when trying to find the maximum), taking the logarithm of the whole expression (since the logarithm is monotonically increasing, the maximum will be preserved), and expanding the square of the remaining expression.  Here we go:   \\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma)  - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2 } +  \\log(\\pi_k)= \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}  \\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp \\left( - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma)  - \\frac{1}{2\\sigma^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2 } +  \\log(\\pi_k)= \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}   where again we have dropped every expression that does not depend on the class  k k .",
            "title": "Exercise 4.2"
        },
        {
            "location": "/sols/chapter4/exercise3/",
            "text": "Exercise 4.3\n\n\nThis proceeds in much the same way as for the LDA case, with the additional consideration that the variance parameters are now different for each class. This means that we will not be able to dispose of these terms in the final expression which, it turns out, makes us pick up terms quadratic in x. So starting from Bayes' theorem \n\n\n\n\n p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}  ,\n\n\n p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}  ,\n\n\n\n\nwe substitute with the normal density for each class:\n\n\n\n\n f_k(x) \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2} (x-\\mu_k)^2\\right)   .\n\n\n f_k(x) \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2} (x-\\mu_k)^2\\right)   .\n\n\n\n\nWe obtain the following expression\n\n\n\n\n p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma_l } \\exp \\left( \\frac{1}{2\\sigma_l^2 } (x-\\mu_k)^2\\right)}. \n\n\n p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma_l } \\exp \\left( \\frac{1}{2\\sigma_l^2 } (x-\\mu_k)^2\\right)}. \n\n\n\n\nfor which we have to find class \nk\nk\n that maximizes this expression. Following the reasoning from the previous exercise, we obtain \n\n\n\n\n\\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma_k)  - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma_k^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}}  - \\frac{\\mu_k^2}{2\\sigma_k^2 } +  \\log(\\pi_k)  + x \\cdot \\frac{\\mu_k}{\\sigma_k^2} - x^2 \\cdot \\frac{1}{2\\sigma_k^2} = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}\n\n\n\\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma_k)  - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma_k^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}}  - \\frac{\\mu_k^2}{2\\sigma_k^2 } +  \\log(\\pi_k)  + x \\cdot \\frac{\\mu_k}{\\sigma_k^2} - x^2 \\cdot \\frac{1}{2\\sigma_k^2} = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}\n\n\n\n\nIt is clear that this simplified expression in quadratic in x (hence the name - quadratic discriminant analysis), since the variance is in general different for each class, and we cannot reduce it further.",
            "title": "4.3"
        },
        {
            "location": "/sols/chapter4/exercise3/#exercise-43",
            "text": "This proceeds in much the same way as for the LDA case, with the additional consideration that the variance parameters are now different for each class. This means that we will not be able to dispose of these terms in the final expression which, it turns out, makes us pick up terms quadratic in x. So starting from Bayes' theorem     p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}  ,   p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}  ,   we substitute with the normal density for each class:    f_k(x) \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2} (x-\\mu_k)^2\\right)   .   f_k(x) \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2} (x-\\mu_k)^2\\right)   .   We obtain the following expression    p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma_l } \\exp \\left( \\frac{1}{2\\sigma_l^2 } (x-\\mu_k)^2\\right)}.    p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) } { \\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma_l } \\exp \\left( \\frac{1}{2\\sigma_l^2 } (x-\\mu_k)^2\\right)}.    for which we have to find class  k k  that maximizes this expression. Following the reasoning from the previous exercise, we obtain    \\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma_k)  - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma_k^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}}  - \\frac{\\mu_k^2}{2\\sigma_k^2 } +  \\log(\\pi_k)  + x \\cdot \\frac{\\mu_k}{\\sigma_k^2} - x^2 \\cdot \\frac{1}{2\\sigma_k^2} = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}  \\begin{align}\n\\underset{k}{\\operatorname{arg max}} p_k(x) & =  \\underset{k}{\\operatorname{arg max}} \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right) = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log \\left(  \\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k } \\exp \\left( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\right)\\right)  = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\log(\\sqrt{2\\pi}\\sigma_k)  - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2 = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\log(\\pi_k) - \\frac{1}{2\\sigma_k^2 } (x^2 - 2x \\mu_k + \\mu_k^2) = \\\\\n& = \\underset{k}{\\operatorname{arg max}}  - \\frac{\\mu_k^2}{2\\sigma_k^2 } +  \\log(\\pi_k)  + x \\cdot \\frac{\\mu_k}{\\sigma_k^2} - x^2 \\cdot \\frac{1}{2\\sigma_k^2} = \\\\\n& = \\underset{k}{\\operatorname{arg max}} \\delta_k(x),\n\\end{align}   It is clear that this simplified expression in quadratic in x (hence the name - quadratic discriminant analysis), since the variance is in general different for each class, and we cannot reduce it further.",
            "title": "Exercise 4.3"
        },
        {
            "location": "/sols/chapter4/exercise4/",
            "text": "Exercise 4.4\n\n\n(a)\n\n\nSince the set of observations X is uniformly (evenly) distributed on [0, 1] and considering that we are using a 10% range, the fraction of available information used is, on average, \n10%\n.\n\n\n(b)\n\n\nWe have two sets of observations, \nX_1\nX_1\n and \nX_2\nX_2\n, that are uniformly distributed on [0,1]x[0,1]. We wish to predict a test observation\u2019s response using only observations that are within 10% of a \nX_1\nX_1\n range \nand\n within 10% of a \nX_2\nX_2\n range. This means that we want to evaluate an \nintersection\n, so we should apply the rule of multiplication. \n\n\nWe have \n10\\% \\times 10\\% = 1\\%\n10\\% \\times 10\\% = 1\\%\n.\n\n\n(c)\n\n\nThis case is an extension of (b), so applying the rule of multiplication we have \n10\\% \\times 10\\% \\times \\cdots \\times 10\\%  = (10\\%)^{100} = 10^{-98}\\%\n10\\% \\times 10\\% \\times \\cdots \\times 10\\%  = (10\\%)^{100} = 10^{-98}\\%\n.\n\n\n(d)\n\n\nThe drawback of KNN when p is large is that there are relatively very few training observations \u201cnear\u201d any given test observation. As we have seen above, the fraction of points near a test observation can be extremely small, and most likely 0 is p is large enough. Essentially to be \"near\" a point implies being \"near\" in every dimension, and this gets less and less likely as the number dimensions increases. In other words, there are no neighbors in high dimensions. (In practice, the underlying distributions are not uniformly random and have a certain structure so points can still lump together somewhat.)\n\n\n(e)\n\n\n\n\np = 1\np = 1\n.\n\n\n\n\nIn this case, the hypercube is a line segment centered around the training  observation. This line has the length needed to contain 10% of the training observations. Since our observations are uniformly distributed on [0,1], the corresponding length  of the hypercube side is \n0.10\n.\n\n\n\n\np = 2\np = 2\n.\n\n\n\n\nTwo features means that the hypercube is a square. We can imagine this square defined by two axis, both correspoding to a set of observations uniformly distributed on [0,1]x[0,1]. To have 10% of the training observations, each side \nx\nx\n of the square must have:\n\n\n\n\nx^2 = 0.10 \\times (1 \\times 1) \\Leftrightarrow x^2 = 0.10 \\Leftrightarrow x = \\sqrt{0.10}\n\n\nx^2 = 0.10 \\times (1 \\times 1) \\Leftrightarrow x^2 = 0.10 \\Leftrightarrow x = \\sqrt{0.10}\n\n\n\n\nThus, the correspoding length of the hypercube side is \n\\sqrt{0.10}\\approx 0.31\n\\sqrt{0.10}\\approx 0.31\n . So to get 10% of the observations we \"need a square\" whose side length is about 30% of the total width of the space.\n\n\n\n\np = 100\np = 100\n.\n\n\n\n\nIn the last case the hypercube is a 100-dimensional cube. Applying the same logic as above, we have:\n\n\n\n\nx^{100} = 0.10 \\times (1^{100}) \\Leftrightarrow x = \\sqrt[100]{0.10}\n\n\nx^{100} = 0.10 \\times (1^{100}) \\Leftrightarrow x = \\sqrt[100]{0.10}\n\n\n\n\nAccordingly, the corresponding length of the hypercube side is \n\\sqrt[100]{0.10}\\approx 0.98\n\\sqrt[100]{0.10}\\approx 0.98\n . Again we see that, in a way, there are no neighbors in high dimensions. It is a strange \"local\" method, one that uses 98% of the observations for each dimension.",
            "title": "4.4"
        },
        {
            "location": "/sols/chapter4/exercise4/#exercise-44",
            "text": "",
            "title": "Exercise 4.4"
        },
        {
            "location": "/sols/chapter4/exercise4/#a",
            "text": "Since the set of observations X is uniformly (evenly) distributed on [0, 1] and considering that we are using a 10% range, the fraction of available information used is, on average,  10% .",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter4/exercise4/#b",
            "text": "We have two sets of observations,  X_1 X_1  and  X_2 X_2 , that are uniformly distributed on [0,1]x[0,1]. We wish to predict a test observation\u2019s response using only observations that are within 10% of a  X_1 X_1  range  and  within 10% of a  X_2 X_2  range. This means that we want to evaluate an  intersection , so we should apply the rule of multiplication.   We have  10\\% \\times 10\\% = 1\\% 10\\% \\times 10\\% = 1\\% .",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter4/exercise4/#c",
            "text": "This case is an extension of (b), so applying the rule of multiplication we have  10\\% \\times 10\\% \\times \\cdots \\times 10\\%  = (10\\%)^{100} = 10^{-98}\\% 10\\% \\times 10\\% \\times \\cdots \\times 10\\%  = (10\\%)^{100} = 10^{-98}\\% .",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter4/exercise4/#d",
            "text": "The drawback of KNN when p is large is that there are relatively very few training observations \u201cnear\u201d any given test observation. As we have seen above, the fraction of points near a test observation can be extremely small, and most likely 0 is p is large enough. Essentially to be \"near\" a point implies being \"near\" in every dimension, and this gets less and less likely as the number dimensions increases. In other words, there are no neighbors in high dimensions. (In practice, the underlying distributions are not uniformly random and have a certain structure so points can still lump together somewhat.)",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter4/exercise4/#e",
            "text": "p = 1 p = 1 .   In this case, the hypercube is a line segment centered around the training  observation. This line has the length needed to contain 10% of the training observations. Since our observations are uniformly distributed on [0,1], the corresponding length  of the hypercube side is  0.10 .   p = 2 p = 2 .   Two features means that the hypercube is a square. We can imagine this square defined by two axis, both correspoding to a set of observations uniformly distributed on [0,1]x[0,1]. To have 10% of the training observations, each side  x x  of the square must have:   x^2 = 0.10 \\times (1 \\times 1) \\Leftrightarrow x^2 = 0.10 \\Leftrightarrow x = \\sqrt{0.10}  x^2 = 0.10 \\times (1 \\times 1) \\Leftrightarrow x^2 = 0.10 \\Leftrightarrow x = \\sqrt{0.10}   Thus, the correspoding length of the hypercube side is  \\sqrt{0.10}\\approx 0.31 \\sqrt{0.10}\\approx 0.31  . So to get 10% of the observations we \"need a square\" whose side length is about 30% of the total width of the space.   p = 100 p = 100 .   In the last case the hypercube is a 100-dimensional cube. Applying the same logic as above, we have:   x^{100} = 0.10 \\times (1^{100}) \\Leftrightarrow x = \\sqrt[100]{0.10}  x^{100} = 0.10 \\times (1^{100}) \\Leftrightarrow x = \\sqrt[100]{0.10}   Accordingly, the corresponding length of the hypercube side is  \\sqrt[100]{0.10}\\approx 0.98 \\sqrt[100]{0.10}\\approx 0.98  . Again we see that, in a way, there are no neighbors in high dimensions. It is a strange \"local\" method, one that uses 98% of the observations for each dimension.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter4/exercise5/",
            "text": "Exercise 4.5\n\n\n(a)\n\n\nRegardless of the Bayes decision boundary, we expect QDA to perform better than LDA on the training set. This is because QDA is more flexible which leads to a closer fit. However if the Bayes decision boundary is linear, the additional flexibility of QDA leads to overfit, and LDA is expected to perform better than QDA on the test set.\n\n\n(b)\n\n\nAs mentioned in the previous paragraph, due to the additional flexibility we expect QDA to perform better than LDA on the training set. If the Bayes decision boundary is non-linear we expect that QDA will also perform better on the test set, since the additional flexibility allows it to capture at least some of the non-linearity. In other words, LDA is biased leading to a worse performance on the test set (QDA could be biased as well depending on the nature of the non-linearity of the Bayes decision boundary, but it will be less so in general).\n\n\n(c)\n\n\nIn both cases of linear and non-linear Bayes decision boundary we expect the performance of QDA to improve relative to LDA, as \nn\nn\n increases.\nIn the linear boundary case, QDA will have a worse performance on the test set since its excessive flexibility will cause it to overfit, but \nthis overfitting\n will decrease as \nn\nn\n increases as the variance is reduced, and QDA will improve relative to LDA.\nFor a non-linear Bayes decision boundary, LDA is biased and will not improve significantly past a certain sample size. QDA on the other hand, will see its variance reduced while benefitting from a more flexible model that captures the underlying non-linearity better leading to a closer fit.\n\n\nIn general, as \nn\nn\n increases the more flexible model (QDA) sees its fit improve as the variance is reduced with the increasing sample size.\n\n\n(d)\n\n\nFalse. For a linear Bayes decision boundary, QDA is too flexible compared to LDA and the noise in the data will cause it to overfit. As the sample size increases the overfitting is reduced, but in general we still expect LDA to better since it is unbiased and less prone to fit the noise.",
            "title": "4.5"
        },
        {
            "location": "/sols/chapter4/exercise5/#exercise-45",
            "text": "",
            "title": "Exercise 4.5"
        },
        {
            "location": "/sols/chapter4/exercise5/#a",
            "text": "Regardless of the Bayes decision boundary, we expect QDA to perform better than LDA on the training set. This is because QDA is more flexible which leads to a closer fit. However if the Bayes decision boundary is linear, the additional flexibility of QDA leads to overfit, and LDA is expected to perform better than QDA on the test set.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter4/exercise5/#b",
            "text": "As mentioned in the previous paragraph, due to the additional flexibility we expect QDA to perform better than LDA on the training set. If the Bayes decision boundary is non-linear we expect that QDA will also perform better on the test set, since the additional flexibility allows it to capture at least some of the non-linearity. In other words, LDA is biased leading to a worse performance on the test set (QDA could be biased as well depending on the nature of the non-linearity of the Bayes decision boundary, but it will be less so in general).",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter4/exercise5/#c",
            "text": "In both cases of linear and non-linear Bayes decision boundary we expect the performance of QDA to improve relative to LDA, as  n n  increases.\nIn the linear boundary case, QDA will have a worse performance on the test set since its excessive flexibility will cause it to overfit, but  this overfitting  will decrease as  n n  increases as the variance is reduced, and QDA will improve relative to LDA.\nFor a non-linear Bayes decision boundary, LDA is biased and will not improve significantly past a certain sample size. QDA on the other hand, will see its variance reduced while benefitting from a more flexible model that captures the underlying non-linearity better leading to a closer fit.  In general, as  n n  increases the more flexible model (QDA) sees its fit improve as the variance is reduced with the increasing sample size.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter4/exercise5/#d",
            "text": "False. For a linear Bayes decision boundary, QDA is too flexible compared to LDA and the noise in the data will cause it to overfit. As the sample size increases the overfitting is reduced, but in general we still expect LDA to better since it is unbiased and less prone to fit the noise.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter4/exercise6/",
            "text": "Exercise 4.6\n\n\n(a)\n\n\nMultiple logistic regression model is represented by the following equation:\n\n\n\n\n p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}\n\n\n p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}\n\n\n\n\nIn this case, since we have two features, the equation is as follows:\n\n\n\n\n p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}\n\n\n p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}\n\n\n\n\nConsidering that \n\\beta_0 = -6\n\\beta_0 = -6\n, \n\\beta_1 = 0.05\n\\beta_1 = 0.05\n, \n\\beta_2 = 1\n\\beta_2 = 1\n, \nX_1 = 40\nX_1 = 40\n and \nX_2 = 3.5\nX_2 = 3.5\n, we have:\n\n\n\n\n p(x) = \\frac{e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}} = 0.3775\n\n\n p(x) = \\frac{e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}} = 0.3775\n\n\n\n\nThus, the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class is \n37.75%\n.\n\n\n(b)\n\n\nWhat we are asking is how many hours a student needs to study (\nX_1\nX_1\n) to have p(x) = 0.5. Replacing in the equation presented before:\n\n\n\n\n p(x) = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}} \\Leftrightarrow \\frac{1}{2} = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}.\n\n\n p(x) = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}} \\Leftrightarrow \\frac{1}{2} = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}.\n\n\n\n\nThis will be true when the numerator equals 1, that is, when\n\n\n\n\ne^{-6 + 0.05 \\times X_1 + 1 \\times 3.5 }= 1 \\Leftrightarrow \n\n\ne^{-6 + 0.05 \\times X_1 + 1 \\times 3.5 }= 1 \\Leftrightarrow \n\n\n\n\n\n\n \\Leftrightarrow -6 + 0.05 \\times X_1 + 1 \\times 3.5 = \\log(1) = 0 \\Leftrightarrow \n\n\n \\Leftrightarrow -6 + 0.05 \\times X_1 + 1 \\times 3.5 = \\log(1) = 0 \\Leftrightarrow \n\n\n\n\n\n\n \\Leftrightarrow X_1 = 50.\n\n\n \\Leftrightarrow X_1 = 50.\n\n\n\n\nTo have a 50% chance of getting an A in the class a student needs to study \n50 hours\n.",
            "title": "4.6"
        },
        {
            "location": "/sols/chapter4/exercise6/#exercise-46",
            "text": "",
            "title": "Exercise 4.6"
        },
        {
            "location": "/sols/chapter4/exercise6/#a",
            "text": "Multiple logistic regression model is represented by the following equation:    p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}   p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + ... + \\beta_p \\times X_p}}   In this case, since we have two features, the equation is as follows:    p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}   p(x) = \\frac{e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}{1 + e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}   Considering that  \\beta_0 = -6 \\beta_0 = -6 ,  \\beta_1 = 0.05 \\beta_1 = 0.05 ,  \\beta_2 = 1 \\beta_2 = 1 ,  X_1 = 40 X_1 = 40  and  X_2 = 3.5 X_2 = 3.5 , we have:    p(x) = \\frac{e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}} = 0.3775   p(x) = \\frac{e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times 40 + 1 \\times 3.5}} = 0.3775   Thus, the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class is  37.75% .",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter4/exercise6/#b",
            "text": "What we are asking is how many hours a student needs to study ( X_1 X_1 ) to have p(x) = 0.5. Replacing in the equation presented before:    p(x) = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}} \\Leftrightarrow \\frac{1}{2} = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}.   p(x) = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}} \\Leftrightarrow \\frac{1}{2} = \\frac{e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}{1 + e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5}}.   This will be true when the numerator equals 1, that is, when   e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5 }= 1 \\Leftrightarrow   e^{-6 + 0.05 \\times X_1 + 1 \\times 3.5 }= 1 \\Leftrightarrow      \\Leftrightarrow -6 + 0.05 \\times X_1 + 1 \\times 3.5 = \\log(1) = 0 \\Leftrightarrow    \\Leftrightarrow -6 + 0.05 \\times X_1 + 1 \\times 3.5 = \\log(1) = 0 \\Leftrightarrow      \\Leftrightarrow X_1 = 50.   \\Leftrightarrow X_1 = 50.   To have a 50% chance of getting an A in the class a student needs to study  50 hours .",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter4/exercise7/",
            "text": "Exercise 4.7\n\n\nFrom the statement of the exercise, we know that\n\n\n\n\n\\begin{align}\n\\ Pr(X \\mid Y = yes) & = f_{yes}(x)  = N(\\mu = 10, \\sigma^2 = 36),\\\\\nPr(X \\mid Y = no) & = f_{no}(x) = N(\\mu = 0, \\sigma^2 =36),\\\\\nPr(Y = yes) & = \\pi_{yes} = 0.8,\\\\\nPr(Y = no)&  = \\pi_{no} = 0.2.\\\\\n\\end{align}\n\n\n\\begin{align}\n\\ Pr(X \\mid Y = yes) & = f_{yes}(x)  = N(\\mu = 10, \\sigma^2 = 36),\\\\\nPr(X \\mid Y = no) & = f_{no}(x) = N(\\mu = 0, \\sigma^2 =36),\\\\\nPr(Y = yes) & = \\pi_{yes} = 0.8,\\\\\nPr(Y = no)&  = \\pi_{no} = 0.2.\\\\\n\\end{align}\n\n\n\n\nWe want to calculate \nPr(Y = \"Yes \\mid X = 4)\nPr(Y = \"Yes \\mid X = 4)\n. Using Bayes' theorem and substituting the expressions above:\n\n\n\n\n\\begin{align}\n\\ \\pi_{yes}(x) & =  \\frac{\\pi_{yes} f_{yes}(x) }{ \\pi_{yes} f_{yes}(x) + \\pi_{no} f_{no}(x)},\\\\\n\\ \\pi_{yes}(4) & =  \\frac{\\pi_{yes} f_{yes}(4) }{ \\pi_{yes} f_{yes}(4) + \\pi_{no} f_{no}(4)} = \\\\\n& = \\frac{0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} }{  0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} + 0.2 e^{\\left( -\\frac{1}{2\\times 36}(4-0)^2 \\right)}} \\\\\n& = 0.7571.\\\\\n\\end{align}\n\n\n\\begin{align}\n\\ \\pi_{yes}(x) & =  \\frac{\\pi_{yes} f_{yes}(x) }{ \\pi_{yes} f_{yes}(x) + \\pi_{no} f_{no}(x)},\\\\\n\\ \\pi_{yes}(4) & =  \\frac{\\pi_{yes} f_{yes}(4) }{ \\pi_{yes} f_{yes}(4) + \\pi_{no} f_{no}(4)} = \\\\\n& = \\frac{0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} }{  0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} + 0.2 e^{\\left( -\\frac{1}{2\\times 36}(4-0)^2 \\right)}} \\\\\n& = 0.7571.\\\\\n\\end{align}",
            "title": "4.7"
        },
        {
            "location": "/sols/chapter4/exercise7/#exercise-47",
            "text": "From the statement of the exercise, we know that   \\begin{align}\n\\ Pr(X \\mid Y = yes) & = f_{yes}(x)  = N(\\mu = 10, \\sigma^2 = 36),\\\\\nPr(X \\mid Y = no) & = f_{no}(x) = N(\\mu = 0, \\sigma^2 =36),\\\\\nPr(Y = yes) & = \\pi_{yes} = 0.8,\\\\\nPr(Y = no)&  = \\pi_{no} = 0.2.\\\\\n\\end{align}  \\begin{align}\n\\ Pr(X \\mid Y = yes) & = f_{yes}(x)  = N(\\mu = 10, \\sigma^2 = 36),\\\\\nPr(X \\mid Y = no) & = f_{no}(x) = N(\\mu = 0, \\sigma^2 =36),\\\\\nPr(Y = yes) & = \\pi_{yes} = 0.8,\\\\\nPr(Y = no)&  = \\pi_{no} = 0.2.\\\\\n\\end{align}   We want to calculate  Pr(Y = \"Yes \\mid X = 4) Pr(Y = \"Yes \\mid X = 4) . Using Bayes' theorem and substituting the expressions above:   \\begin{align}\n\\ \\pi_{yes}(x) & =  \\frac{\\pi_{yes} f_{yes}(x) }{ \\pi_{yes} f_{yes}(x) + \\pi_{no} f_{no}(x)},\\\\\n\\ \\pi_{yes}(4) & =  \\frac{\\pi_{yes} f_{yes}(4) }{ \\pi_{yes} f_{yes}(4) + \\pi_{no} f_{no}(4)} = \\\\\n& = \\frac{0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} }{  0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} + 0.2 e^{\\left( -\\frac{1}{2\\times 36}(4-0)^2 \\right)}} \\\\\n& = 0.7571.\\\\\n\\end{align}  \\begin{align}\n\\ \\pi_{yes}(x) & =  \\frac{\\pi_{yes} f_{yes}(x) }{ \\pi_{yes} f_{yes}(x) + \\pi_{no} f_{no}(x)},\\\\\n\\ \\pi_{yes}(4) & =  \\frac{\\pi_{yes} f_{yes}(4) }{ \\pi_{yes} f_{yes}(4) + \\pi_{no} f_{no}(4)} = \\\\\n& = \\frac{0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} }{  0.8 e^{\\left( -\\frac{1}{2\\times 36}(4-10)^2 \\right)} + 0.2 e^{\\left( -\\frac{1}{2\\times 36}(4-0)^2 \\right)}} \\\\\n& = 0.7571.\\\\\n\\end{align}",
            "title": "Exercise 4.7"
        },
        {
            "location": "/sols/chapter4/exercise8/",
            "text": "Exercise 4.8\n\n\nWe should prefer the method that has a lower error rate on the test data because this shows how well the model works on unseen data.\nIn other words, choosing the model with the lowest error rate on the test data means that we're choosing the model with better prediction capacity.\n\n\nIn this case, the error rate on test data for the logistic regression is explicit: 30%. For the 1-nearest neighbor we have to do some computations.\n\n\nWe know that when we use the 1-nearest neighbor, the average error rate is 18%. This error (\n\\varepsilon\n\\varepsilon\n) is averaged over both test and training sets, which means that:\n\n\n\n\n \\frac{\\varepsilon_{training} + \\varepsilon_{test}}{2} = 0.18 \\Leftrightarrow \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training}\n\n\n \\frac{\\varepsilon_{training} + \\varepsilon_{test}}{2} = 0.18 \\Leftrightarrow \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training}\n\n\n\n\nFor a 1-nearest neighbor model, we have \n\\varepsilon_{training} = 0\n\\varepsilon_{training} = 0\n. This happens because for any training example, its nearest neighbor is always going to be itself.\nTherefore:\n\n\n\n\n \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training} \\Leftrightarrow \\varepsilon_{test} = 0.36 - 0 \\Leftrightarrow \\varepsilon_{test} = 0.36\n\n\n \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training} \\Leftrightarrow \\varepsilon_{test} = 0.36 - 0 \\Leftrightarrow \\varepsilon_{test} = 0.36\n\n\n\n\nThe method we should prefer to use for classification of new observations is the \nlogistic regression\n method, even though its error rate on the training data is larger than the one for the 1-nearest neighbor model (18% vs. 0%).",
            "title": "4.8"
        },
        {
            "location": "/sols/chapter4/exercise8/#exercise-48",
            "text": "We should prefer the method that has a lower error rate on the test data because this shows how well the model works on unseen data.\nIn other words, choosing the model with the lowest error rate on the test data means that we're choosing the model with better prediction capacity.  In this case, the error rate on test data for the logistic regression is explicit: 30%. For the 1-nearest neighbor we have to do some computations.  We know that when we use the 1-nearest neighbor, the average error rate is 18%. This error ( \\varepsilon \\varepsilon ) is averaged over both test and training sets, which means that:    \\frac{\\varepsilon_{training} + \\varepsilon_{test}}{2} = 0.18 \\Leftrightarrow \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training}   \\frac{\\varepsilon_{training} + \\varepsilon_{test}}{2} = 0.18 \\Leftrightarrow \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training}   For a 1-nearest neighbor model, we have  \\varepsilon_{training} = 0 \\varepsilon_{training} = 0 . This happens because for any training example, its nearest neighbor is always going to be itself.\nTherefore:    \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training} \\Leftrightarrow \\varepsilon_{test} = 0.36 - 0 \\Leftrightarrow \\varepsilon_{test} = 0.36   \\varepsilon_{test} = 2 \\times 0.18 - \\varepsilon_{training} \\Leftrightarrow \\varepsilon_{test} = 0.36 - 0 \\Leftrightarrow \\varepsilon_{test} = 0.36   The method we should prefer to use for classification of new observations is the  logistic regression  method, even though its error rate on the training data is larger than the one for the 1-nearest neighbor model (18% vs. 0%).",
            "title": "Exercise 4.8"
        },
        {
            "location": "/sols/chapter4/exercise9/",
            "text": "Exercise 4.9\n\n\nThe formulas for the odds in terms of the probability of a certain event, and vice-versa, are the following:\n\n\n\n\n odds = \\frac{p}{1-p},\n\n\n odds = \\frac{p}{1-p},\n\n\n\n\n\n\n p = \\frac{odds}{1+odds}.\n\n\n p = \\frac{odds}{1+odds}.\n\n\n\n\n(a)\n\n\nWith odds=0.37, we get\n\n\n\n\np = \\frac{0.37}{1 + 0.37} \\approx 0.27.\n\n\np = \\frac{0.37}{1 + 0.37} \\approx 0.27.\n\n\n\n\n(b)\n\n\nSimilarly, if p = 0.16, we have\n\n\n\n\nodds = \\frac{0.16}{1-0.16} \\approx 0.19.\n\n\nodds = \\frac{0.16}{1-0.16} \\approx 0.19.",
            "title": "4.9"
        },
        {
            "location": "/sols/chapter4/exercise9/#exercise-49",
            "text": "The formulas for the odds in terms of the probability of a certain event, and vice-versa, are the following:    odds = \\frac{p}{1-p},   odds = \\frac{p}{1-p},     p = \\frac{odds}{1+odds}.   p = \\frac{odds}{1+odds}.",
            "title": "Exercise 4.9"
        },
        {
            "location": "/sols/chapter4/exercise9/#a",
            "text": "With odds=0.37, we get   p = \\frac{0.37}{1 + 0.37} \\approx 0.27.  p = \\frac{0.37}{1 + 0.37} \\approx 0.27.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter4/exercise9/#b",
            "text": "Similarly, if p = 0.16, we have   odds = \\frac{0.16}{1-0.16} \\approx 0.19.  odds = \\frac{0.16}{1-0.16} \\approx 0.19.",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter4/exercise10/",
            "text": "Exercise 4.10\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns #visualization library\nfrom sklearn.linear_model import LogisticRegression #problem will be solved with scikit\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis #linear discriminant analysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis #quadratic discriminant analysis\nfrom sklearn.neighbors import KNeighborsClassifier #K nearest neighbours (KNN)\n\nimport statsmodels.api as sm #to compute p-values\nfrom patsy import dmatrices\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/Weekly.csv',index_col=0)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nYear\n\n      \nLag1\n\n      \nLag2\n\n      \nLag3\n\n      \nLag4\n\n      \nLag5\n\n      \nVolume\n\n      \nToday\n\n      \nDirection\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n1990\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n-0.229\n\n      \n-3.484\n\n      \n0.154976\n\n      \n-0.270\n\n      \nDown\n\n    \n\n    \n\n      \n2\n\n      \n1990\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n-0.229\n\n      \n0.148574\n\n      \n-2.576\n\n      \nDown\n\n    \n\n    \n\n      \n3\n\n      \n1990\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n0.159837\n\n      \n3.514\n\n      \nUp\n\n    \n\n    \n\n      \n4\n\n      \n1990\n\n      \n3.514\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n0.161630\n\n      \n0.712\n\n      \nUp\n\n    \n\n    \n\n      \n5\n\n      \n1990\n\n      \n0.712\n\n      \n3.514\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n0.153728\n\n      \n1.178\n\n      \nUp\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\ndf.describe() #descriptive statistics\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nYear\n\n      \nLag1\n\n      \nLag2\n\n      \nLag3\n\n      \nLag4\n\n      \nLag5\n\n      \nVolume\n\n      \nToday\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n1089.000000\n\n      \n1089.000000\n\n      \n1089.000000\n\n      \n1089.000000\n\n      \n1089.000000\n\n      \n1089.000000\n\n      \n1089.000000\n\n      \n1089.000000\n\n    \n\n    \n\n      \nmean\n\n      \n2000.048669\n\n      \n0.150585\n\n      \n0.151079\n\n      \n0.147205\n\n      \n0.145818\n\n      \n0.139893\n\n      \n1.574618\n\n      \n0.149899\n\n    \n\n    \n\n      \nstd\n\n      \n6.033182\n\n      \n2.357013\n\n      \n2.357254\n\n      \n2.360502\n\n      \n2.360279\n\n      \n2.361285\n\n      \n1.686636\n\n      \n2.356927\n\n    \n\n    \n\n      \nmin\n\n      \n1990.000000\n\n      \n-18.195000\n\n      \n-18.195000\n\n      \n-18.195000\n\n      \n-18.195000\n\n      \n-18.195000\n\n      \n0.087465\n\n      \n-18.195000\n\n    \n\n    \n\n      \n25%\n\n      \n1995.000000\n\n      \n-1.154000\n\n      \n-1.154000\n\n      \n-1.158000\n\n      \n-1.158000\n\n      \n-1.166000\n\n      \n0.332022\n\n      \n-1.154000\n\n    \n\n    \n\n      \n50%\n\n      \n2000.000000\n\n      \n0.241000\n\n      \n0.241000\n\n      \n0.241000\n\n      \n0.238000\n\n      \n0.234000\n\n      \n1.002680\n\n      \n0.241000\n\n    \n\n    \n\n      \n75%\n\n      \n2005.000000\n\n      \n1.405000\n\n      \n1.409000\n\n      \n1.409000\n\n      \n1.409000\n\n      \n1.405000\n\n      \n2.053727\n\n      \n1.405000\n\n    \n\n    \n\n      \nmax\n\n      \n2010.000000\n\n      \n12.026000\n\n      \n12.026000\n\n      \n12.026000\n\n      \n12.026000\n\n      \n12.026000\n\n      \n9.328214\n\n      \n12.026000\n\n    \n\n  \n\n\n\n\n\n\n\ndf.corr() #correlation matrix\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nYear\n\n      \nLag1\n\n      \nLag2\n\n      \nLag3\n\n      \nLag4\n\n      \nLag5\n\n      \nVolume\n\n      \nToday\n\n    \n\n  \n\n  \n\n    \n\n      \nYear\n\n      \n1.000000\n\n      \n-0.032289\n\n      \n-0.033390\n\n      \n-0.030006\n\n      \n-0.031128\n\n      \n-0.030519\n\n      \n0.841942\n\n      \n-0.032460\n\n    \n\n    \n\n      \nLag1\n\n      \n-0.032289\n\n      \n1.000000\n\n      \n-0.074853\n\n      \n0.058636\n\n      \n-0.071274\n\n      \n-0.008183\n\n      \n-0.064951\n\n      \n-0.075032\n\n    \n\n    \n\n      \nLag2\n\n      \n-0.033390\n\n      \n-0.074853\n\n      \n1.000000\n\n      \n-0.075721\n\n      \n0.058382\n\n      \n-0.072499\n\n      \n-0.085513\n\n      \n0.059167\n\n    \n\n    \n\n      \nLag3\n\n      \n-0.030006\n\n      \n0.058636\n\n      \n-0.075721\n\n      \n1.000000\n\n      \n-0.075396\n\n      \n0.060657\n\n      \n-0.069288\n\n      \n-0.071244\n\n    \n\n    \n\n      \nLag4\n\n      \n-0.031128\n\n      \n-0.071274\n\n      \n0.058382\n\n      \n-0.075396\n\n      \n1.000000\n\n      \n-0.075675\n\n      \n-0.061075\n\n      \n-0.007826\n\n    \n\n    \n\n      \nLag5\n\n      \n-0.030519\n\n      \n-0.008183\n\n      \n-0.072499\n\n      \n0.060657\n\n      \n-0.075675\n\n      \n1.000000\n\n      \n-0.058517\n\n      \n0.011013\n\n    \n\n    \n\n      \nVolume\n\n      \n0.841942\n\n      \n-0.064951\n\n      \n-0.085513\n\n      \n-0.069288\n\n      \n-0.061075\n\n      \n-0.058517\n\n      \n1.000000\n\n      \n-0.033078\n\n    \n\n    \n\n      \nToday\n\n      \n-0.032460\n\n      \n-0.075032\n\n      \n0.059167\n\n      \n-0.071244\n\n      \n-0.007826\n\n      \n0.011013\n\n      \n-0.033078\n\n      \n1.000000\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nRelationship Year/Volume should be explored\n\n\n\n\nsns.distplot(df['Today']);\n\n\n\n\nC:\\Program Files\\Anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\nsns.boxplot(x='Direction', y='Volume', data=df);\n\n\n\n\n\n\nsns.pairplot(df);\n\n\n\n\n\n\nplt.scatter(df['Year'],df['Volume']);\n\n\n\n\n\n\n\n\nThe Year/Volume relationship is the only one with a visible pattern\n\n\n\n\n(b)\n\n\n#logistic regression model\ntrain_cols = ['Lag1','Lag2','Lag3','Lag4','Lag5','Volume'] #independent var. considered in the logistic model\nlr = LogisticRegression()\nmod = lr.fit(df[train_cols], df['Direction'])\n\n\n\n\nmod.coef_ #independent var. coefficients\n\n\n\n\narray([[-0.04117292,  0.05846974, -0.01599122, -0.02769998, -0.01440289,\n        -0.02212844]])\n\n\n\nmod.intercept_ #interception\n\n\n\n\narray([ 0.26484745])\n\n\n\n#p-values determine if the predictors appear to be statistically significant \n#it's easier to get p-values with statsmodel\n\n#we need to transform the target value to non-categorical\n#if we don't do this, statsmodel will try to use dummy variables\n#dummy variables don't work here because they will create a target value with 2 columns\ndf['Direction'] = df['Direction'].map({'Down':0, 'Up':1})\n\n#fit model\ny, X = dmatrices('Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume', data=df, return_type='dataframe')\n#logit = sm.Logit(y.ix[:,0], X) #\nlogit = sm.Logit(y, X) \nresult = logit.fit()\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.682441\n         Iterations 4\n\n\n\nprint(result.summary())\n\n\n\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              Direction   No. Observations:                 1089\nModel:                          Logit   Df Residuals:                     1082\nMethod:                           MLE   Df Model:                            6\nDate:                Fri, 27 Jan 2017   Pseudo R-squ.:                0.006580\nTime:                        05:36:33   Log-Likelihood:                -743.18\nconverged:                       True   LL-Null:                       -748.10\n                                        LLR p-value:                    0.1313\n==============================================================================\n                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept      0.2669      0.086      3.106      0.002         0.098     0.435\nLag1          -0.0413      0.026     -1.563      0.118        -0.093     0.010\nLag2           0.0584      0.027      2.175      0.030         0.006     0.111\nLag3          -0.0161      0.027     -0.602      0.547        -0.068     0.036\nLag4          -0.0278      0.026     -1.050      0.294        -0.080     0.024\nLag5          -0.0145      0.026     -0.549      0.583        -0.066     0.037\nVolume        -0.0227      0.037     -0.616      0.538        -0.095     0.050\n==============================================================================\n\n\n\nLag2 seems to be a predictor with statistical significance. We can say that because Lag2 has a small P|z|, meaning that there's a small probability that Lag2 coefficient is equal to zero.\n\n\nNote\n: Notice that the coefficients are not exactly the same. That happens because scikit-learn applies some kind of parameter regularization. You can confirm this by reading the scikit-learn documentation, as suggested here: http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels. An option to overcome this is to for LogisticRegression to use a big C value (e.g. 1e9).\n\n\n(c)\n\n\n#confusion matrix\nconf_mat = confusion_matrix(df['Direction'], lr.predict(df[train_cols]))\nprint(conf_mat) #alternative to 'fancy' plot\n\n\n\n\n[[ 55 429]\n [ 47 558]]\n\n\n\n#'fancy' confusion matrix plot\n#based on: Raschka (2014)\nfig, ax = plt.subplots(figsize=(2, 2))\nax.matshow(conf_mat, cmap=plt.cm.Reds, alpha=0.3)\nfor i in range(conf_mat.shape[0]):\n    for j in range(conf_mat.shape[1]):\n        ax.text(x=j, y=i,\n        s=conf_mat[i, j],\n        va='center', ha='center')\nplt.xlabel('predicted label')\nplt.ylabel('true label')\nplt.show()\n\n\n\n\n\n\n\n\nThe confusion matrix gives us two types of mistakes: 1) false positives; 2) false negatives.\n\n\nA \nfalse positive\n occurs when our prediction gives us a positive value but the real value is negative. A \nfalse negative\n occurs when our prediction gives us a negative value but the real value is positive.\n\n\nAssuming that class 1 ('Up') is the positive class, our model correctly classified 55 samples that belong to class 0 (true negatives) and 558 samples that belong to class 1 (true positives). However, our model also incorrectly misclassified 429 samples from class 0 to class 1 (false negatives), and it predicted that 47 samples were 'Up' although they were 'Down' (false positives).\n\n\n\n\nNote:\n 'Down' is class 0 because it is the first 'Direction' class in the dataset. When encoding the categorical variable, scikit will automatically attribute the value 0 to the first class it gets, value 1 to the second class it gets and so one.\n\n\n#overall fraction of correct predictions\nlr.score(df[train_cols],df['Direction'])\n\n\n\n\n0.56290174471992649\n\n\n\n(d)\n\n\n#partitioning the dataset\ndf_9908 = df[(df['Year'] >=1990) & (df['Year'] <=2008)]\ndf_0910 = df[(df['Year'] >=2009) & (df['Year'] <=2010)]\n\n\n\n\n#to avoid 'ValueError: Found arrays with inconsistent numbers of sample'\n#shape must be (X,1) and not (X,)\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n\n\n\n#logistic regression\nmod = lr.fit(X,df_9908['Direction']) #regression object already exists; just need to fit it to the new data\n\n\n\n\n#confusion matrix\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\nconf_mat = confusion_matrix(df_0910['Direction'], lr.predict(X))\nprint(conf_mat)\n\n\n\n\n[[ 9 34]\n [ 5 56]]\n\n\n\n#overall fraction of correct predictions\nlr.score(X, df_0910['Direction'])\n\n\n\n\n0.625\n\n\n\n(e)\n\n\n#getting data ready\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#linear discriminant analysis (LDA)\nlda = LinearDiscriminantAnalysis()\nlda.fit(X,df_9908['Direction'])\n\n\n\n\nLinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n              solver='svd', store_covariance=False, tol=0.0001)\n\n\n\n#getting data ready\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], lda.predict(X))\nprint(conf_mat)\n\n\n\n\n[[ 9 34]\n [ 5 56]]\n\n\n\n#overall fraction of correct predictions\n#it will be helpful for the next question\nlda.score(X, df_0910['Direction'])\n\n\n\n\n0.625\n\n\n\n(f)\n\n\n#getting data ready\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#quadratic discriminant analysis (QDA)\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(X,df_9908['Direction'])\n\n\n\n\nQuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n               store_covariances=False, tol=0.0001)\n\n\n\n#getting data ready\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], qda.predict(X))\nprint(conf_mat)\n\n\n\n\n[[ 0 43]\n [ 0 61]]\n\n\n\n#overall fraction of correct predictions\n#it will be helpful for the next question\nqda.score(X, df_0910['Direction'])\n\n\n\n\n0.58653846153846156\n\n\n\n(g)\n\n\n#getting data ready\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#creating an instance of Neighbours Classifier and fitting the data\nnbrs = KNeighborsClassifier(n_neighbors=1)\nnbrs.fit(X,df_9908['Direction'])\n\n\n\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n           weights='uniform')\n\n\n\n#getting data ready\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], nbrs.predict(X))\nprint(conf_mat)\n\n\n\n\n[[21 22]\n [31 30]]\n\n\n\n#overall fraction of correct predictions\n#it will be helpful for the next question\nnbrs.score(X, df_0910['Direction'])\n\n\n\n\n0.49038461538461536\n\n\n\n(h)\n\n\nThe methods that appear to provide the best results are the \nlogistic regression\n and the \nlinear discriminant analysis (LDA)\n. They are the methods where score (overall fraction of correct predictions) is higher (0.625 vs 0.587 vs 0.490)\n\n\n(i)\n\n\n#trying with a different number of neighbors\nn_nbrs = 10\n\n#getting data ready for the neigb\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#creating an instance of Neighbours Classifier and fitting the data\nnbrs = KNeighborsClassifier(n_neighbors=n_nbrs)\nnbrs.fit(X,df_9908['Direction'])\n\n#getting data ready for the confusion matrix\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], nbrs.predict(X))\nprint(conf_mat)\n\n\n\n\n[[22 21]\n [24 37]]\n\n\n\nNow, it's time to play a little bit with models. We leave this entertaining task to the reader, as homework.\n\n\nReferences\n\n\n\n\nRaschka, S., 2014, Python Machine Learning",
            "title": "4.10"
        },
        {
            "location": "/sols/chapter4/exercise10/#exercise-410",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns #visualization library\nfrom sklearn.linear_model import LogisticRegression #problem will be solved with scikit\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis #linear discriminant analysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis #quadratic discriminant analysis\nfrom sklearn.neighbors import KNeighborsClassifier #K nearest neighbours (KNN)\n\nimport statsmodels.api as sm #to compute p-values\nfrom patsy import dmatrices\n\n%matplotlib inline  df = pd.read_csv('../data/Weekly.csv',index_col=0)  df.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Year \n       Lag1 \n       Lag2 \n       Lag3 \n       Lag4 \n       Lag5 \n       Volume \n       Today \n       Direction \n     \n   \n   \n     \n       1 \n       1990 \n       0.816 \n       1.572 \n       -3.936 \n       -0.229 \n       -3.484 \n       0.154976 \n       -0.270 \n       Down \n     \n     \n       2 \n       1990 \n       -0.270 \n       0.816 \n       1.572 \n       -3.936 \n       -0.229 \n       0.148574 \n       -2.576 \n       Down \n     \n     \n       3 \n       1990 \n       -2.576 \n       -0.270 \n       0.816 \n       1.572 \n       -3.936 \n       0.159837 \n       3.514 \n       Up \n     \n     \n       4 \n       1990 \n       3.514 \n       -2.576 \n       -0.270 \n       0.816 \n       1.572 \n       0.161630 \n       0.712 \n       Up \n     \n     \n       5 \n       1990 \n       0.712 \n       3.514 \n       -2.576 \n       -0.270 \n       0.816 \n       0.153728 \n       1.178 \n       Up",
            "title": "Exercise 4.10"
        },
        {
            "location": "/sols/chapter4/exercise10/#a",
            "text": "df.describe() #descriptive statistics   \n   \n     \n       \n       Year \n       Lag1 \n       Lag2 \n       Lag3 \n       Lag4 \n       Lag5 \n       Volume \n       Today \n     \n   \n   \n     \n       count \n       1089.000000 \n       1089.000000 \n       1089.000000 \n       1089.000000 \n       1089.000000 \n       1089.000000 \n       1089.000000 \n       1089.000000 \n     \n     \n       mean \n       2000.048669 \n       0.150585 \n       0.151079 \n       0.147205 \n       0.145818 \n       0.139893 \n       1.574618 \n       0.149899 \n     \n     \n       std \n       6.033182 \n       2.357013 \n       2.357254 \n       2.360502 \n       2.360279 \n       2.361285 \n       1.686636 \n       2.356927 \n     \n     \n       min \n       1990.000000 \n       -18.195000 \n       -18.195000 \n       -18.195000 \n       -18.195000 \n       -18.195000 \n       0.087465 \n       -18.195000 \n     \n     \n       25% \n       1995.000000 \n       -1.154000 \n       -1.154000 \n       -1.158000 \n       -1.158000 \n       -1.166000 \n       0.332022 \n       -1.154000 \n     \n     \n       50% \n       2000.000000 \n       0.241000 \n       0.241000 \n       0.241000 \n       0.238000 \n       0.234000 \n       1.002680 \n       0.241000 \n     \n     \n       75% \n       2005.000000 \n       1.405000 \n       1.409000 \n       1.409000 \n       1.409000 \n       1.405000 \n       2.053727 \n       1.405000 \n     \n     \n       max \n       2010.000000 \n       12.026000 \n       12.026000 \n       12.026000 \n       12.026000 \n       12.026000 \n       9.328214 \n       12.026000 \n     \n      df.corr() #correlation matrix   \n   \n     \n       \n       Year \n       Lag1 \n       Lag2 \n       Lag3 \n       Lag4 \n       Lag5 \n       Volume \n       Today \n     \n   \n   \n     \n       Year \n       1.000000 \n       -0.032289 \n       -0.033390 \n       -0.030006 \n       -0.031128 \n       -0.030519 \n       0.841942 \n       -0.032460 \n     \n     \n       Lag1 \n       -0.032289 \n       1.000000 \n       -0.074853 \n       0.058636 \n       -0.071274 \n       -0.008183 \n       -0.064951 \n       -0.075032 \n     \n     \n       Lag2 \n       -0.033390 \n       -0.074853 \n       1.000000 \n       -0.075721 \n       0.058382 \n       -0.072499 \n       -0.085513 \n       0.059167 \n     \n     \n       Lag3 \n       -0.030006 \n       0.058636 \n       -0.075721 \n       1.000000 \n       -0.075396 \n       0.060657 \n       -0.069288 \n       -0.071244 \n     \n     \n       Lag4 \n       -0.031128 \n       -0.071274 \n       0.058382 \n       -0.075396 \n       1.000000 \n       -0.075675 \n       -0.061075 \n       -0.007826 \n     \n     \n       Lag5 \n       -0.030519 \n       -0.008183 \n       -0.072499 \n       0.060657 \n       -0.075675 \n       1.000000 \n       -0.058517 \n       0.011013 \n     \n     \n       Volume \n       0.841942 \n       -0.064951 \n       -0.085513 \n       -0.069288 \n       -0.061075 \n       -0.058517 \n       1.000000 \n       -0.033078 \n     \n     \n       Today \n       -0.032460 \n       -0.075032 \n       0.059167 \n       -0.071244 \n       -0.007826 \n       0.011013 \n       -0.033078 \n       1.000000 \n     \n       Relationship Year/Volume should be explored   sns.distplot(df['Today']);  C:\\Program Files\\Anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j   sns.boxplot(x='Direction', y='Volume', data=df);   sns.pairplot(df);   plt.scatter(df['Year'],df['Volume']);    The Year/Volume relationship is the only one with a visible pattern",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter4/exercise10/#b",
            "text": "#logistic regression model\ntrain_cols = ['Lag1','Lag2','Lag3','Lag4','Lag5','Volume'] #independent var. considered in the logistic model\nlr = LogisticRegression()\nmod = lr.fit(df[train_cols], df['Direction'])  mod.coef_ #independent var. coefficients  array([[-0.04117292,  0.05846974, -0.01599122, -0.02769998, -0.01440289,\n        -0.02212844]])  mod.intercept_ #interception  array([ 0.26484745])  #p-values determine if the predictors appear to be statistically significant \n#it's easier to get p-values with statsmodel\n\n#we need to transform the target value to non-categorical\n#if we don't do this, statsmodel will try to use dummy variables\n#dummy variables don't work here because they will create a target value with 2 columns\ndf['Direction'] = df['Direction'].map({'Down':0, 'Up':1})\n\n#fit model\ny, X = dmatrices('Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume', data=df, return_type='dataframe')\n#logit = sm.Logit(y.ix[:,0], X) #\nlogit = sm.Logit(y, X) \nresult = logit.fit()  Optimization terminated successfully.\n         Current function value: 0.682441\n         Iterations 4  print(result.summary())                             Logit Regression Results                           \n==============================================================================\nDep. Variable:              Direction   No. Observations:                 1089\nModel:                          Logit   Df Residuals:                     1082\nMethod:                           MLE   Df Model:                            6\nDate:                Fri, 27 Jan 2017   Pseudo R-squ.:                0.006580\nTime:                        05:36:33   Log-Likelihood:                -743.18\nconverged:                       True   LL-Null:                       -748.10\n                                        LLR p-value:                    0.1313\n==============================================================================\n                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept      0.2669      0.086      3.106      0.002         0.098     0.435\nLag1          -0.0413      0.026     -1.563      0.118        -0.093     0.010\nLag2           0.0584      0.027      2.175      0.030         0.006     0.111\nLag3          -0.0161      0.027     -0.602      0.547        -0.068     0.036\nLag4          -0.0278      0.026     -1.050      0.294        -0.080     0.024\nLag5          -0.0145      0.026     -0.549      0.583        -0.066     0.037\nVolume        -0.0227      0.037     -0.616      0.538        -0.095     0.050\n==============================================================================  Lag2 seems to be a predictor with statistical significance. We can say that because Lag2 has a small P|z|, meaning that there's a small probability that Lag2 coefficient is equal to zero.  Note : Notice that the coefficients are not exactly the same. That happens because scikit-learn applies some kind of parameter regularization. You can confirm this by reading the scikit-learn documentation, as suggested here: http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels. An option to overcome this is to for LogisticRegression to use a big C value (e.g. 1e9).",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter4/exercise10/#c",
            "text": "#confusion matrix\nconf_mat = confusion_matrix(df['Direction'], lr.predict(df[train_cols]))\nprint(conf_mat) #alternative to 'fancy' plot  [[ 55 429]\n [ 47 558]]  #'fancy' confusion matrix plot\n#based on: Raschka (2014)\nfig, ax = plt.subplots(figsize=(2, 2))\nax.matshow(conf_mat, cmap=plt.cm.Reds, alpha=0.3)\nfor i in range(conf_mat.shape[0]):\n    for j in range(conf_mat.shape[1]):\n        ax.text(x=j, y=i,\n        s=conf_mat[i, j],\n        va='center', ha='center')\nplt.xlabel('predicted label')\nplt.ylabel('true label')\nplt.show()    The confusion matrix gives us two types of mistakes: 1) false positives; 2) false negatives.  A  false positive  occurs when our prediction gives us a positive value but the real value is negative. A  false negative  occurs when our prediction gives us a negative value but the real value is positive.  Assuming that class 1 ('Up') is the positive class, our model correctly classified 55 samples that belong to class 0 (true negatives) and 558 samples that belong to class 1 (true positives). However, our model also incorrectly misclassified 429 samples from class 0 to class 1 (false negatives), and it predicted that 47 samples were 'Up' although they were 'Down' (false positives).   Note:  'Down' is class 0 because it is the first 'Direction' class in the dataset. When encoding the categorical variable, scikit will automatically attribute the value 0 to the first class it gets, value 1 to the second class it gets and so one.  #overall fraction of correct predictions\nlr.score(df[train_cols],df['Direction'])  0.56290174471992649",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter4/exercise10/#d",
            "text": "#partitioning the dataset\ndf_9908 = df[(df['Year'] >=1990) & (df['Year'] <=2008)]\ndf_0910 = df[(df['Year'] >=2009) & (df['Year'] <=2010)]  #to avoid 'ValueError: Found arrays with inconsistent numbers of sample'\n#shape must be (X,1) and not (X,)\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)  #logistic regression\nmod = lr.fit(X,df_9908['Direction']) #regression object already exists; just need to fit it to the new data  #confusion matrix\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\nconf_mat = confusion_matrix(df_0910['Direction'], lr.predict(X))\nprint(conf_mat)  [[ 9 34]\n [ 5 56]]  #overall fraction of correct predictions\nlr.score(X, df_0910['Direction'])  0.625",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter4/exercise10/#e",
            "text": "#getting data ready\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#linear discriminant analysis (LDA)\nlda = LinearDiscriminantAnalysis()\nlda.fit(X,df_9908['Direction'])  LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n              solver='svd', store_covariance=False, tol=0.0001)  #getting data ready\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], lda.predict(X))\nprint(conf_mat)  [[ 9 34]\n [ 5 56]]  #overall fraction of correct predictions\n#it will be helpful for the next question\nlda.score(X, df_0910['Direction'])  0.625",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter4/exercise10/#f",
            "text": "#getting data ready\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#quadratic discriminant analysis (QDA)\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(X,df_9908['Direction'])  QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n               store_covariances=False, tol=0.0001)  #getting data ready\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], qda.predict(X))\nprint(conf_mat)  [[ 0 43]\n [ 0 61]]  #overall fraction of correct predictions\n#it will be helpful for the next question\nqda.score(X, df_0910['Direction'])  0.58653846153846156",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter4/exercise10/#g",
            "text": "#getting data ready\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#creating an instance of Neighbours Classifier and fitting the data\nnbrs = KNeighborsClassifier(n_neighbors=1)\nnbrs.fit(X,df_9908['Direction'])  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n           weights='uniform')  #getting data ready\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], nbrs.predict(X))\nprint(conf_mat)  [[21 22]\n [31 30]]  #overall fraction of correct predictions\n#it will be helpful for the next question\nnbrs.score(X, df_0910['Direction'])  0.49038461538461536",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter4/exercise10/#h",
            "text": "The methods that appear to provide the best results are the  logistic regression  and the  linear discriminant analysis (LDA) . They are the methods where score (overall fraction of correct predictions) is higher (0.625 vs 0.587 vs 0.490)",
            "title": "(h)"
        },
        {
            "location": "/sols/chapter4/exercise10/#i",
            "text": "#trying with a different number of neighbors\nn_nbrs = 10\n\n#getting data ready for the neigb\nX = df_9908['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#creating an instance of Neighbours Classifier and fitting the data\nnbrs = KNeighborsClassifier(n_neighbors=n_nbrs)\nnbrs.fit(X,df_9908['Direction'])\n\n#getting data ready for the confusion matrix\nX = df_0910['Lag2']\nX = X.reshape(np.shape(X)[0],1)\n\n#confusion matrix\nconf_mat = confusion_matrix(df_0910['Direction'], nbrs.predict(X))\nprint(conf_mat)  [[22 21]\n [24 37]]  Now, it's time to play a little bit with models. We leave this entertaining task to the reader, as homework.",
            "title": "(i)"
        },
        {
            "location": "/sols/chapter4/exercise10/#references",
            "text": "Raschka, S., 2014, Python Machine Learning",
            "title": "References"
        },
        {
            "location": "/sols/chapter4/exercise11/",
            "text": "Exercise 4.11\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis \nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.neighbors import KNeighborsClassifier \n\n%matplotlib inline\n\n\n\n\n/Users/disciplina/anaconda3/envs/islp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n\n\n\ndf = pd.read_csv('../data/Auto.csv')\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.0\n\n      \n8\n\n      \n307.0\n\n      \n130\n\n      \n3504\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.0\n\n      \n8\n\n      \n350.0\n\n      \n165\n\n      \n3693\n\n      \n11.5\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.0\n\n      \n8\n\n      \n318.0\n\n      \n150\n\n      \n3436\n\n      \n11.0\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.0\n\n      \n8\n\n      \n304.0\n\n      \n150\n\n      \n3433\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.0\n\n      \n8\n\n      \n302.0\n\n      \n140\n\n      \n3449\n\n      \n10.5\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\ndf['mpg01']= np.where(df['mpg'] > df['mpg'].median(), 1, 0)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n      \nmpg01\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.0\n\n      \n8\n\n      \n307.0\n\n      \n130\n\n      \n3504\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n15.0\n\n      \n8\n\n      \n350.0\n\n      \n165\n\n      \n3693\n\n      \n11.5\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n18.0\n\n      \n8\n\n      \n318.0\n\n      \n150\n\n      \n3436\n\n      \n11.0\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n      \n0\n\n    \n\n    \n\n      \n3\n\n      \n16.0\n\n      \n8\n\n      \n304.0\n\n      \n150\n\n      \n3433\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \n17.0\n\n      \n8\n\n      \n302.0\n\n      \n140\n\n      \n3449\n\n      \n10.5\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\ndf = df.drop('mpg', axis=1)\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n      \nmpg01\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n8\n\n      \n307.0\n\n      \n130\n\n      \n3504\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n8\n\n      \n350.0\n\n      \n165\n\n      \n3693\n\n      \n11.5\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n8\n\n      \n318.0\n\n      \n150\n\n      \n3436\n\n      \n11.0\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n      \n0\n\n    \n\n    \n\n      \n3\n\n      \n8\n\n      \n304.0\n\n      \n150\n\n      \n3433\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \n8\n\n      \n302.0\n\n      \n140\n\n      \n3449\n\n      \n10.5\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\n(b)\n\n\ng = sns.PairGrid(df, size=2)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.fig.set_size_inches(12, 12)\n\n\n\n\n/Users/disciplina/anaconda3/envs/islp/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'label', 'color'\n  s)\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \ncylinders\n\n      \ndisplacement\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nmpg01\n\n    \n\n  \n\n  \n\n    \n\n      \ncylinders\n\n      \n1.000000\n\n      \n0.950920\n\n      \n0.897017\n\n      \n-0.504061\n\n      \n-0.346717\n\n      \n-0.564972\n\n      \n-0.740327\n\n    \n\n    \n\n      \ndisplacement\n\n      \n0.950920\n\n      \n1.000000\n\n      \n0.933104\n\n      \n-0.544162\n\n      \n-0.369804\n\n      \n-0.610664\n\n      \n-0.738607\n\n    \n\n    \n\n      \nweight\n\n      \n0.897017\n\n      \n0.933104\n\n      \n1.000000\n\n      \n-0.419502\n\n      \n-0.307900\n\n      \n-0.581265\n\n      \n-0.745734\n\n    \n\n    \n\n      \nacceleration\n\n      \n-0.504061\n\n      \n-0.544162\n\n      \n-0.419502\n\n      \n1.000000\n\n      \n0.282901\n\n      \n0.210084\n\n      \n0.322629\n\n    \n\n    \n\n      \nyear\n\n      \n-0.346717\n\n      \n-0.369804\n\n      \n-0.307900\n\n      \n0.282901\n\n      \n1.000000\n\n      \n0.184314\n\n      \n0.454108\n\n    \n\n    \n\n      \norigin\n\n      \n-0.564972\n\n      \n-0.610664\n\n      \n-0.581265\n\n      \n0.210084\n\n      \n0.184314\n\n      \n1.000000\n\n      \n0.511393\n\n    \n\n    \n\n      \nmpg01\n\n      \n-0.740327\n\n      \n-0.738607\n\n      \n-0.745734\n\n      \n0.322629\n\n      \n0.454108\n\n      \n0.511393\n\n      \n1.000000\n\n    \n\n  \n\n\n\n\n\n\n\nFrom the scatterplots and the last line of the correlation matrix above, we see that several variables are highly correlated with \nmpg01\n, specially \ncylinders\n, \ndisplacement\n and \nweight\n. We take these three variables as the ones most associated with \nmpg01\n for the rest of the exercise.\n\n\n(c)\n\n\nx = df[['cylinders', 'displacement', 'weight']].values\ny = df['mpg01'].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n\n\n\n\n(d)\n\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(x_train, y_train)\naccuracy_score(y_test, lda.predict(x_test))\n\n\n\n\n0.89000000000000001\n\n\n\n(e)\n\n\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(x_train, y_train)\naccuracy_score(y_test, qda.predict(x_test))\n\n\n\n\n0.88\n\n\n\n(f)\n\n\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\naccuracy_score(y_test, lr.predict(x_test))\n\n\n\n\n0.87\n\n\n\n(g)\n\n\nfor K in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn.fit(x_train, y_train)\n    acc = accuracy_score(y_test, knn.predict(x_test))\n    print('K = {:3}, accuracy = {:.4f}'.format(K, acc))\n\n\n\n\nK =   1, accuracy = 0.9000\nK =   2, accuracy = 0.8800\nK =   3, accuracy = 0.9100\nK =   4, accuracy = 0.8900\nK =   5, accuracy = 0.8700\nK =   6, accuracy = 0.8900\nK =   7, accuracy = 0.9000\nK =   8, accuracy = 0.9100\nK =   9, accuracy = 0.8900\nK =  10, accuracy = 0.8900\nK =  11, accuracy = 0.8900\nK =  12, accuracy = 0.8800\nK =  13, accuracy = 0.8900\nK =  14, accuracy = 0.8900\nK =  15, accuracy = 0.8900\nK =  16, accuracy = 0.8900\nK =  17, accuracy = 0.8700\nK =  18, accuracy = 0.8900\nK =  19, accuracy = 0.8800\nK =  20, accuracy = 0.8900\nK =  21, accuracy = 0.8900\nK =  22, accuracy = 0.8900\nK =  23, accuracy = 0.8800\nK =  24, accuracy = 0.8800\nK =  25, accuracy = 0.8800\nK =  26, accuracy = 0.8800\nK =  27, accuracy = 0.8800\nK =  28, accuracy = 0.8800\nK =  29, accuracy = 0.8800\nK =  30, accuracy = 0.8800\nK =  31, accuracy = 0.8800\nK =  32, accuracy = 0.8900\nK =  33, accuracy = 0.8900\nK =  34, accuracy = 0.8900\nK =  35, accuracy = 0.8700\nK =  36, accuracy = 0.8800\nK =  37, accuracy = 0.8700\nK =  38, accuracy = 0.8800\nK =  39, accuracy = 0.8800\nK =  40, accuracy = 0.8800\nK =  41, accuracy = 0.8800\nK =  42, accuracy = 0.8800\nK =  43, accuracy = 0.8800\nK =  44, accuracy = 0.8800\nK =  45, accuracy = 0.8800\nK =  46, accuracy = 0.8800\nK =  47, accuracy = 0.8700\nK =  48, accuracy = 0.8800\nK =  49, accuracy = 0.8700\nK =  50, accuracy = 0.8700\nK =  51, accuracy = 0.8700\nK =  52, accuracy = 0.8700\nK =  53, accuracy = 0.8700\nK =  54, accuracy = 0.8700\nK =  55, accuracy = 0.8700\nK =  56, accuracy = 0.8700\nK =  57, accuracy = 0.8700\nK =  58, accuracy = 0.8700\nK =  59, accuracy = 0.8700\nK =  60, accuracy = 0.8700\nK =  61, accuracy = 0.8700\nK =  62, accuracy = 0.8700\nK =  63, accuracy = 0.8700\nK =  64, accuracy = 0.8700\nK =  65, accuracy = 0.8700\nK =  66, accuracy = 0.8700\nK =  67, accuracy = 0.8700\nK =  68, accuracy = 0.8700\nK =  69, accuracy = 0.8700\nK =  70, accuracy = 0.8700\nK =  71, accuracy = 0.8700\nK =  72, accuracy = 0.8700\nK =  73, accuracy = 0.8700\nK =  74, accuracy = 0.8800\nK =  75, accuracy = 0.8700\nK =  76, accuracy = 0.8800\nK =  77, accuracy = 0.8700\nK =  78, accuracy = 0.8800\nK =  79, accuracy = 0.8800\nK =  80, accuracy = 0.8800\nK =  81, accuracy = 0.8800\nK =  82, accuracy = 0.8800\nK =  83, accuracy = 0.8800\nK =  84, accuracy = 0.8800\nK =  85, accuracy = 0.8800\nK =  86, accuracy = 0.8800\nK =  87, accuracy = 0.8700\nK =  88, accuracy = 0.8800\nK =  89, accuracy = 0.8800\nK =  90, accuracy = 0.8800\nK =  91, accuracy = 0.8800\nK =  92, accuracy = 0.8900\nK =  93, accuracy = 0.8800\nK =  94, accuracy = 0.8800\nK =  95, accuracy = 0.8800\nK =  96, accuracy = 0.8900\nK =  97, accuracy = 0.8800\nK =  98, accuracy = 0.8800\nK =  99, accuracy = 0.8800\nK = 100, accuracy = 0.8800\n\n\n\nThe results above seem to indicated that the best values of K are somewhere around 1 and 22, although the difference in accuracy is not very large between different values of K.",
            "title": "4.11"
        },
        {
            "location": "/sols/chapter4/exercise11/#exercise-411",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis \nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.neighbors import KNeighborsClassifier \n\n%matplotlib inline  /Users/disciplina/anaconda3/envs/islp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)  df = pd.read_csv('../data/Auto.csv')  df.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n     \n   \n   \n     \n       0 \n       18.0 \n       8 \n       307.0 \n       130 \n       3504 \n       12.0 \n       70 \n       1 \n       chevrolet chevelle malibu \n     \n     \n       1 \n       15.0 \n       8 \n       350.0 \n       165 \n       3693 \n       11.5 \n       70 \n       1 \n       buick skylark 320 \n     \n     \n       2 \n       18.0 \n       8 \n       318.0 \n       150 \n       3436 \n       11.0 \n       70 \n       1 \n       plymouth satellite \n     \n     \n       3 \n       16.0 \n       8 \n       304.0 \n       150 \n       3433 \n       12.0 \n       70 \n       1 \n       amc rebel sst \n     \n     \n       4 \n       17.0 \n       8 \n       302.0 \n       140 \n       3449 \n       10.5 \n       70 \n       1 \n       ford torino",
            "title": "Exercise 4.11"
        },
        {
            "location": "/sols/chapter4/exercise11/#a",
            "text": "df['mpg01']= np.where(df['mpg'] > df['mpg'].median(), 1, 0)  df.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n       mpg01 \n     \n   \n   \n     \n       0 \n       18.0 \n       8 \n       307.0 \n       130 \n       3504 \n       12.0 \n       70 \n       1 \n       chevrolet chevelle malibu \n       0 \n     \n     \n       1 \n       15.0 \n       8 \n       350.0 \n       165 \n       3693 \n       11.5 \n       70 \n       1 \n       buick skylark 320 \n       0 \n     \n     \n       2 \n       18.0 \n       8 \n       318.0 \n       150 \n       3436 \n       11.0 \n       70 \n       1 \n       plymouth satellite \n       0 \n     \n     \n       3 \n       16.0 \n       8 \n       304.0 \n       150 \n       3433 \n       12.0 \n       70 \n       1 \n       amc rebel sst \n       0 \n     \n     \n       4 \n       17.0 \n       8 \n       302.0 \n       140 \n       3449 \n       10.5 \n       70 \n       1 \n       ford torino \n       0 \n     \n      df = df.drop('mpg', axis=1)\ndf.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n       mpg01 \n     \n   \n   \n     \n       0 \n       8 \n       307.0 \n       130 \n       3504 \n       12.0 \n       70 \n       1 \n       chevrolet chevelle malibu \n       0 \n     \n     \n       1 \n       8 \n       350.0 \n       165 \n       3693 \n       11.5 \n       70 \n       1 \n       buick skylark 320 \n       0 \n     \n     \n       2 \n       8 \n       318.0 \n       150 \n       3436 \n       11.0 \n       70 \n       1 \n       plymouth satellite \n       0 \n     \n     \n       3 \n       8 \n       304.0 \n       150 \n       3433 \n       12.0 \n       70 \n       1 \n       amc rebel sst \n       0 \n     \n     \n       4 \n       8 \n       302.0 \n       140 \n       3449 \n       10.5 \n       70 \n       1 \n       ford torino \n       0",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter4/exercise11/#b",
            "text": "g = sns.PairGrid(df, size=2)\ng.map_upper(plt.scatter, s=3)\ng.map_diag(plt.hist)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.fig.set_size_inches(12, 12)  /Users/disciplina/anaconda3/envs/islp/lib/python3.6/site-packages/matplotlib/contour.py:967: UserWarning: The following kwargs were not used by contour: 'label', 'color'\n  s)   df.corr()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       cylinders \n       displacement \n       weight \n       acceleration \n       year \n       origin \n       mpg01 \n     \n   \n   \n     \n       cylinders \n       1.000000 \n       0.950920 \n       0.897017 \n       -0.504061 \n       -0.346717 \n       -0.564972 \n       -0.740327 \n     \n     \n       displacement \n       0.950920 \n       1.000000 \n       0.933104 \n       -0.544162 \n       -0.369804 \n       -0.610664 \n       -0.738607 \n     \n     \n       weight \n       0.897017 \n       0.933104 \n       1.000000 \n       -0.419502 \n       -0.307900 \n       -0.581265 \n       -0.745734 \n     \n     \n       acceleration \n       -0.504061 \n       -0.544162 \n       -0.419502 \n       1.000000 \n       0.282901 \n       0.210084 \n       0.322629 \n     \n     \n       year \n       -0.346717 \n       -0.369804 \n       -0.307900 \n       0.282901 \n       1.000000 \n       0.184314 \n       0.454108 \n     \n     \n       origin \n       -0.564972 \n       -0.610664 \n       -0.581265 \n       0.210084 \n       0.184314 \n       1.000000 \n       0.511393 \n     \n     \n       mpg01 \n       -0.740327 \n       -0.738607 \n       -0.745734 \n       0.322629 \n       0.454108 \n       0.511393 \n       1.000000 \n     \n      From the scatterplots and the last line of the correlation matrix above, we see that several variables are highly correlated with  mpg01 , specially  cylinders ,  displacement  and  weight . We take these three variables as the ones most associated with  mpg01  for the rest of the exercise.",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter4/exercise11/#c",
            "text": "x = df[['cylinders', 'displacement', 'weight']].values\ny = df['mpg01'].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter4/exercise11/#d",
            "text": "lda = LinearDiscriminantAnalysis()\nlda.fit(x_train, y_train)\naccuracy_score(y_test, lda.predict(x_test))  0.89000000000000001",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter4/exercise11/#e",
            "text": "qda = QuadraticDiscriminantAnalysis()\nqda.fit(x_train, y_train)\naccuracy_score(y_test, qda.predict(x_test))  0.88",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter4/exercise11/#f",
            "text": "lr = LogisticRegression()\nlr.fit(x_train, y_train)\naccuracy_score(y_test, lr.predict(x_test))  0.87",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter4/exercise11/#g",
            "text": "for K in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn.fit(x_train, y_train)\n    acc = accuracy_score(y_test, knn.predict(x_test))\n    print('K = {:3}, accuracy = {:.4f}'.format(K, acc))  K =   1, accuracy = 0.9000\nK =   2, accuracy = 0.8800\nK =   3, accuracy = 0.9100\nK =   4, accuracy = 0.8900\nK =   5, accuracy = 0.8700\nK =   6, accuracy = 0.8900\nK =   7, accuracy = 0.9000\nK =   8, accuracy = 0.9100\nK =   9, accuracy = 0.8900\nK =  10, accuracy = 0.8900\nK =  11, accuracy = 0.8900\nK =  12, accuracy = 0.8800\nK =  13, accuracy = 0.8900\nK =  14, accuracy = 0.8900\nK =  15, accuracy = 0.8900\nK =  16, accuracy = 0.8900\nK =  17, accuracy = 0.8700\nK =  18, accuracy = 0.8900\nK =  19, accuracy = 0.8800\nK =  20, accuracy = 0.8900\nK =  21, accuracy = 0.8900\nK =  22, accuracy = 0.8900\nK =  23, accuracy = 0.8800\nK =  24, accuracy = 0.8800\nK =  25, accuracy = 0.8800\nK =  26, accuracy = 0.8800\nK =  27, accuracy = 0.8800\nK =  28, accuracy = 0.8800\nK =  29, accuracy = 0.8800\nK =  30, accuracy = 0.8800\nK =  31, accuracy = 0.8800\nK =  32, accuracy = 0.8900\nK =  33, accuracy = 0.8900\nK =  34, accuracy = 0.8900\nK =  35, accuracy = 0.8700\nK =  36, accuracy = 0.8800\nK =  37, accuracy = 0.8700\nK =  38, accuracy = 0.8800\nK =  39, accuracy = 0.8800\nK =  40, accuracy = 0.8800\nK =  41, accuracy = 0.8800\nK =  42, accuracy = 0.8800\nK =  43, accuracy = 0.8800\nK =  44, accuracy = 0.8800\nK =  45, accuracy = 0.8800\nK =  46, accuracy = 0.8800\nK =  47, accuracy = 0.8700\nK =  48, accuracy = 0.8800\nK =  49, accuracy = 0.8700\nK =  50, accuracy = 0.8700\nK =  51, accuracy = 0.8700\nK =  52, accuracy = 0.8700\nK =  53, accuracy = 0.8700\nK =  54, accuracy = 0.8700\nK =  55, accuracy = 0.8700\nK =  56, accuracy = 0.8700\nK =  57, accuracy = 0.8700\nK =  58, accuracy = 0.8700\nK =  59, accuracy = 0.8700\nK =  60, accuracy = 0.8700\nK =  61, accuracy = 0.8700\nK =  62, accuracy = 0.8700\nK =  63, accuracy = 0.8700\nK =  64, accuracy = 0.8700\nK =  65, accuracy = 0.8700\nK =  66, accuracy = 0.8700\nK =  67, accuracy = 0.8700\nK =  68, accuracy = 0.8700\nK =  69, accuracy = 0.8700\nK =  70, accuracy = 0.8700\nK =  71, accuracy = 0.8700\nK =  72, accuracy = 0.8700\nK =  73, accuracy = 0.8700\nK =  74, accuracy = 0.8800\nK =  75, accuracy = 0.8700\nK =  76, accuracy = 0.8800\nK =  77, accuracy = 0.8700\nK =  78, accuracy = 0.8800\nK =  79, accuracy = 0.8800\nK =  80, accuracy = 0.8800\nK =  81, accuracy = 0.8800\nK =  82, accuracy = 0.8800\nK =  83, accuracy = 0.8800\nK =  84, accuracy = 0.8800\nK =  85, accuracy = 0.8800\nK =  86, accuracy = 0.8800\nK =  87, accuracy = 0.8700\nK =  88, accuracy = 0.8800\nK =  89, accuracy = 0.8800\nK =  90, accuracy = 0.8800\nK =  91, accuracy = 0.8800\nK =  92, accuracy = 0.8900\nK =  93, accuracy = 0.8800\nK =  94, accuracy = 0.8800\nK =  95, accuracy = 0.8800\nK =  96, accuracy = 0.8900\nK =  97, accuracy = 0.8800\nK =  98, accuracy = 0.8800\nK =  99, accuracy = 0.8800\nK = 100, accuracy = 0.8800  The results above seem to indicated that the best values of K are somewhere around 1 and 22, although the difference in accuracy is not very large between different values of K.",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter4/exercise12/",
            "text": "Exercise 4.12\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\ndef Power():\n    print(2**3)\n\n\n\n\nPower()\n\n\n\n\n8\n\n\n\n(b)\n\n\ndef Power2(x,a):\n    print(x**a)\n\n\n\n\nPower2(3,8)\n\n\n\n\n6561\n\n\n\n(c)\n\n\nPower2(10,3)\n\n\n\n\n1000\n\n\n\nPower2(8,17)\n\n\n\n\n2251799813685248\n\n\n\nPower2(131,3)\n\n\n\n\n2248091\n\n\n\n(d)\n\n\ndef Power3(x,a):\n    result = x**a\n    return result\n\n\n\n\n(e)\n\n\ndef Plot(log=''):\n    x = np.arange(1,10)\n    y = Power3(x,2)\n\n    #create plot\n    fig, ax = plt.subplots()\n\n    #config plot\n    ax.set_xlabel('x')\n    ax.set_ylabel('y=x^2')\n    ax.set_title('Power3()')\n\n    #change scale according to axis\n    if log == 'x':\n        ax.set_xscale('log')\n        ax.set_xlabel('log(x)')\n    if log == 'y':\n        ax.set_yscale('log')\n        ax.set_ylabel('log(y=x^2)')\n    if log == 'xy':\n        ax.set_xscale('log')\n        ax.set_yscale('log')\n        ax.set_xlabel('log(x)')\n        ax.set_ylabel('log(y=x^2)')\n\n    #draw plot\n    ax.plot(x, y)\n\n\n\n\nPlot(log='xy')\n\n\n\n\n\n\n(f)\n\n\ndef PlotPower(start,end,power,log=''):\n    x = np.arange(start,end)\n    y = np.power(x,end)\n\n    #create plot\n    fig, ax = plt.subplots()\n\n    #config plot\n    ax.set_xlabel('x')\n    ax.set_ylabel('y=x^2')\n    ax.set_title('PlotPower()')\n\n    #change scale according to axis\n    if log == 'x':\n        ax.set_xscale('log')\n        ax.set_xlabel('log(x)')\n    if log == 'y':\n        ax.set_yscale('log')\n        ax.set_ylabel('log(y=x^2)')\n    if log == 'xy':\n        ax.set_xscale('log')\n        ax.set_yscale('log')\n        ax.set_xlabel('log(x)')\n        ax.set_ylabel('log(y=x^2)')\n\n    #draw plot\n    ax.plot(x, y)\n\n\n\n\nPlotPower(1,10,3)",
            "title": "4.12"
        },
        {
            "location": "/sols/chapter4/exercise12/#exercise-412",
            "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline",
            "title": "Exercise 4.12"
        },
        {
            "location": "/sols/chapter4/exercise12/#a",
            "text": "def Power():\n    print(2**3)  Power()  8",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter4/exercise12/#b",
            "text": "def Power2(x,a):\n    print(x**a)  Power2(3,8)  6561",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter4/exercise12/#c",
            "text": "Power2(10,3)  1000  Power2(8,17)  2251799813685248  Power2(131,3)  2248091",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter4/exercise12/#d",
            "text": "def Power3(x,a):\n    result = x**a\n    return result",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter4/exercise12/#e",
            "text": "def Plot(log=''):\n    x = np.arange(1,10)\n    y = Power3(x,2)\n\n    #create plot\n    fig, ax = plt.subplots()\n\n    #config plot\n    ax.set_xlabel('x')\n    ax.set_ylabel('y=x^2')\n    ax.set_title('Power3()')\n\n    #change scale according to axis\n    if log == 'x':\n        ax.set_xscale('log')\n        ax.set_xlabel('log(x)')\n    if log == 'y':\n        ax.set_yscale('log')\n        ax.set_ylabel('log(y=x^2)')\n    if log == 'xy':\n        ax.set_xscale('log')\n        ax.set_yscale('log')\n        ax.set_xlabel('log(x)')\n        ax.set_ylabel('log(y=x^2)')\n\n    #draw plot\n    ax.plot(x, y)  Plot(log='xy')",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter4/exercise12/#f",
            "text": "def PlotPower(start,end,power,log=''):\n    x = np.arange(start,end)\n    y = np.power(x,end)\n\n    #create plot\n    fig, ax = plt.subplots()\n\n    #config plot\n    ax.set_xlabel('x')\n    ax.set_ylabel('y=x^2')\n    ax.set_title('PlotPower()')\n\n    #change scale according to axis\n    if log == 'x':\n        ax.set_xscale('log')\n        ax.set_xlabel('log(x)')\n    if log == 'y':\n        ax.set_yscale('log')\n        ax.set_ylabel('log(y=x^2)')\n    if log == 'xy':\n        ax.set_xscale('log')\n        ax.set_yscale('log')\n        ax.set_xlabel('log(x)')\n        ax.set_ylabel('log(y=x^2)')\n\n    #draw plot\n    ax.plot(x, y)  PlotPower(1,10,3)",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter4/exercise13/",
            "text": "Exercise 4.13\n\n\nThis is similar to exercise 4.11. We define a new variable with the value of 1 if the crime rate is above the median and 0 if it is below.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis \nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.datasets import load_boston\n\n%matplotlib inline\n\n\n\n\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n    \n\n  \n\n\n\n\n\n\n\ndf['CRIM01'] = np.where(df['CRIM'] > df['CRIM'].median(), 1, 0)\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nCRIM01\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n0\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\ndf = df.drop('CRIM', axis=1)\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nCRIM01\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n0\n\n    \n\n    \n\n      \n3\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\nAs in 4.11, we try to assess which variables are more associated with 'CRIM01' by looking at correlations between the variables.\n\n\ndf.corr()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nCRIM01\n\n    \n\n  \n\n  \n\n    \n\n      \nZN\n\n      \n1.000000\n\n      \n-0.533828\n\n      \n-0.042697\n\n      \n-0.516604\n\n      \n0.311991\n\n      \n-0.569537\n\n      \n0.664408\n\n      \n-0.311948\n\n      \n-0.314563\n\n      \n-0.391679\n\n      \n0.175520\n\n      \n-0.412995\n\n      \n-0.436151\n\n    \n\n    \n\n      \nINDUS\n\n      \n-0.533828\n\n      \n1.000000\n\n      \n0.062938\n\n      \n0.763651\n\n      \n-0.391676\n\n      \n0.644779\n\n      \n-0.708027\n\n      \n0.595129\n\n      \n0.720760\n\n      \n0.383248\n\n      \n-0.356977\n\n      \n0.603800\n\n      \n0.603260\n\n    \n\n    \n\n      \nCHAS\n\n      \n-0.042697\n\n      \n0.062938\n\n      \n1.000000\n\n      \n0.091203\n\n      \n0.091251\n\n      \n0.086518\n\n      \n-0.099176\n\n      \n-0.007368\n\n      \n-0.035587\n\n      \n-0.121515\n\n      \n0.048788\n\n      \n-0.053929\n\n      \n0.070097\n\n    \n\n    \n\n      \nNOX\n\n      \n-0.516604\n\n      \n0.763651\n\n      \n0.091203\n\n      \n1.000000\n\n      \n-0.302188\n\n      \n0.731470\n\n      \n-0.769230\n\n      \n0.611441\n\n      \n0.668023\n\n      \n0.188933\n\n      \n-0.380051\n\n      \n0.590879\n\n      \n0.723235\n\n    \n\n    \n\n      \nRM\n\n      \n0.311991\n\n      \n-0.391676\n\n      \n0.091251\n\n      \n-0.302188\n\n      \n1.000000\n\n      \n-0.240265\n\n      \n0.205246\n\n      \n-0.209847\n\n      \n-0.292048\n\n      \n-0.355501\n\n      \n0.128069\n\n      \n-0.613808\n\n      \n-0.156372\n\n    \n\n    \n\n      \nAGE\n\n      \n-0.569537\n\n      \n0.644779\n\n      \n0.086518\n\n      \n0.731470\n\n      \n-0.240265\n\n      \n1.000000\n\n      \n-0.747881\n\n      \n0.456022\n\n      \n0.506456\n\n      \n0.261515\n\n      \n-0.273534\n\n      \n0.602339\n\n      \n0.613940\n\n    \n\n    \n\n      \nDIS\n\n      \n0.664408\n\n      \n-0.708027\n\n      \n-0.099176\n\n      \n-0.769230\n\n      \n0.205246\n\n      \n-0.747881\n\n      \n1.000000\n\n      \n-0.494588\n\n      \n-0.534432\n\n      \n-0.232471\n\n      \n0.291512\n\n      \n-0.496996\n\n      \n-0.616342\n\n    \n\n    \n\n      \nRAD\n\n      \n-0.311948\n\n      \n0.595129\n\n      \n-0.007368\n\n      \n0.611441\n\n      \n-0.209847\n\n      \n0.456022\n\n      \n-0.494588\n\n      \n1.000000\n\n      \n0.910228\n\n      \n0.464741\n\n      \n-0.444413\n\n      \n0.488676\n\n      \n0.619786\n\n    \n\n    \n\n      \nTAX\n\n      \n-0.314563\n\n      \n0.720760\n\n      \n-0.035587\n\n      \n0.668023\n\n      \n-0.292048\n\n      \n0.506456\n\n      \n-0.534432\n\n      \n0.910228\n\n      \n1.000000\n\n      \n0.460853\n\n      \n-0.441808\n\n      \n0.543993\n\n      \n0.608741\n\n    \n\n    \n\n      \nPTRATIO\n\n      \n-0.391679\n\n      \n0.383248\n\n      \n-0.121515\n\n      \n0.188933\n\n      \n-0.355501\n\n      \n0.261515\n\n      \n-0.232471\n\n      \n0.464741\n\n      \n0.460853\n\n      \n1.000000\n\n      \n-0.177383\n\n      \n0.374044\n\n      \n0.253568\n\n    \n\n    \n\n      \nB\n\n      \n0.175520\n\n      \n-0.356977\n\n      \n0.048788\n\n      \n-0.380051\n\n      \n0.128069\n\n      \n-0.273534\n\n      \n0.291512\n\n      \n-0.444413\n\n      \n-0.441808\n\n      \n-0.177383\n\n      \n1.000000\n\n      \n-0.366087\n\n      \n-0.351211\n\n    \n\n    \n\n      \nLSTAT\n\n      \n-0.412995\n\n      \n0.603800\n\n      \n-0.053929\n\n      \n0.590879\n\n      \n-0.613808\n\n      \n0.602339\n\n      \n-0.496996\n\n      \n0.488676\n\n      \n0.543993\n\n      \n0.374044\n\n      \n-0.366087\n\n      \n1.000000\n\n      \n0.453263\n\n    \n\n    \n\n      \nCRIM01\n\n      \n-0.436151\n\n      \n0.603260\n\n      \n0.070097\n\n      \n0.723235\n\n      \n-0.156372\n\n      \n0.613940\n\n      \n-0.616342\n\n      \n0.619786\n\n      \n0.608741\n\n      \n0.253568\n\n      \n-0.351211\n\n      \n0.453263\n\n      \n1.000000\n\n    \n\n  \n\n\n\n\n\n\n\nx_all = df.iloc[:,:-1].values\nx_6 = df[['INDUS','NOX','AGE','DIS','RAD','TAX']].values\ny = df['CRIM01'].values\n\nx_all_train, x_all_test, y_train, y_test = train_test_split(x_all, y, random_state=1)\nx_6_train, x_6_test, y_train, y_test = train_test_split(x_6, y, random_state=1)\n\n\n\n\nlr = LogisticRegression()\nlr.fit(x_6_train, y_train)\nprint(\"Accuracy with x_6  :\",accuracy_score(y_test, lr.predict(x_6_test)))\nlr.fit(x_all_train, y_train)\nprint(\"Accuracy with x_all:\",accuracy_score(y_test, lr.predict(x_all_test)))\n\n\n\n\nAccuracy with x_6  : 0.842519685039\nAccuracy with x_all: 0.858267716535\n\n\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(x_6_train, y_train)\nprint(\"Accuracy with x_6  :\",accuracy_score(y_test, lda.predict(x_6_test)))\nlda.fit(x_all_train, y_train)\nprint(\"Accuracy with x_all:\",accuracy_score(y_test, lda.predict(x_all_test)))\n\n\n\n\nAccuracy with x_6  : 0.818897637795\nAccuracy with x_all: 0.834645669291\n\n\n\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(x_6_train, y_train)\nprint(\"Accuracy with x_6  :\",accuracy_score(y_test, qda.predict(x_6_test)))\nqda.fit(x_all_train, y_train)\nprint(\"Accuracy with x_all:\",accuracy_score(y_test, qda.predict(x_all_test)))\n\n\n\n\nAccuracy with x_6  : 0.874015748031\nAccuracy with x_all: 0.905511811024\n\n\n\nbest_acc = 0\nbest_k = 0\nfor K in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn.fit(x_all_train, y_train)\n    acc = accuracy_score(y_test, knn.predict(x_all_test))\n    if acc > best_acc:\n        best_acc, best_k = acc, K\n\nprint('Best accuracy = {:.4f} with K = {:3}'.format(best_acc, best_k))\n\n\n\n\nBest accuracy = 0.9370 with K =   1\n\n\n\nbest_acc = 0\nbest_k = 0\nfor K in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn.fit(x_6_train, y_train)\n    acc = accuracy_score(y_test, knn.predict(x_6_test))\n    if acc > best_acc:\n        best_acc, best_k = acc, K\n\nprint('Best accuracy = {:.4f} with K = {:3}'.format(best_acc, best_k))\n\n\n\n\nBest accuracy = 0.9370 with K =   1\n\n\n\nWe used two subsets of the predictors: all of them and the 6 most correlated with the response. From the results above, we can see that including all the predictors improves the accuracy (except for KNN) but not by much (less than 3%). QDA has better accuracy test performance than LDA, indicating a non-linear Bayes decision boundary. The best model was KNN with K = 1. For a more robust analysis we should perform cross-validation. Which leads us to the next chapter :)",
            "title": "4.13"
        },
        {
            "location": "/sols/chapter4/exercise13/#exercise-413",
            "text": "This is similar to exercise 4.11. We define a new variable with the value of 1 if the crime rate is above the median and 0 if it is below.  import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis \nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.datasets import load_boston\n\n%matplotlib inline  boston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n     \n      df['CRIM01'] = np.where(df['CRIM'] > df['CRIM'].median(), 1, 0)\ndf.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       CRIM01 \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       0 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       0 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       0 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       0 \n     \n      df = df.drop('CRIM', axis=1)\ndf.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       CRIM01 \n     \n   \n   \n     \n       0 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       0 \n     \n     \n       1 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       0 \n     \n     \n       2 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       0 \n     \n     \n       3 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       0 \n     \n     \n       4 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       0 \n     \n      As in 4.11, we try to assess which variables are more associated with 'CRIM01' by looking at correlations between the variables.  df.corr()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       CRIM01 \n     \n   \n   \n     \n       ZN \n       1.000000 \n       -0.533828 \n       -0.042697 \n       -0.516604 \n       0.311991 \n       -0.569537 \n       0.664408 \n       -0.311948 \n       -0.314563 \n       -0.391679 \n       0.175520 \n       -0.412995 \n       -0.436151 \n     \n     \n       INDUS \n       -0.533828 \n       1.000000 \n       0.062938 \n       0.763651 \n       -0.391676 \n       0.644779 \n       -0.708027 \n       0.595129 \n       0.720760 \n       0.383248 \n       -0.356977 \n       0.603800 \n       0.603260 \n     \n     \n       CHAS \n       -0.042697 \n       0.062938 \n       1.000000 \n       0.091203 \n       0.091251 \n       0.086518 \n       -0.099176 \n       -0.007368 \n       -0.035587 \n       -0.121515 \n       0.048788 \n       -0.053929 \n       0.070097 \n     \n     \n       NOX \n       -0.516604 \n       0.763651 \n       0.091203 \n       1.000000 \n       -0.302188 \n       0.731470 \n       -0.769230 \n       0.611441 \n       0.668023 \n       0.188933 \n       -0.380051 \n       0.590879 \n       0.723235 \n     \n     \n       RM \n       0.311991 \n       -0.391676 \n       0.091251 \n       -0.302188 \n       1.000000 \n       -0.240265 \n       0.205246 \n       -0.209847 \n       -0.292048 \n       -0.355501 \n       0.128069 \n       -0.613808 \n       -0.156372 \n     \n     \n       AGE \n       -0.569537 \n       0.644779 \n       0.086518 \n       0.731470 \n       -0.240265 \n       1.000000 \n       -0.747881 \n       0.456022 \n       0.506456 \n       0.261515 \n       -0.273534 \n       0.602339 \n       0.613940 \n     \n     \n       DIS \n       0.664408 \n       -0.708027 \n       -0.099176 \n       -0.769230 \n       0.205246 \n       -0.747881 \n       1.000000 \n       -0.494588 \n       -0.534432 \n       -0.232471 \n       0.291512 \n       -0.496996 \n       -0.616342 \n     \n     \n       RAD \n       -0.311948 \n       0.595129 \n       -0.007368 \n       0.611441 \n       -0.209847 \n       0.456022 \n       -0.494588 \n       1.000000 \n       0.910228 \n       0.464741 \n       -0.444413 \n       0.488676 \n       0.619786 \n     \n     \n       TAX \n       -0.314563 \n       0.720760 \n       -0.035587 \n       0.668023 \n       -0.292048 \n       0.506456 \n       -0.534432 \n       0.910228 \n       1.000000 \n       0.460853 \n       -0.441808 \n       0.543993 \n       0.608741 \n     \n     \n       PTRATIO \n       -0.391679 \n       0.383248 \n       -0.121515 \n       0.188933 \n       -0.355501 \n       0.261515 \n       -0.232471 \n       0.464741 \n       0.460853 \n       1.000000 \n       -0.177383 \n       0.374044 \n       0.253568 \n     \n     \n       B \n       0.175520 \n       -0.356977 \n       0.048788 \n       -0.380051 \n       0.128069 \n       -0.273534 \n       0.291512 \n       -0.444413 \n       -0.441808 \n       -0.177383 \n       1.000000 \n       -0.366087 \n       -0.351211 \n     \n     \n       LSTAT \n       -0.412995 \n       0.603800 \n       -0.053929 \n       0.590879 \n       -0.613808 \n       0.602339 \n       -0.496996 \n       0.488676 \n       0.543993 \n       0.374044 \n       -0.366087 \n       1.000000 \n       0.453263 \n     \n     \n       CRIM01 \n       -0.436151 \n       0.603260 \n       0.070097 \n       0.723235 \n       -0.156372 \n       0.613940 \n       -0.616342 \n       0.619786 \n       0.608741 \n       0.253568 \n       -0.351211 \n       0.453263 \n       1.000000 \n     \n      x_all = df.iloc[:,:-1].values\nx_6 = df[['INDUS','NOX','AGE','DIS','RAD','TAX']].values\ny = df['CRIM01'].values\n\nx_all_train, x_all_test, y_train, y_test = train_test_split(x_all, y, random_state=1)\nx_6_train, x_6_test, y_train, y_test = train_test_split(x_6, y, random_state=1)  lr = LogisticRegression()\nlr.fit(x_6_train, y_train)\nprint(\"Accuracy with x_6  :\",accuracy_score(y_test, lr.predict(x_6_test)))\nlr.fit(x_all_train, y_train)\nprint(\"Accuracy with x_all:\",accuracy_score(y_test, lr.predict(x_all_test)))  Accuracy with x_6  : 0.842519685039\nAccuracy with x_all: 0.858267716535  lda = LinearDiscriminantAnalysis()\nlda.fit(x_6_train, y_train)\nprint(\"Accuracy with x_6  :\",accuracy_score(y_test, lda.predict(x_6_test)))\nlda.fit(x_all_train, y_train)\nprint(\"Accuracy with x_all:\",accuracy_score(y_test, lda.predict(x_all_test)))  Accuracy with x_6  : 0.818897637795\nAccuracy with x_all: 0.834645669291  qda = QuadraticDiscriminantAnalysis()\nqda.fit(x_6_train, y_train)\nprint(\"Accuracy with x_6  :\",accuracy_score(y_test, qda.predict(x_6_test)))\nqda.fit(x_all_train, y_train)\nprint(\"Accuracy with x_all:\",accuracy_score(y_test, qda.predict(x_all_test)))  Accuracy with x_6  : 0.874015748031\nAccuracy with x_all: 0.905511811024  best_acc = 0\nbest_k = 0\nfor K in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn.fit(x_all_train, y_train)\n    acc = accuracy_score(y_test, knn.predict(x_all_test))\n    if acc > best_acc:\n        best_acc, best_k = acc, K\n\nprint('Best accuracy = {:.4f} with K = {:3}'.format(best_acc, best_k))  Best accuracy = 0.9370 with K =   1  best_acc = 0\nbest_k = 0\nfor K in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn.fit(x_6_train, y_train)\n    acc = accuracy_score(y_test, knn.predict(x_6_test))\n    if acc > best_acc:\n        best_acc, best_k = acc, K\n\nprint('Best accuracy = {:.4f} with K = {:3}'.format(best_acc, best_k))  Best accuracy = 0.9370 with K =   1  We used two subsets of the predictors: all of them and the 6 most correlated with the response. From the results above, we can see that including all the predictors improves the accuracy (except for KNN) but not by much (less than 3%). QDA has better accuracy test performance than LDA, indicating a non-linear Bayes decision boundary. The best model was KNN with K = 1. For a more robust analysis we should perform cross-validation. Which leads us to the next chapter :)",
            "title": "Exercise 4.13"
        },
        {
            "location": "/sols/chapter5/exercise1/",
            "text": "Exercise 5.1\n\n\nWe will use the following properties of the variance:\n\n\n\n\n\\begin{align}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\Var(X + Y)    & = \\Var(X)+\\Var(Y) + 2\\Cov(X,Y)\\\\\n\\Var(\\alpha X) & = \\alpha^2 \\Var(X) \\\\\n\\Cov(\\alpha X, \\beta Y) & = \\alpha \\beta \\Cov(X,Y)\n\\end{align}\n\n\n\\begin{align}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\Var(X + Y)    & = \\Var(X)+\\Var(Y) + 2\\Cov(X,Y)\\\\\n\\Var(\\alpha X) & = \\alpha^2 \\Var(X) \\\\\n\\Cov(\\alpha X, \\beta Y) & = \\alpha \\beta \\Cov(X,Y)\n\\end{align}\n\n\n\n\nWith these properties, we get:\n\n\n\n\n\\begin{align}\n\\Var(\\alpha X + (1-\\alpha) Y) & = \\Var(\\alpha X)   + \\Var((1-\\alpha)Y)    + 2\\Cov(\\alpha X,(1-\\alpha) Y)\\\\\n\\                             & = \\alpha^2 \\Var(X) + (1-\\alpha)^2 \\Var(Y) + 2\\alpha\\beta\\Cov(X,Y)\n\\end{align}\n\n\n\\begin{align}\n\\Var(\\alpha X + (1-\\alpha) Y) & = \\Var(\\alpha X)   + \\Var((1-\\alpha)Y)    + 2\\Cov(\\alpha X,(1-\\alpha) Y)\\\\\n\\                             & = \\alpha^2 \\Var(X) + (1-\\alpha)^2 \\Var(Y) + 2\\alpha\\beta\\Cov(X,Y)\n\\end{align}\n\n\n\n\nNow, we should find the minimum in the expression above. To do so, we differentiate the expression, equal it to 0 and solve for \n\\alpha\n\\alpha\n:\n\n\n\n\n\\begin{align}\n\\  0 & = 2 \\alpha\\Var(X) - 2(1-\\alpha) \\Var(Y)  + 2(1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   0 & = \\alpha\\Var(X) - (1-\\alpha) \\Var(Y)  + (1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha\\Var(X) + \\alpha\\Var(Y) - 2\\alpha\\Cov(X,Y) & =  \\Var(Y)  -\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha & =  \\frac{\\Var(Y)  -\\Cov(X,Y)}{\\Var(X) + \\Var(Y) - 2\\Cov(X,Y)} = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_X\\sigma_Y}\\\\\n\\end{align}\n\n\n\\begin{align}\n\\  0 & = 2 \\alpha\\Var(X) - 2(1-\\alpha) \\Var(Y)  + 2(1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   0 & = \\alpha\\Var(X) - (1-\\alpha) \\Var(Y)  + (1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha\\Var(X) + \\alpha\\Var(Y) - 2\\alpha\\Cov(X,Y) & =  \\Var(Y)  -\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha & =  \\frac{\\Var(Y)  -\\Cov(X,Y)}{\\Var(X) + \\Var(Y) - 2\\Cov(X,Y)} = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_X\\sigma_Y}\\\\\n\\end{align}",
            "title": "5.1"
        },
        {
            "location": "/sols/chapter5/exercise1/#exercise-51",
            "text": "We will use the following properties of the variance:   \\begin{align}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\Var(X + Y)    & = \\Var(X)+\\Var(Y) + 2\\Cov(X,Y)\\\\\n\\Var(\\alpha X) & = \\alpha^2 \\Var(X) \\\\\n\\Cov(\\alpha X, \\beta Y) & = \\alpha \\beta \\Cov(X,Y)\n\\end{align}  \\begin{align}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\Var(X + Y)    & = \\Var(X)+\\Var(Y) + 2\\Cov(X,Y)\\\\\n\\Var(\\alpha X) & = \\alpha^2 \\Var(X) \\\\\n\\Cov(\\alpha X, \\beta Y) & = \\alpha \\beta \\Cov(X,Y)\n\\end{align}   With these properties, we get:   \\begin{align}\n\\Var(\\alpha X + (1-\\alpha) Y) & = \\Var(\\alpha X)   + \\Var((1-\\alpha)Y)    + 2\\Cov(\\alpha X,(1-\\alpha) Y)\\\\\n\\                             & = \\alpha^2 \\Var(X) + (1-\\alpha)^2 \\Var(Y) + 2\\alpha\\beta\\Cov(X,Y)\n\\end{align}  \\begin{align}\n\\Var(\\alpha X + (1-\\alpha) Y) & = \\Var(\\alpha X)   + \\Var((1-\\alpha)Y)    + 2\\Cov(\\alpha X,(1-\\alpha) Y)\\\\\n\\                             & = \\alpha^2 \\Var(X) + (1-\\alpha)^2 \\Var(Y) + 2\\alpha\\beta\\Cov(X,Y)\n\\end{align}   Now, we should find the minimum in the expression above. To do so, we differentiate the expression, equal it to 0 and solve for  \\alpha \\alpha :   \\begin{align}\n\\  0 & = 2 \\alpha\\Var(X) - 2(1-\\alpha) \\Var(Y)  + 2(1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   0 & = \\alpha\\Var(X) - (1-\\alpha) \\Var(Y)  + (1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha\\Var(X) + \\alpha\\Var(Y) - 2\\alpha\\Cov(X,Y) & =  \\Var(Y)  -\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha & =  \\frac{\\Var(Y)  -\\Cov(X,Y)}{\\Var(X) + \\Var(Y) - 2\\Cov(X,Y)} = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_X\\sigma_Y}\\\\\n\\end{align}  \\begin{align}\n\\  0 & = 2 \\alpha\\Var(X) - 2(1-\\alpha) \\Var(Y)  + 2(1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   0 & = \\alpha\\Var(X) - (1-\\alpha) \\Var(Y)  + (1-2\\alpha)\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha\\Var(X) + \\alpha\\Var(Y) - 2\\alpha\\Cov(X,Y) & =  \\Var(Y)  -\\Cov(X,Y) \\Leftrightarrow \\\\\n\\ \\Leftrightarrow   \\alpha & =  \\frac{\\Var(Y)  -\\Cov(X,Y)}{\\Var(X) + \\Var(Y) - 2\\Cov(X,Y)} = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_X\\sigma_Y}\\\\\n\\end{align}",
            "title": "Exercise 5.1"
        },
        {
            "location": "/sols/chapter5/exercise2/",
            "text": "Exercise 5.2\n\n\nTo solve this exercise, it is important to remember that bootstrap samples have \nreplacement\n.\n\n\n(a)\n\n\nThe probability that the first bootstrap observation is not the jth observation is:\n\n\n\n\n\\frac{n-1}{n}.\n\n\n\\frac{n-1}{n}.\n\n\n\n\nWhy? Since all observations have the same chance of being chosen (equiprobable events), the probability we are looking for is just the number of favorable cases dividing by the total number of possible cases. That is, \nn-1\nn-1\n observations other than the \nj\nj\nth observation dividing by the \nn\nn\n total observations.\n\n\n(b)\n\n\nThe probability that the second bootstrap observation is not the jth observation is also:\n\n\n\n\n\\frac{n-1}{n}\n\n\n\\frac{n-1}{n}\n\n\n\n\nSince we sample with replacement, everything stays the same as in (a). So, when we go for the second bootstrap observation, our set of observations is composed by all the initial observations. Thus, from \nn\n possible observations, \nn-1\n observations correspond to the probability that we want to compute (probability that the second observation is not the jth observation).\n\n\n(c)\n\n\nWe already saw that the probability of \nnot\n taking the jth observation is \n\\frac{n-1}{n}\n\\frac{n-1}{n}\n. Now, we want to the probability of not having the jth observation in the boostrap sample. In other words, we are saying that we don't get the jth observation in the first boostrap observation, nor in the second, nor in the third and so one.\n\n\nCrucially, these events are independent. That is, the probability of picking the \nj\nj\nth observation in, say, the second observation does not depend on whether it was chosen on the first, and so on. This means, we need only multiply the probabilities of all events. Let's consider that our boostrap sample has size equal to the number of observations \nn\n. If so, the probability of \nnot\n taking the jth observation is:\n\n\n\n\n (\\frac{n-1}{n})_1 \\times (\\frac{n-1}{n})_2 \\times \\ldots \\times (\\frac{n-1}{n})_n = \\left(\\frac{n-1}{n}\\right)^n  = \\left(1 - \\frac{1}{n}\\right)^n \n\n\n (\\frac{n-1}{n})_1 \\times (\\frac{n-1}{n})_2 \\times \\ldots \\times (\\frac{n-1}{n})_n = \\left(\\frac{n-1}{n}\\right)^n  = \\left(1 - \\frac{1}{n}\\right)^n \n\n\n\n\n(d)\n\n\nHaving the \nj\nj\nth observation in the bootstrap sample is the complementary event of \nnot\n having the jth observation in any of the bootstrap samples. \nThis means, that the probabiliy of the \nj\nj\nth observation \nbeing in the bootstrap sample\n (\nPr(in)\n) is one minus the probability of \nnot being in the bootstrap sample\n (\nPr(out)\n).\n\n\nUsing the general expression deducted in (c) and considering that \nn = 5\nn = 5\n, it follows that:\n\n\n\n\n Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{5}\\right)^5 \\approx 0.67232.\n\n\n Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{5}\\right)^5 \\approx 0.67232.\n\n\n\n\n(e)\n\n\nThis exercise is similar to (d), the only difference is that now we have \nn=100\nn=100\n. Applying the same reasoning as before:\n\n\n\n\n Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{100}\\right)^{100} \\approx 0.63397. \n\n\n Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{100}\\right)^{100} \\approx 0.63397. \n\n\n\n\n(f)\n\n\nSimilar to (d) and (e):\n\n\n\n\n Pr(in) = 1 - Pr(out)  = 1 - \\left(1- \\frac{1}{10000}\\right)^{10000}  \\approx 0.63231 \n\n\n Pr(in) = 1 - Pr(out)  = 1 - \\left(1- \\frac{1}{10000}\\right)^{10000}  \\approx 0.63231 \n\n\n\n\n(g)\n\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\nn = 100000   # Number of observations\nprob = [1 - (1-1/n)**n for n in range(1, n)]\n\nplt.scatter(range(1,n),prob);\nplt.xlim(0, n);\nplt.ylim(.5,1);\n\n\n\n\n\n\nAs we can see, the first point has probability equal to one, which makes sense because if we only have on observation it must be in the bootstrap sample.\nAfter, the probability starts decreasing until it reaches a plateau close to the value 0.63.\n\n\nThe limit of the probability when \nn\nn\n goes to infinity is well known:\n\n\n\n\n \\lim_{n\\to\\infty} 1-\\left(\\frac{n-1}{n}\\right)^n \\Leftrightarrow 1 -  \\lim_{n\\to\\infty} \\left(1-\\frac{1}{n}\\right)^n = 1 - e^{-1} \\approx 1 - 0.36788 = 0.63212.\n\n\n \\lim_{n\\to\\infty} 1-\\left(\\frac{n-1}{n}\\right)^n \\Leftrightarrow 1 -  \\lim_{n\\to\\infty} \\left(1-\\frac{1}{n}\\right)^n = 1 - e^{-1} \\approx 1 - 0.36788 = 0.63212.\n\n\n\n\nLet's try zooming in the plot above for smaller values of \nn\nn\n so we can better observe the approach to the asymptote.\n\n\nn = 500   # Number of observations\nprob = [1 - (1-1/n)**n for n in range(1, n)]\n\nplt.scatter(range(1,n),prob);\nplt.xlim(0, n);\nplt.ylim(.6,.7);\n\n\n\n\n\n\n(h)\n\n\nFrom (e) we know this value is approximately 0.63397.\n\n\nSince we are solving it in Python, we will adapt the code proposed in the statement of the exercise.\n\n\nimport random\n\nn = 100\nN = 10000\ncount = 0\n\nfor _ in range(N):\n    # True equals 1 in Python\n    # choices(l, k=k) takes k samples from the list l with replacement\n    count += 4 in random.choices(range(1,n+1), k=n) \n\nprint(count/N)\n\n\n\n\n0.6316\n\n\n\nAs expected, the value found is close to 0.63397, the value given by the formula. But it is an underestimate.\nIf we repeat the calculation several times, we would expect approximately half the estimates to be an underestimate, and the other half to be an overestimate.\nLet's try it.\nSo, for fun, we redo the calculation above a few more times (now with numpy) and do confirm this from the histogram below.\n\n\nimport numpy as np\nx = []\n\nfor i in range(100):\n    # https://stackoverflow.com/questions/367565/how-do-i-build-a-numpy-array-from-a-generator\n    a = np.fromiter((4 in np.random.choice(100, 100, replace=True) for _ in range(N)), dtype=bool)\n    x.append(np.mean(a)) \n\nplt.hist(x)\n\n\n\n\n(array([  1.,   3.,   5.,  16.,  18.,  17.,  18.,  14.,   4.,   4.]),\n array([ 0.6202,  0.623 ,  0.6258,  0.6286,  0.6314,  0.6342,  0.637 ,\n         0.6398,  0.6426,  0.6454,  0.6482]),\n <a list of 10 Patch objects>)",
            "title": "5.2"
        },
        {
            "location": "/sols/chapter5/exercise2/#exercise-52",
            "text": "To solve this exercise, it is important to remember that bootstrap samples have  replacement .",
            "title": "Exercise 5.2"
        },
        {
            "location": "/sols/chapter5/exercise2/#a",
            "text": "The probability that the first bootstrap observation is not the jth observation is:   \\frac{n-1}{n}.  \\frac{n-1}{n}.   Why? Since all observations have the same chance of being chosen (equiprobable events), the probability we are looking for is just the number of favorable cases dividing by the total number of possible cases. That is,  n-1 n-1  observations other than the  j j th observation dividing by the  n n  total observations.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter5/exercise2/#b",
            "text": "The probability that the second bootstrap observation is not the jth observation is also:   \\frac{n-1}{n}  \\frac{n-1}{n}   Since we sample with replacement, everything stays the same as in (a). So, when we go for the second bootstrap observation, our set of observations is composed by all the initial observations. Thus, from  n  possible observations,  n-1  observations correspond to the probability that we want to compute (probability that the second observation is not the jth observation).",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter5/exercise2/#c",
            "text": "We already saw that the probability of  not  taking the jth observation is  \\frac{n-1}{n} \\frac{n-1}{n} . Now, we want to the probability of not having the jth observation in the boostrap sample. In other words, we are saying that we don't get the jth observation in the first boostrap observation, nor in the second, nor in the third and so one.  Crucially, these events are independent. That is, the probability of picking the  j j th observation in, say, the second observation does not depend on whether it was chosen on the first, and so on. This means, we need only multiply the probabilities of all events. Let's consider that our boostrap sample has size equal to the number of observations  n . If so, the probability of  not  taking the jth observation is:    (\\frac{n-1}{n})_1 \\times (\\frac{n-1}{n})_2 \\times \\ldots \\times (\\frac{n-1}{n})_n = \\left(\\frac{n-1}{n}\\right)^n  = \\left(1 - \\frac{1}{n}\\right)^n    (\\frac{n-1}{n})_1 \\times (\\frac{n-1}{n})_2 \\times \\ldots \\times (\\frac{n-1}{n})_n = \\left(\\frac{n-1}{n}\\right)^n  = \\left(1 - \\frac{1}{n}\\right)^n",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter5/exercise2/#d",
            "text": "Having the  j j th observation in the bootstrap sample is the complementary event of  not  having the jth observation in any of the bootstrap samples. \nThis means, that the probabiliy of the  j j th observation  being in the bootstrap sample  ( Pr(in) ) is one minus the probability of  not being in the bootstrap sample  ( Pr(out) ).  Using the general expression deducted in (c) and considering that  n = 5 n = 5 , it follows that:    Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{5}\\right)^5 \\approx 0.67232.   Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{5}\\right)^5 \\approx 0.67232.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter5/exercise2/#e",
            "text": "This exercise is similar to (d), the only difference is that now we have  n=100 n=100 . Applying the same reasoning as before:    Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{100}\\right)^{100} \\approx 0.63397.    Pr(in) = 1 - Pr(out) = 1 - \\left(1-\\frac{1}{100}\\right)^{100} \\approx 0.63397.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter5/exercise2/#f",
            "text": "Similar to (d) and (e):    Pr(in) = 1 - Pr(out)  = 1 - \\left(1- \\frac{1}{10000}\\right)^{10000}  \\approx 0.63231    Pr(in) = 1 - Pr(out)  = 1 - \\left(1- \\frac{1}{10000}\\right)^{10000}  \\approx 0.63231",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter5/exercise2/#g",
            "text": "import matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\nn = 100000   # Number of observations\nprob = [1 - (1-1/n)**n for n in range(1, n)]\n\nplt.scatter(range(1,n),prob);\nplt.xlim(0, n);\nplt.ylim(.5,1);   As we can see, the first point has probability equal to one, which makes sense because if we only have on observation it must be in the bootstrap sample.\nAfter, the probability starts decreasing until it reaches a plateau close to the value 0.63.  The limit of the probability when  n n  goes to infinity is well known:    \\lim_{n\\to\\infty} 1-\\left(\\frac{n-1}{n}\\right)^n \\Leftrightarrow 1 -  \\lim_{n\\to\\infty} \\left(1-\\frac{1}{n}\\right)^n = 1 - e^{-1} \\approx 1 - 0.36788 = 0.63212.   \\lim_{n\\to\\infty} 1-\\left(\\frac{n-1}{n}\\right)^n \\Leftrightarrow 1 -  \\lim_{n\\to\\infty} \\left(1-\\frac{1}{n}\\right)^n = 1 - e^{-1} \\approx 1 - 0.36788 = 0.63212.   Let's try zooming in the plot above for smaller values of  n n  so we can better observe the approach to the asymptote.  n = 500   # Number of observations\nprob = [1 - (1-1/n)**n for n in range(1, n)]\n\nplt.scatter(range(1,n),prob);\nplt.xlim(0, n);\nplt.ylim(.6,.7);",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter5/exercise2/#h",
            "text": "From (e) we know this value is approximately 0.63397.  Since we are solving it in Python, we will adapt the code proposed in the statement of the exercise.  import random\n\nn = 100\nN = 10000\ncount = 0\n\nfor _ in range(N):\n    # True equals 1 in Python\n    # choices(l, k=k) takes k samples from the list l with replacement\n    count += 4 in random.choices(range(1,n+1), k=n) \n\nprint(count/N)  0.6316  As expected, the value found is close to 0.63397, the value given by the formula. But it is an underestimate.\nIf we repeat the calculation several times, we would expect approximately half the estimates to be an underestimate, and the other half to be an overestimate.\nLet's try it.\nSo, for fun, we redo the calculation above a few more times (now with numpy) and do confirm this from the histogram below.  import numpy as np\nx = []\n\nfor i in range(100):\n    # https://stackoverflow.com/questions/367565/how-do-i-build-a-numpy-array-from-a-generator\n    a = np.fromiter((4 in np.random.choice(100, 100, replace=True) for _ in range(N)), dtype=bool)\n    x.append(np.mean(a)) \n\nplt.hist(x)  (array([  1.,   3.,   5.,  16.,  18.,  17.,  18.,  14.,   4.,   4.]),\n array([ 0.6202,  0.623 ,  0.6258,  0.6286,  0.6314,  0.6342,  0.637 ,\n         0.6398,  0.6426,  0.6454,  0.6482]),\n <a list of 10 Patch objects>)",
            "title": "(h)"
        },
        {
            "location": "/sols/chapter5/exercise3/",
            "text": "Problem 5.3\n\n\n(a)\n\n\nThe k-fold cross-validation algorithm is a method to estimate the test error of a given statistical estimator.\nWe start by taking the training set (of size N) and partitioning it into k non-overlapping equal parts.\nIf k does not divide N evenly, we split it into k folds as \"equal\" as possible; for example, if N=1003, and k=10, we will have 3 folds with 101 elements and 7 with 100 elements.\nWe then fit the model k times, each time leaving out a different fold as validation set.\nFor each of these k model fits, we use the left-out fold to calculate the validation error. \nFinally, the average of the k validation errors is our estimate of the test error.\n\n\n(b)\n\n\ni.\n\n\nRelative to the the k-fold cross validation, the validation set approach requires less computation (since it only fits the model once) and is simpler and easier to implement. On the other hand, the validation set approach tends to overestimate the test error since it only uses half the sample to fit the model (in general a larger sample size leads to lower test error). Additionally, fitting only half the model will make the test error estimate dependent on which half of the sample we choose. Both of these aspects are also true for the k-fold cross-validation but the effect is much smaller (the difference is evident when comparing the right-hand panels of Figures 5.2 and 5.4 of the text).\n\n\nii.\n\n\nLeave-one-out cross-validation (LOOCV) has less bias than k-fold cross validation since it uses almost all of the points of the data set, nearly unbiased.\nOn the other hand, this is yet another instance of the bias-variance trade-off, and LOOCV will have more variance than k-fold cross validation.\nThis is because when every pair of the n-1 fitted models differs only by 2 points, making the validation errors extremely correlated - they share n-2 of the n-1 points used for each fit.\nWith k-fold cross validation this correlation is greatly diminished if k=5 or k=10, for instance. \n\n\nConsider a k=10 example: each pair of the 10 fits shares about only 80% of the points. With k=5, each pair of the 5 model fits shares only approximately 60% of the points.\nAdditionally, k-fold cross validation is less computationally intensive, requiring only k model fits instead of the n fits with LOOCV. As pointed out in the text (Equation 5.2), there's an exception to this case for the least squares linear or polinomial regression - in this case LOOCV requires only the same computation as a single model fit.",
            "title": "5.3"
        },
        {
            "location": "/sols/chapter5/exercise3/#problem-53",
            "text": "",
            "title": "Problem 5.3"
        },
        {
            "location": "/sols/chapter5/exercise3/#a",
            "text": "The k-fold cross-validation algorithm is a method to estimate the test error of a given statistical estimator.\nWe start by taking the training set (of size N) and partitioning it into k non-overlapping equal parts.\nIf k does not divide N evenly, we split it into k folds as \"equal\" as possible; for example, if N=1003, and k=10, we will have 3 folds with 101 elements and 7 with 100 elements.\nWe then fit the model k times, each time leaving out a different fold as validation set.\nFor each of these k model fits, we use the left-out fold to calculate the validation error. \nFinally, the average of the k validation errors is our estimate of the test error.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter5/exercise3/#b",
            "text": "",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter5/exercise3/#i",
            "text": "Relative to the the k-fold cross validation, the validation set approach requires less computation (since it only fits the model once) and is simpler and easier to implement. On the other hand, the validation set approach tends to overestimate the test error since it only uses half the sample to fit the model (in general a larger sample size leads to lower test error). Additionally, fitting only half the model will make the test error estimate dependent on which half of the sample we choose. Both of these aspects are also true for the k-fold cross-validation but the effect is much smaller (the difference is evident when comparing the right-hand panels of Figures 5.2 and 5.4 of the text).",
            "title": "i."
        },
        {
            "location": "/sols/chapter5/exercise3/#ii",
            "text": "Leave-one-out cross-validation (LOOCV) has less bias than k-fold cross validation since it uses almost all of the points of the data set, nearly unbiased.\nOn the other hand, this is yet another instance of the bias-variance trade-off, and LOOCV will have more variance than k-fold cross validation.\nThis is because when every pair of the n-1 fitted models differs only by 2 points, making the validation errors extremely correlated - they share n-2 of the n-1 points used for each fit.\nWith k-fold cross validation this correlation is greatly diminished if k=5 or k=10, for instance.   Consider a k=10 example: each pair of the 10 fits shares about only 80% of the points. With k=5, each pair of the 5 model fits shares only approximately 60% of the points.\nAdditionally, k-fold cross validation is less computationally intensive, requiring only k model fits instead of the n fits with LOOCV. As pointed out in the text (Equation 5.2), there's an exception to this case for the least squares linear or polinomial regression - in this case LOOCV requires only the same computation as a single model fit.",
            "title": "ii."
        },
        {
            "location": "/sols/chapter5/exercise4/",
            "text": "Exercise 5.4\n\n\nThe standard deviation of our prediction can be estimated using bootstrap.\nThis technique allows us to obtain new data sets by repeatedly sampling, with replacement, the original data set.\n\n\nIn this case, we want to estimate the standard deviation.\nThe first thing to do is to repeat the bootstrap process a large number of times (e.g. \n10 000 times\n).\nAfter that, to each of the data sets generated by bootstrap (bootstrap samples), we fit a model and make a prediction.\nThe standard deviation of the resulting set of predictions is then used to estimate the standard deviation of our predictor. \n\n\nThe bootstrap approach can be used to estimate other properties of an estimator (e.g. mean, confidence intervals, prediction error).",
            "title": "5.4"
        },
        {
            "location": "/sols/chapter5/exercise4/#exercise-54",
            "text": "The standard deviation of our prediction can be estimated using bootstrap.\nThis technique allows us to obtain new data sets by repeatedly sampling, with replacement, the original data set.  In this case, we want to estimate the standard deviation.\nThe first thing to do is to repeat the bootstrap process a large number of times (e.g.  10 000 times ).\nAfter that, to each of the data sets generated by bootstrap (bootstrap samples), we fit a model and make a prediction.\nThe standard deviation of the resulting set of predictions is then used to estimate the standard deviation of our predictor.   The bootstrap approach can be used to estimate other properties of an estimator (e.g. mean, confidence intervals, prediction error).",
            "title": "Exercise 5.4"
        },
        {
            "location": "/sols/chapter5/exercise5/",
            "text": "Exercise 5.5\n\n\nimport pandas as pd\nimport numpy as np\nimport patsy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\n# statsmodels issue: https://github.com/statsmodels/statsmodels/issues/3931\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n\nsns.set(style=\"white\")\n%matplotlib inline\n\n\n\n\nnp.random.seed(1)\n\ndf = pd.read_csv(\"../data/Default.csv\", index_col=0)\n\ndf['default_yes'] = (df['default'] == 'Yes').astype('int')\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \ndefault\n\n      \nstudent\n\n      \nbalance\n\n      \nincome\n\n      \ndefault_yes\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \nNo\n\n      \nNo\n\n      \n729.526495\n\n      \n44361.625074\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \nNo\n\n      \nYes\n\n      \n817.180407\n\n      \n12106.134700\n\n      \n0\n\n    \n\n    \n\n      \n3\n\n      \nNo\n\n      \nNo\n\n      \n1073.549164\n\n      \n31767.138947\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \nNo\n\n      \nNo\n\n      \n529.250605\n\n      \n35704.493935\n\n      \n0\n\n    \n\n    \n\n      \n5\n\n      \nNo\n\n      \nNo\n\n      \n785.655883\n\n      \n38463.495879\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\ndf.info()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000 entries, 1 to 10000\nData columns (total 5 columns):\ndefault        10000 non-null object\nstudent        10000 non-null object\nbalance        10000 non-null float64\nincome         10000 non-null float64\ndefault_yes    10000 non-null int64\ndtypes: float64(2), int64(1), object(2)\nmemory usage: 468.8+ KB\n\n\n\n(a)\n\n\nWe are looking for LogisticRegression without regularization. In sklearn this is not implemented, but we can use l2 regularization and set C, the inverese strenght, to a very high number, effectively removing the regularization. We also compare the coefficients to the ones obtained from statsmodel LogisticRegression which has no regularization, and we verify that the coefficient estimates match.\n\n\nAnother parameter we have to consider is the tolerance. In this case, the default 1e-4 is not enough to reach convergence, so we increased it until it did.\n\n\nBelow we that the coefficients obtained with sklearn agree with those from statsmodels.\n\n\nlr = LogisticRegression(C=10**6, tol=1e-6)\nX = df[['income', 'balance']]\ny = df['default_yes']\nmod = lr.fit(X, y)\nmod.coef_\n\n\n\n\narray([[  2.07267113e-05,   5.64079143e-03]])\n\n\n\nf = 'default_yes ~ income + balance'\nres = smf.logit(formula=f, data=df).fit()\nres.summary()\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.078948\n         Iterations 10\n\n\n\n\n\nLogit Regression Results\n\n\n\n  \nDep. Variable:\n    \ndefault_yes\n   \n  No. Observations:  \n   \n 10000\n  \n\n\n\n\n  \nModel:\n               \nLogit\n      \n  Df Residuals:      \n   \n  9997\n  \n\n\n\n\n  \nMethod:\n               \nMLE\n       \n  Df Model:          \n   \n     2\n  \n\n\n\n\n  \nDate:\n          \nFri, 05 Jan 2018\n \n  Pseudo R-squ.:     \n   \n0.4594\n  \n\n\n\n\n  \nTime:\n              \n16:05:52\n     \n  Log-Likelihood:    \n  \n -789.48\n \n\n\n\n\n  \nconverged:\n           \nTrue\n       \n  LL-Null:           \n  \n -1460.3\n \n\n\n\n\n  \n \n                      \n \n        \n  LLR p-value:       \n \n4.541e-292\n\n\n\n\n\n\n\n\n\n\n      \n         \ncoef\n     \nstd err\n      \nz\n      \nP>|z|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n \n  -11.5405\n \n    0.435\n \n  -26.544\n \n 0.000\n \n  -12.393\n \n  -10.688\n\n\n\n\n\n  \nincome\n    \n 2.081e-05\n \n 4.99e-06\n \n    4.174\n \n 0.000\n \n  1.1e-05\n \n 3.06e-05\n\n\n\n\n\n  \nbalance\n   \n    0.0056\n \n    0.000\n \n   24.835\n \n 0.000\n \n    0.005\n \n    0.006\n\n\n\n\n\n\n\n(b)\n\n\ni.\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n\n\n\n\nii.\n\n\nmod = lr.fit(X_train, y_train)\nmod.coef_\n\n\n\n\narray([[  1.62553551e-05,   5.83500517e-03]])\n\n\n\niii.\n\n\nLet's try to plot this region and boundary. For more info on how to draw the boundary for logistic regression, here's \ntwo\n \nanswers\n by Michael Waskom, the creator of seaborn, and you can also see Jake VanderPlas' \nbook\n for an introduction to contour plots in general.\n\n\nxx, yy = np.mgrid[0:80000:100, -100:3000:10]\ngrid = np.c_[xx.ravel(), yy.ravel()]                    # https://www.quora.com/Can-anybody-elaborate-the-use-of-c_-in-numpy\nprobs = mod.predict_proba(grid)[:, 1].reshape(xx.shape)\n\n\n\n\nf, ax = plt.subplots(figsize=(8,6))\ncontour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",    # 25 levels\n                     vmin=0, vmax=1)\nax_c = f.colorbar(contour)\nax_c.set_label(\"P(default)\")\nax_c.set_ticks([0,0.25,0.5,.75,1])\n\nax.scatter(X_test['income'], X_test['balance'], c=y_test, s=50, \n          cmap=\"RdBu\", vmin=-0.2, vmax=1.2,\n          edgecolor=\"white\", linewidth=1)\n\nax.set(xlabel=\"income\", ylabel=\"balance\");\n\n\n\n\n\n\niv.\n\n\ny_pred = mod.predict(X_test)\n1-(y_pred == y_test).mean()\n\n\n\n\n0.025000000000000022\n\n\n\nSo our general test error is 2.5%.\n\n\nBut from the figure above, it seems that the error rates are very different depending on whether we are considering a positive or a negative.\nLet's have a look at the \nconfusion matrix\n as well (page 145 of ISLR). \nThe function show_confusion_matrix() we use below is from \nthis blog post\n by Matt Hancock (with a slight modification).\n\n\ndef show_confusion_matrix(C,class_labels=['0','1']):\n    \"\"\"\n    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n    class_labels: list of strings, default simply labels 0 and 1.\n\n    Draws confusion matrix with associated metrics.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n\n    # true negative, false positive, etc...\n    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n\n    NP = fn+tp # Num positive examples\n    NN = tn+fp # Num negative examples\n    N  = NP+NN\n\n    fig = plt.figure(figsize=(8,8))\n    ax  = fig.add_subplot(111)\n    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n\n    # Draw the grid boxes\n    ax.set_xlim(-0.5,2.5)\n    ax.set_ylim(2.5,-0.5)\n    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n\n    # Set xlabels\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks([0,1,2])\n    ax.set_xticklabels(class_labels + [''])\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n    # These coordinate might require some tinkering. Ditto for y, below.\n    ax.xaxis.set_label_coords(0.34,1.06)\n\n    # Set ylabels\n    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n    ax.set_yticklabels(class_labels + [''],rotation=90)\n    ax.set_yticks([0,1,2])\n    ax.yaxis.set_label_coords(-0.09,0.65)\n\n\n    # Fill in initial metrics: tp, tn, etc...\n    ax.text(0,0,\n            'True Neg: %d\\n(Num Neg: %d)'%(tn,NN),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,1,\n            'False Neg: %d'%fn,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,0,\n            'False Pos: %d'%fp,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    ax.text(1,1,\n            'True Pos: %d\\n(Num Pos: %d)'%(tp,NP),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    # Fill in secondary metrics: accuracy, true pos rate, etc...\n    ax.text(2,0,\n            'False Pos Rate: %.4f'%(fp / (fp+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,1,\n            'True Pos Rate: %.4f'%(tp / (tp+fn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,2,\n            'Accuracy: %.4f'%((tp+tn+0.)/N),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,2,\n            'Neg Pre Val: %.4f'%(1-fn/(fn+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,2,\n            'Pos Pred Val: %.4f'%(tp/(tp+fp+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nC = confusion_matrix(y_test, y_pred)\nshow_confusion_matrix(C, ['No Default', 'Default'])\n\n\n\n\n\n\nRecall the definitions used in the \nconfusion matrix\n above:\n\n P - condition positive (the number of real positive cases in the data)\n\n N - condition negative (the number of real negative cases in the data)\n\n TP - true positive (hit)\n\n TN - true negative (correct rejection)\n\n FP - false positive (false alarm, Type I error)\n\n FN - false negative (miss, Type II error)\n\n True positive rate, TPR = \n\\frac{TP}{P} = \\frac{TP}{TP + FN}\n\\frac{TP}{P} = \\frac{TP}{TP + FN}\n\n\n False positive rate, FPR = \n\\frac{FP}{N} = \\frac{FP}{FP + TN}\n\\frac{FP}{N} = \\frac{FP}{FP + TN}\n\n\n Positive predictive value, PPV = \n\\frac{TP}{TP + FP}\n\\frac{TP}{TP + FP}\n\n\n Negative predictive value, NPV = \n\\frac{TN}{TN + FN}\n\\frac{TN}{TN + FN}\n\n* Accuracy, ACC = \n\\frac{TP + TN}{P+N} = \\frac{TP+TN}{TP+TN+FP+FN}\n\\frac{TP + TN}{P+N} = \\frac{TP+TN}{TP+TN+FP+FN}\n\n\nSo, our true positive rate (or sensitivity, recall or hit rate) is 0.3648, our false positive rate (or fall-out) is 0.0050, our positive predictive value (or precision) is 0.7073, our negative predictive value is 0.9795, and our accuracy is 0.9750.\n\n\n(c)\n\n\nLet's keep a vector of the confusion matrices, and compute the different errors for each validation set. \n\n\nC = [C]\n\n\n\n\nfor i in range(1,4):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n    mod = lr.fit(X_train, y_train)\n    y_pred = mod.predict(X_test)\n    C.append(confusion_matrix(y_test, y_pred))\n\n\n\n\ntpr, fpr, ppv, npv, acc = ([] for i in range(5))\n\nfor c in C:\n    tn = c[0,0] \n    fp = c[0,1]\n    fn = c[1,0]\n    tp = c[1,1]\n    tpr.append((tp / (tp+fn+0.)))\n    fpr.append((fp / (fp+tn+0.)))\n    ppv.append((tp/(tp+fp+0.)))\n    npv.append((1-fn/(fn+tn+0.)))\n    acc.append(((tp+tn+0.)/(tn+fp+fn+tp)))\n\n\n\n\ndef line(l):\n    return \" \".join( '{:06.4f}'.format(a) for a in l) + ', Average: ' +'{:06.4f}'.format(sum(l)/ len(l))\n\nprint('TPR: ')\nprint(line(tpr))\nprint('FPR: ')\nprint(line(fpr))\nprint('PPV: ')\nprint(line(ppv))\nprint('NPV: ')\nprint(line(npv))\nprint('ACC: ')\nprint(line(acc))\n\n\n\n\nTPR: \n0.3648 0.3452 0.3030 0.3293, Average: 0.3356\nFPR: \n0.0050 0.0029 0.0041 0.0029, Average: 0.0037\nPPV: \n0.7073 0.8056 0.7143 0.7941, Average: 0.7553\nNPV: \n0.9795 0.9777 0.9767 0.9777, Average: 0.9779\nACC: \n0.9750 0.9752 0.9730 0.9752, Average: 0.9746\n\n\n\nThe values above indicate that some quantities vary more than others when we change the validation set. In particular, the positive predicted value (PPV) varies from 0.71 to 0.81. The PPV is the ratio of the true positives over the sum of the true positives and false positives. In other words, it's a ratio involving the quantities above the boundary in the region above. And since both these quantities vary significantly in this case, the variance of the PPV is expected. The accuracy on the other handle is much more robust across different validation sets, for the opposite reason. The quantities involved in its computation do not vary as much. The denominator is a constant, and the numerator, TP + TN, is somewhat stable, since TP and TN would on average vary in opposite directions.\n\n\n(d)\n\n\ndf['student_yes'] = (df['student'] == 'Yes').astype('int')\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \ndefault\n\n      \nstudent\n\n      \nbalance\n\n      \nincome\n\n      \ndefault_yes\n\n      \nstudent_yes\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \nNo\n\n      \nNo\n\n      \n729.526495\n\n      \n44361.625074\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \nNo\n\n      \nYes\n\n      \n817.180407\n\n      \n12106.134700\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n3\n\n      \nNo\n\n      \nNo\n\n      \n1073.549164\n\n      \n31767.138947\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \nNo\n\n      \nNo\n\n      \n529.250605\n\n      \n35704.493935\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n5\n\n      \nNo\n\n      \nNo\n\n      \n785.655883\n\n      \n38463.495879\n\n      \n0\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\nX = df[['income','balance','student_yes']]\ny = df['default_yes']\n\nf = 'default_yes ~ income + balance + student_yes'\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\ntrain = X_train.join(y_train)\n\n\nres = smf.logit(formula=f, data=train).fit()\nres.summary()\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.076250\n         Iterations 10\n\n\n\n\n\nLogit Regression Results\n\n\n\n  \nDep. Variable:\n    \ndefault_yes\n   \n  No. Observations:  \n   \n  7500\n  \n\n\n\n\n  \nModel:\n               \nLogit\n      \n  Df Residuals:      \n   \n  7496\n  \n\n\n\n\n  \nMethod:\n               \nMLE\n       \n  Df Model:          \n   \n     3\n  \n\n\n\n\n  \nDate:\n          \nFri, 05 Jan 2018\n \n  Pseudo R-squ.:     \n   \n0.4799\n  \n\n\n\n\n  \nTime:\n              \n16:07:39\n     \n  Log-Likelihood:    \n  \n -571.88\n \n\n\n\n\n  \nconverged:\n           \nTrue\n       \n  LL-Null:           \n  \n -1099.5\n \n\n\n\n\n  \n \n                      \n \n        \n  LLR p-value:       \n \n1.954e-228\n\n\n\n\n\n\n\n\n\n\n       \n          \ncoef\n     \nstd err\n      \nz\n      \nP>|z|\n  \n[0.025\n    \n0.975]\n  \n\n\n\n\n  \nIntercept\n   \n  -10.9255\n \n    0.579\n \n  -18.873\n \n 0.000\n \n  -12.060\n \n   -9.791\n\n\n\n\n\n  \nincome\n      \n-6.764e-06\n \n  9.4e-06\n \n   -0.720\n \n 0.472\n \n-2.52e-05\n \n 1.17e-05\n\n\n\n\n\n  \nbalance\n     \n    0.0061\n \n    0.000\n \n   21.174\n \n 0.000\n \n    0.006\n \n    0.007\n\n\n\n\n\n  \nstudent_yes\n \n   -1.0870\n \n    0.274\n \n   -3.961\n \n 0.000\n \n   -1.625\n \n   -0.549\n\n\n\n\n\n\n\ny_pred = (res.predict(X_test) > .5) * 1\n\n\n\n\nC = confusion_matrix(y_test, y_pred)\nshow_confusion_matrix(C, ['No Default', 'Default'])\n\n\n\n\n\n\nSo compared to the values above without the student dummy variable, it seems that adding the student variable does not help in any of the metrics since they are worse or very similar (although we should consider the variance, in a more careful analysis).",
            "title": "5.5"
        },
        {
            "location": "/sols/chapter5/exercise5/#exercise-55",
            "text": "import pandas as pd\nimport numpy as np\nimport patsy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\n# statsmodels issue: https://github.com/statsmodels/statsmodels/issues/3931\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n\nsns.set(style=\"white\")\n%matplotlib inline  np.random.seed(1)\n\ndf = pd.read_csv(\"../data/Default.csv\", index_col=0)\n\ndf['default_yes'] = (df['default'] == 'Yes').astype('int')\ndf.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       default \n       student \n       balance \n       income \n       default_yes \n     \n   \n   \n     \n       1 \n       No \n       No \n       729.526495 \n       44361.625074 \n       0 \n     \n     \n       2 \n       No \n       Yes \n       817.180407 \n       12106.134700 \n       0 \n     \n     \n       3 \n       No \n       No \n       1073.549164 \n       31767.138947 \n       0 \n     \n     \n       4 \n       No \n       No \n       529.250605 \n       35704.493935 \n       0 \n     \n     \n       5 \n       No \n       No \n       785.655883 \n       38463.495879 \n       0 \n     \n      df.info()  <class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000 entries, 1 to 10000\nData columns (total 5 columns):\ndefault        10000 non-null object\nstudent        10000 non-null object\nbalance        10000 non-null float64\nincome         10000 non-null float64\ndefault_yes    10000 non-null int64\ndtypes: float64(2), int64(1), object(2)\nmemory usage: 468.8+ KB",
            "title": "Exercise 5.5"
        },
        {
            "location": "/sols/chapter5/exercise5/#a",
            "text": "We are looking for LogisticRegression without regularization. In sklearn this is not implemented, but we can use l2 regularization and set C, the inverese strenght, to a very high number, effectively removing the regularization. We also compare the coefficients to the ones obtained from statsmodel LogisticRegression which has no regularization, and we verify that the coefficient estimates match.  Another parameter we have to consider is the tolerance. In this case, the default 1e-4 is not enough to reach convergence, so we increased it until it did.  Below we that the coefficients obtained with sklearn agree with those from statsmodels.  lr = LogisticRegression(C=10**6, tol=1e-6)\nX = df[['income', 'balance']]\ny = df['default_yes']\nmod = lr.fit(X, y)\nmod.coef_  array([[  2.07267113e-05,   5.64079143e-03]])  f = 'default_yes ~ income + balance'\nres = smf.logit(formula=f, data=df).fit()\nres.summary()  Optimization terminated successfully.\n         Current function value: 0.078948\n         Iterations 10   Logit Regression Results  \n   Dep. Variable:      default_yes       No. Observations:        10000     \n   Model:                 Logit          Df Residuals:             9997     \n   Method:                 MLE           Df Model:                    2     \n   Date:            Fri, 05 Jan 2018     Pseudo R-squ.:          0.4594     \n   Time:                16:05:52         Log-Likelihood:         -789.48    \n   converged:             True           LL-Null:                -1460.3    \n                                         LLR p-value:          4.541e-292     \n                 coef       std err        z        P>|z|    [0.025      0.975]     \n   Intercept     -11.5405       0.435     -26.544    0.000     -12.393     -10.688   \n   income       2.081e-05    4.99e-06       4.174    0.000     1.1e-05    3.06e-05   \n   balance         0.0056       0.000      24.835    0.000       0.005       0.006",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter5/exercise5/#b",
            "text": "",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter5/exercise5/#i",
            "text": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)",
            "title": "i."
        },
        {
            "location": "/sols/chapter5/exercise5/#ii",
            "text": "mod = lr.fit(X_train, y_train)\nmod.coef_  array([[  1.62553551e-05,   5.83500517e-03]])",
            "title": "ii."
        },
        {
            "location": "/sols/chapter5/exercise5/#iii",
            "text": "Let's try to plot this region and boundary. For more info on how to draw the boundary for logistic regression, here's  two   answers  by Michael Waskom, the creator of seaborn, and you can also see Jake VanderPlas'  book  for an introduction to contour plots in general.  xx, yy = np.mgrid[0:80000:100, -100:3000:10]\ngrid = np.c_[xx.ravel(), yy.ravel()]                    # https://www.quora.com/Can-anybody-elaborate-the-use-of-c_-in-numpy\nprobs = mod.predict_proba(grid)[:, 1].reshape(xx.shape)  f, ax = plt.subplots(figsize=(8,6))\ncontour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",    # 25 levels\n                     vmin=0, vmax=1)\nax_c = f.colorbar(contour)\nax_c.set_label(\"P(default)\")\nax_c.set_ticks([0,0.25,0.5,.75,1])\n\nax.scatter(X_test['income'], X_test['balance'], c=y_test, s=50, \n          cmap=\"RdBu\", vmin=-0.2, vmax=1.2,\n          edgecolor=\"white\", linewidth=1)\n\nax.set(xlabel=\"income\", ylabel=\"balance\");",
            "title": "iii."
        },
        {
            "location": "/sols/chapter5/exercise5/#iv",
            "text": "y_pred = mod.predict(X_test)\n1-(y_pred == y_test).mean()  0.025000000000000022  So our general test error is 2.5%.  But from the figure above, it seems that the error rates are very different depending on whether we are considering a positive or a negative.\nLet's have a look at the  confusion matrix  as well (page 145 of ISLR). \nThe function show_confusion_matrix() we use below is from  this blog post  by Matt Hancock (with a slight modification).  def show_confusion_matrix(C,class_labels=['0','1']):\n    \"\"\"\n    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n    class_labels: list of strings, default simply labels 0 and 1.\n\n    Draws confusion matrix with associated metrics.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n\n    # true negative, false positive, etc...\n    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n\n    NP = fn+tp # Num positive examples\n    NN = tn+fp # Num negative examples\n    N  = NP+NN\n\n    fig = plt.figure(figsize=(8,8))\n    ax  = fig.add_subplot(111)\n    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n\n    # Draw the grid boxes\n    ax.set_xlim(-0.5,2.5)\n    ax.set_ylim(2.5,-0.5)\n    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n\n    # Set xlabels\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks([0,1,2])\n    ax.set_xticklabels(class_labels + [''])\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n    # These coordinate might require some tinkering. Ditto for y, below.\n    ax.xaxis.set_label_coords(0.34,1.06)\n\n    # Set ylabels\n    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n    ax.set_yticklabels(class_labels + [''],rotation=90)\n    ax.set_yticks([0,1,2])\n    ax.yaxis.set_label_coords(-0.09,0.65)\n\n\n    # Fill in initial metrics: tp, tn, etc...\n    ax.text(0,0,\n            'True Neg: %d\\n(Num Neg: %d)'%(tn,NN),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,1,\n            'False Neg: %d'%fn,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,0,\n            'False Pos: %d'%fp,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    ax.text(1,1,\n            'True Pos: %d\\n(Num Pos: %d)'%(tp,NP),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    # Fill in secondary metrics: accuracy, true pos rate, etc...\n    ax.text(2,0,\n            'False Pos Rate: %.4f'%(fp / (fp+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,1,\n            'True Pos Rate: %.4f'%(tp / (tp+fn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,2,\n            'Accuracy: %.4f'%((tp+tn+0.)/N),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,2,\n            'Neg Pre Val: %.4f'%(1-fn/(fn+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,2,\n            'Pos Pred Val: %.4f'%(tp/(tp+fp+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    plt.tight_layout()\n    plt.show()  C = confusion_matrix(y_test, y_pred)\nshow_confusion_matrix(C, ['No Default', 'Default'])   Recall the definitions used in the  confusion matrix  above:  P - condition positive (the number of real positive cases in the data)  N - condition negative (the number of real negative cases in the data)  TP - true positive (hit)  TN - true negative (correct rejection)  FP - false positive (false alarm, Type I error)  FN - false negative (miss, Type II error)  True positive rate, TPR =  \\frac{TP}{P} = \\frac{TP}{TP + FN} \\frac{TP}{P} = \\frac{TP}{TP + FN}   False positive rate, FPR =  \\frac{FP}{N} = \\frac{FP}{FP + TN} \\frac{FP}{N} = \\frac{FP}{FP + TN}   Positive predictive value, PPV =  \\frac{TP}{TP + FP} \\frac{TP}{TP + FP}   Negative predictive value, NPV =  \\frac{TN}{TN + FN} \\frac{TN}{TN + FN} \n* Accuracy, ACC =  \\frac{TP + TN}{P+N} = \\frac{TP+TN}{TP+TN+FP+FN} \\frac{TP + TN}{P+N} = \\frac{TP+TN}{TP+TN+FP+FN}  So, our true positive rate (or sensitivity, recall or hit rate) is 0.3648, our false positive rate (or fall-out) is 0.0050, our positive predictive value (or precision) is 0.7073, our negative predictive value is 0.9795, and our accuracy is 0.9750.",
            "title": "iv."
        },
        {
            "location": "/sols/chapter5/exercise5/#c",
            "text": "Let's keep a vector of the confusion matrices, and compute the different errors for each validation set.   C = [C]  for i in range(1,4):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n    mod = lr.fit(X_train, y_train)\n    y_pred = mod.predict(X_test)\n    C.append(confusion_matrix(y_test, y_pred))  tpr, fpr, ppv, npv, acc = ([] for i in range(5))\n\nfor c in C:\n    tn = c[0,0] \n    fp = c[0,1]\n    fn = c[1,0]\n    tp = c[1,1]\n    tpr.append((tp / (tp+fn+0.)))\n    fpr.append((fp / (fp+tn+0.)))\n    ppv.append((tp/(tp+fp+0.)))\n    npv.append((1-fn/(fn+tn+0.)))\n    acc.append(((tp+tn+0.)/(tn+fp+fn+tp)))  def line(l):\n    return \" \".join( '{:06.4f}'.format(a) for a in l) + ', Average: ' +'{:06.4f}'.format(sum(l)/ len(l))\n\nprint('TPR: ')\nprint(line(tpr))\nprint('FPR: ')\nprint(line(fpr))\nprint('PPV: ')\nprint(line(ppv))\nprint('NPV: ')\nprint(line(npv))\nprint('ACC: ')\nprint(line(acc))  TPR: \n0.3648 0.3452 0.3030 0.3293, Average: 0.3356\nFPR: \n0.0050 0.0029 0.0041 0.0029, Average: 0.0037\nPPV: \n0.7073 0.8056 0.7143 0.7941, Average: 0.7553\nNPV: \n0.9795 0.9777 0.9767 0.9777, Average: 0.9779\nACC: \n0.9750 0.9752 0.9730 0.9752, Average: 0.9746  The values above indicate that some quantities vary more than others when we change the validation set. In particular, the positive predicted value (PPV) varies from 0.71 to 0.81. The PPV is the ratio of the true positives over the sum of the true positives and false positives. In other words, it's a ratio involving the quantities above the boundary in the region above. And since both these quantities vary significantly in this case, the variance of the PPV is expected. The accuracy on the other handle is much more robust across different validation sets, for the opposite reason. The quantities involved in its computation do not vary as much. The denominator is a constant, and the numerator, TP + TN, is somewhat stable, since TP and TN would on average vary in opposite directions.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter5/exercise5/#d",
            "text": "df['student_yes'] = (df['student'] == 'Yes').astype('int')  df.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       default \n       student \n       balance \n       income \n       default_yes \n       student_yes \n     \n   \n   \n     \n       1 \n       No \n       No \n       729.526495 \n       44361.625074 \n       0 \n       0 \n     \n     \n       2 \n       No \n       Yes \n       817.180407 \n       12106.134700 \n       0 \n       1 \n     \n     \n       3 \n       No \n       No \n       1073.549164 \n       31767.138947 \n       0 \n       0 \n     \n     \n       4 \n       No \n       No \n       529.250605 \n       35704.493935 \n       0 \n       0 \n     \n     \n       5 \n       No \n       No \n       785.655883 \n       38463.495879 \n       0 \n       0 \n     \n      X = df[['income','balance','student_yes']]\ny = df['default_yes']\n\nf = 'default_yes ~ income + balance + student_yes'\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\ntrain = X_train.join(y_train)\n\n\nres = smf.logit(formula=f, data=train).fit()\nres.summary()  Optimization terminated successfully.\n         Current function value: 0.076250\n         Iterations 10   Logit Regression Results  \n   Dep. Variable:      default_yes       No. Observations:         7500     \n   Model:                 Logit          Df Residuals:             7496     \n   Method:                 MLE           Df Model:                    3     \n   Date:            Fri, 05 Jan 2018     Pseudo R-squ.:          0.4799     \n   Time:                16:07:39         Log-Likelihood:         -571.88    \n   converged:             True           LL-Null:                -1099.5    \n                                         LLR p-value:          1.954e-228     \n                   coef       std err        z        P>|z|    [0.025      0.975]     \n   Intercept       -10.9255       0.579     -18.873    0.000     -12.060      -9.791   \n   income        -6.764e-06     9.4e-06      -0.720    0.472   -2.52e-05    1.17e-05   \n   balance           0.0061       0.000      21.174    0.000       0.006       0.007   \n   student_yes      -1.0870       0.274      -3.961    0.000      -1.625      -0.549    y_pred = (res.predict(X_test) > .5) * 1  C = confusion_matrix(y_test, y_pred)\nshow_confusion_matrix(C, ['No Default', 'Default'])   So compared to the values above without the student dummy variable, it seems that adding the student variable does not help in any of the metrics since they are worse or very similar (although we should consider the variance, in a more careful analysis).",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter5/exercise6/",
            "text": "Exercise 5.6\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf \nimport numpy as np\n\n\n\n\ndf = pd.read_csv('../data/Default.csv', index_col=0)\n\n\n\n\ndf.head() #just checking\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \ndefault\n\n      \nstudent\n\n      \nbalance\n\n      \nincome\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \nNo\n\n      \nNo\n\n      \n729.526495\n\n      \n44361.625074\n\n    \n\n    \n\n      \n2\n\n      \nNo\n\n      \nYes\n\n      \n817.180407\n\n      \n12106.134700\n\n    \n\n    \n\n      \n3\n\n      \nNo\n\n      \nNo\n\n      \n1073.549164\n\n      \n31767.138947\n\n    \n\n    \n\n      \n4\n\n      \nNo\n\n      \nNo\n\n      \n529.250605\n\n      \n35704.493935\n\n    \n\n    \n\n      \n5\n\n      \nNo\n\n      \nNo\n\n      \n785.655883\n\n      \n38463.495879\n\n    \n\n  \n\n\n\n\n\n\n\nnp.random.seed(0) #asked in the exercise\n\n\n\n\n(a)\n\n\n#using generalized linear models with statsmodel\n#see the wikipedia reference to understand why family is binomial\nmod1 = smf.glm(formula='default ~ income + balance', data=df, family=sm.families.Binomial()).fit() #create & fit model\nprint(mod1.summary()) #show results\n\n\n\n\n                        Generalized Linear Model Regression Results                        \n===========================================================================================\nDep. Variable:     ['default[No]', 'default[Yes]']   No. Observations:                10000\nModel:                                         GLM   Df Residuals:                     9997\nModel Family:                             Binomial   Df Model:                            2\nLink Function:                               logit   Scale:                             1.0\nMethod:                                       IRLS   Log-Likelihood:                -789.48\nDate:                             Fri, 05 Jan 2018   Deviance:                       1579.0\nTime:                                     21:08:26   Pearson chi2:                 6.95e+03\nNo. Iterations:                                  9                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.5405      0.435     26.544      0.000      10.688      12.393\nincome     -2.081e-05   4.99e-06     -4.174      0.000   -3.06e-05    -1.1e-05\nbalance       -0.0056      0.000    -24.835      0.000      -0.006      -0.005\n==============================================================================\n\n\n\nEstimated standard errors for the coefficients associated with \nincome\n and \nbalance\n are \n4.99e-06\n and \n0\n, respectively.\n\n\n(b)\n\n\ndef boot_fn(default):\n    mod1 = smf.glm(formula='default ~ income + balance', data=default, family=sm.families.Binomial()).fit()\n    coef_income = mod1.params[1]\n    coef_balance = mod1.params[2]\n    return [coef_income, coef_balance]\n\n\n\n\nboot_fn(df)\n\n\n\n\n[-2.0808975528986941e-05, -0.005647102950316488]\n\n\n\n(c)\n\n\nSince there is not Python equivalent to R boot function, we will create a boot function for Python.\n\n\n#bootstrap function\ndef boot(X, bootSample_size=None):\n    '''\n    Sampling observations from a dataframe\n\n    Parameters\n    ------------\n    X : pandas dataframe\n        Data to be resampled\n\n    bootSample_size: int, optional\n        Dimension of the bootstrapped samples\n\n    Returns\n    ------------\n    bootSample_X : pandas dataframe\n        Resampled data\n\n    Examples\n    ----------\n    To resample data from the X dataframe:\n        >> boot(X)\n    The resampled data will have length equal to len(X).\n\n    To resample data from the X dataframe in order to have length 5:\n        >> boot(X,5)\n\n    References\n    ------------\n    http://nbviewer.jupyter.org/gist/aflaxman/6871948\n\n    '''\n\n    #assign default size if non-specified\n    if bootSample_size == None:\n        bootSample_size = len(X)\n\n    #create random integers to use as indices for bootstrap sample based on original data\n    bootSample_i = (np.random.rand(bootSample_size)*len(X)).astype(int)\n    bootSample_i = np.array(bootSample_i)\n    bootSample_X = X.iloc[bootSample_i]\n\n    return bootSample_X\n\n\n\n\nNow, we will call the \nboot\n function \nn\n times, apply \nboot_fn\n and compute the coefficients average value. We used \nn = 100\n to have convergent results. Other values could be used.\n\n\n#running model for bootstrapped samples\ncoefficients = [] #variable initialization\nn = 100 #number of bootstrapped samples\n\nfor i in range(0,n):\n    coef_i = boot_fn(boot(df)) #determining coefficients for specific bootstrapped sample\n    coefficients.append(coef_i) #saving coefficients value\n\nprint(pd.DataFrame(coefficients).mean()) #print average of coefficients\n\n\n\n\n0   -0.000021\n1   -0.005716\ndtype: float64\n\n\n\n(d)\n\n\n\n\nResults (b):\n [-2.0808975528987073e-05, -0.005647102950316495]\n\n\nResults (c):\n [-0.000021, -0.005672]\n\n\n\n\nReferences\n\n\n\n\nWikipedia, Generalized linear model, https://en.wikipedia.org/wiki/Generalized_linear_model\n\n\nhttp://nbviewer.jupyter.org/gist/aflaxman/6871948",
            "title": "5.6"
        },
        {
            "location": "/sols/chapter5/exercise6/#exercise-56",
            "text": "import pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf \nimport numpy as np  df = pd.read_csv('../data/Default.csv', index_col=0)  df.head() #just checking   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       default \n       student \n       balance \n       income \n     \n   \n   \n     \n       1 \n       No \n       No \n       729.526495 \n       44361.625074 \n     \n     \n       2 \n       No \n       Yes \n       817.180407 \n       12106.134700 \n     \n     \n       3 \n       No \n       No \n       1073.549164 \n       31767.138947 \n     \n     \n       4 \n       No \n       No \n       529.250605 \n       35704.493935 \n     \n     \n       5 \n       No \n       No \n       785.655883 \n       38463.495879 \n     \n      np.random.seed(0) #asked in the exercise",
            "title": "Exercise 5.6"
        },
        {
            "location": "/sols/chapter5/exercise6/#a",
            "text": "#using generalized linear models with statsmodel\n#see the wikipedia reference to understand why family is binomial\nmod1 = smf.glm(formula='default ~ income + balance', data=df, family=sm.families.Binomial()).fit() #create & fit model\nprint(mod1.summary()) #show results                          Generalized Linear Model Regression Results                        \n===========================================================================================\nDep. Variable:     ['default[No]', 'default[Yes]']   No. Observations:                10000\nModel:                                         GLM   Df Residuals:                     9997\nModel Family:                             Binomial   Df Model:                            2\nLink Function:                               logit   Scale:                             1.0\nMethod:                                       IRLS   Log-Likelihood:                -789.48\nDate:                             Fri, 05 Jan 2018   Deviance:                       1579.0\nTime:                                     21:08:26   Pearson chi2:                 6.95e+03\nNo. Iterations:                                  9                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.5405      0.435     26.544      0.000      10.688      12.393\nincome     -2.081e-05   4.99e-06     -4.174      0.000   -3.06e-05    -1.1e-05\nbalance       -0.0056      0.000    -24.835      0.000      -0.006      -0.005\n==============================================================================  Estimated standard errors for the coefficients associated with  income  and  balance  are  4.99e-06  and  0 , respectively.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter5/exercise6/#b",
            "text": "def boot_fn(default):\n    mod1 = smf.glm(formula='default ~ income + balance', data=default, family=sm.families.Binomial()).fit()\n    coef_income = mod1.params[1]\n    coef_balance = mod1.params[2]\n    return [coef_income, coef_balance]  boot_fn(df)  [-2.0808975528986941e-05, -0.005647102950316488]",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter5/exercise6/#c",
            "text": "Since there is not Python equivalent to R boot function, we will create a boot function for Python.  #bootstrap function\ndef boot(X, bootSample_size=None):\n    '''\n    Sampling observations from a dataframe\n\n    Parameters\n    ------------\n    X : pandas dataframe\n        Data to be resampled\n\n    bootSample_size: int, optional\n        Dimension of the bootstrapped samples\n\n    Returns\n    ------------\n    bootSample_X : pandas dataframe\n        Resampled data\n\n    Examples\n    ----------\n    To resample data from the X dataframe:\n        >> boot(X)\n    The resampled data will have length equal to len(X).\n\n    To resample data from the X dataframe in order to have length 5:\n        >> boot(X,5)\n\n    References\n    ------------\n    http://nbviewer.jupyter.org/gist/aflaxman/6871948\n\n    '''\n\n    #assign default size if non-specified\n    if bootSample_size == None:\n        bootSample_size = len(X)\n\n    #create random integers to use as indices for bootstrap sample based on original data\n    bootSample_i = (np.random.rand(bootSample_size)*len(X)).astype(int)\n    bootSample_i = np.array(bootSample_i)\n    bootSample_X = X.iloc[bootSample_i]\n\n    return bootSample_X  Now, we will call the  boot  function  n  times, apply  boot_fn  and compute the coefficients average value. We used  n = 100  to have convergent results. Other values could be used.  #running model for bootstrapped samples\ncoefficients = [] #variable initialization\nn = 100 #number of bootstrapped samples\n\nfor i in range(0,n):\n    coef_i = boot_fn(boot(df)) #determining coefficients for specific bootstrapped sample\n    coefficients.append(coef_i) #saving coefficients value\n\nprint(pd.DataFrame(coefficients).mean()) #print average of coefficients  0   -0.000021\n1   -0.005716\ndtype: float64",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter5/exercise6/#d",
            "text": "Results (b):  [-2.0808975528987073e-05, -0.005647102950316495]  Results (c):  [-0.000021, -0.005672]",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter5/exercise6/#references",
            "text": "Wikipedia, Generalized linear model, https://en.wikipedia.org/wiki/Generalized_linear_model  http://nbviewer.jupyter.org/gist/aflaxman/6871948",
            "title": "References"
        },
        {
            "location": "/sols/chapter5/exercise7/",
            "text": "Exercise 5.7\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\ndf = pd.read_csv(\"../data/Weekly.csv\")\n\n%matplotlib inline\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nUnnamed: 0\n\n      \nYear\n\n      \nLag1\n\n      \nLag2\n\n      \nLag3\n\n      \nLag4\n\n      \nLag5\n\n      \nVolume\n\n      \nToday\n\n      \nDirection\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n1990\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n-0.229\n\n      \n-3.484\n\n      \n0.154976\n\n      \n-0.270\n\n      \nDown\n\n    \n\n    \n\n      \n1\n\n      \n2\n\n      \n1990\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n-0.229\n\n      \n0.148574\n\n      \n-2.576\n\n      \nDown\n\n    \n\n    \n\n      \n2\n\n      \n3\n\n      \n1990\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n0.159837\n\n      \n3.514\n\n      \nUp\n\n    \n\n    \n\n      \n3\n\n      \n4\n\n      \n1990\n\n      \n3.514\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n0.161630\n\n      \n0.712\n\n      \nUp\n\n    \n\n    \n\n      \n4\n\n      \n5\n\n      \n1990\n\n      \n0.712\n\n      \n3.514\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n0.153728\n\n      \n1.178\n\n      \nUp\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\ndf['Direction_Up'] = (df['Direction'] == 'Up').astype(int)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nUnnamed: 0\n\n      \nYear\n\n      \nLag1\n\n      \nLag2\n\n      \nLag3\n\n      \nLag4\n\n      \nLag5\n\n      \nVolume\n\n      \nToday\n\n      \nDirection\n\n      \nDirection_Up\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n1990\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n-0.229\n\n      \n-3.484\n\n      \n0.154976\n\n      \n-0.270\n\n      \nDown\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n2\n\n      \n1990\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n-0.229\n\n      \n0.148574\n\n      \n-2.576\n\n      \nDown\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n3\n\n      \n1990\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n-3.936\n\n      \n0.159837\n\n      \n3.514\n\n      \nUp\n\n      \n1\n\n    \n\n    \n\n      \n3\n\n      \n4\n\n      \n1990\n\n      \n3.514\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n1.572\n\n      \n0.161630\n\n      \n0.712\n\n      \nUp\n\n      \n1\n\n    \n\n    \n\n      \n4\n\n      \n5\n\n      \n1990\n\n      \n0.712\n\n      \n3.514\n\n      \n-2.576\n\n      \n-0.270\n\n      \n0.816\n\n      \n0.153728\n\n      \n1.178\n\n      \nUp\n\n      \n1\n\n    \n\n  \n\n\n\n\n\n\n\nX = df[['Lag1', 'Lag2']]\ny = df['Direction_Up']\n\n\n\n\nmod = LogisticRegression(C=10**6, tol=10**-7)\nmod.fit(X, y)\n\n\n\n\nLogisticRegression(C=1000000, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=1e-07, verbose=0, warm_start=False)\n\n\n\nmod.intercept_, mod.coef_\n\n\n\n\n(array([ 0.22122405]), array([[-0.03872222,  0.0602483 ]]))\n\n\n\ndef show_confusion_matrix(C,class_labels=['0','1']):\n    \"\"\"\n    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n    class_labels: list of strings, default simply labels 0 and 1.\n\n    Draws confusion matrix with associated metrics.\n\n    Reference: http://notmatthancock.github.io/2015/10/28/confusion-matrix.html\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n\n    # true negative, false positive, etc...\n    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n\n    NP = fn+tp # Num positive examples\n    NN = tn+fp # Num negative examples\n    N  = NP+NN\n\n    fig = plt.figure(figsize=(8,8))\n    ax  = fig.add_subplot(111)\n    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n\n    # Draw the grid boxes\n    ax.set_xlim(-0.5,2.5)\n    ax.set_ylim(2.5,-0.5)\n    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n\n    # Set xlabels\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks([0,1,2])\n    ax.set_xticklabels(class_labels + [''])\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n    # These coordinate might require some tinkering. Ditto for y, below.\n    ax.xaxis.set_label_coords(0.34,1.06)\n\n    # Set ylabels\n    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n    ax.set_yticklabels(class_labels + [''],rotation=90)\n    ax.set_yticks([0,1,2])\n    ax.yaxis.set_label_coords(-0.09,0.65)\n\n\n    # Fill in initial metrics: tp, tn, etc...\n    ax.text(0,0,\n            'True Neg: %d\\n(Num Neg: %d)'%(tn,NN),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,1,\n            'False Neg: %d'%fn,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,0,\n            'False Pos: %d'%fp,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    ax.text(1,1,\n            'True Pos: %d\\n(Num Pos: %d)'%(tp,NP),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    # Fill in secondary metrics: accuracy, true pos rate, etc...\n    ax.text(2,0,\n            'False Pos Rate: %.4f'%(fp / (fp+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,1,\n            'True Pos Rate: %.4f'%(tp / (tp+fn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,2,\n            'Accuracy: %.4f'%((tp+tn+0.)/N),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,2,\n            'Neg Pre Val: %.4f'%(1-fn/(fn+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,2,\n            'Pos Pred Val: %.4f'%(tp/(tp+fp+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\ny_pred = mod.predict(X)\n\nC = confusion_matrix(y, y_pred)    # see exercise 5.5 for more on the confusion matrix\nshow_confusion_matrix(C, ['Direction_Down', 'Direction_Up'])\n\n\n\n\n\n\n(b)\n\n\nmod.fit(X, y)\nprint(mod.intercept_, mod.coef_, (mod.predict(X) == y).mean())  # accuracy\nmod.fit(X.iloc[1:], y.iloc[1:])\nprint(mod.intercept_, mod.coef_, (mod.predict(X) == y).mean())\n\n\n\n\n[ 0.22122405] [[-0.03872222  0.0602483 ]] 0.555555555556\n[ 0.22324305] [[-0.03843317  0.06084763]] 0.556473829201\n\n\n\n(c)\n\n\nmod.predict([X.iloc[0]]), y[0]\n\n\n\n\n(array([1]), 0)\n\n\n\nThis observation was not correctly classified.\n\n\n(d)\n\n\nn = len(X)\nerrors = np.zeros(n)\n\nfor i in range(n):\n    one_out  = ~X.index.isin([i])\n    # i.\n    mod.fit(X[one_out], y[one_out])\n    # ii. iii. iv.\n    if mod.predict([X.iloc[i]]) != y[i]:\n        errors[i] = 1\n\n\n\n\n(e)\n\n\nerrors.mean()\n\n\n\n\n0.44995408631772266",
            "title": "5.7"
        },
        {
            "location": "/sols/chapter5/exercise7/#exercise-57",
            "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\ndf = pd.read_csv(\"../data/Weekly.csv\")\n\n%matplotlib inline  df.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Unnamed: 0 \n       Year \n       Lag1 \n       Lag2 \n       Lag3 \n       Lag4 \n       Lag5 \n       Volume \n       Today \n       Direction \n     \n   \n   \n     \n       0 \n       1 \n       1990 \n       0.816 \n       1.572 \n       -3.936 \n       -0.229 \n       -3.484 \n       0.154976 \n       -0.270 \n       Down \n     \n     \n       1 \n       2 \n       1990 \n       -0.270 \n       0.816 \n       1.572 \n       -3.936 \n       -0.229 \n       0.148574 \n       -2.576 \n       Down \n     \n     \n       2 \n       3 \n       1990 \n       -2.576 \n       -0.270 \n       0.816 \n       1.572 \n       -3.936 \n       0.159837 \n       3.514 \n       Up \n     \n     \n       3 \n       4 \n       1990 \n       3.514 \n       -2.576 \n       -0.270 \n       0.816 \n       1.572 \n       0.161630 \n       0.712 \n       Up \n     \n     \n       4 \n       5 \n       1990 \n       0.712 \n       3.514 \n       -2.576 \n       -0.270 \n       0.816 \n       0.153728 \n       1.178 \n       Up",
            "title": "Exercise 5.7"
        },
        {
            "location": "/sols/chapter5/exercise7/#a",
            "text": "df['Direction_Up'] = (df['Direction'] == 'Up').astype(int)  df.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Unnamed: 0 \n       Year \n       Lag1 \n       Lag2 \n       Lag3 \n       Lag4 \n       Lag5 \n       Volume \n       Today \n       Direction \n       Direction_Up \n     \n   \n   \n     \n       0 \n       1 \n       1990 \n       0.816 \n       1.572 \n       -3.936 \n       -0.229 \n       -3.484 \n       0.154976 \n       -0.270 \n       Down \n       0 \n     \n     \n       1 \n       2 \n       1990 \n       -0.270 \n       0.816 \n       1.572 \n       -3.936 \n       -0.229 \n       0.148574 \n       -2.576 \n       Down \n       0 \n     \n     \n       2 \n       3 \n       1990 \n       -2.576 \n       -0.270 \n       0.816 \n       1.572 \n       -3.936 \n       0.159837 \n       3.514 \n       Up \n       1 \n     \n     \n       3 \n       4 \n       1990 \n       3.514 \n       -2.576 \n       -0.270 \n       0.816 \n       1.572 \n       0.161630 \n       0.712 \n       Up \n       1 \n     \n     \n       4 \n       5 \n       1990 \n       0.712 \n       3.514 \n       -2.576 \n       -0.270 \n       0.816 \n       0.153728 \n       1.178 \n       Up \n       1 \n     \n      X = df[['Lag1', 'Lag2']]\ny = df['Direction_Up']  mod = LogisticRegression(C=10**6, tol=10**-7)\nmod.fit(X, y)  LogisticRegression(C=1000000, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=1e-07, verbose=0, warm_start=False)  mod.intercept_, mod.coef_  (array([ 0.22122405]), array([[-0.03872222,  0.0602483 ]]))  def show_confusion_matrix(C,class_labels=['0','1']):\n    \"\"\"\n    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n    class_labels: list of strings, default simply labels 0 and 1.\n\n    Draws confusion matrix with associated metrics.\n\n    Reference: http://notmatthancock.github.io/2015/10/28/confusion-matrix.html\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n\n    # true negative, false positive, etc...\n    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n\n    NP = fn+tp # Num positive examples\n    NN = tn+fp # Num negative examples\n    N  = NP+NN\n\n    fig = plt.figure(figsize=(8,8))\n    ax  = fig.add_subplot(111)\n    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n\n    # Draw the grid boxes\n    ax.set_xlim(-0.5,2.5)\n    ax.set_ylim(2.5,-0.5)\n    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n\n    # Set xlabels\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks([0,1,2])\n    ax.set_xticklabels(class_labels + [''])\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n    # These coordinate might require some tinkering. Ditto for y, below.\n    ax.xaxis.set_label_coords(0.34,1.06)\n\n    # Set ylabels\n    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n    ax.set_yticklabels(class_labels + [''],rotation=90)\n    ax.set_yticks([0,1,2])\n    ax.yaxis.set_label_coords(-0.09,0.65)\n\n\n    # Fill in initial metrics: tp, tn, etc...\n    ax.text(0,0,\n            'True Neg: %d\\n(Num Neg: %d)'%(tn,NN),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,1,\n            'False Neg: %d'%fn,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,0,\n            'False Pos: %d'%fp,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    ax.text(1,1,\n            'True Pos: %d\\n(Num Pos: %d)'%(tp,NP),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    # Fill in secondary metrics: accuracy, true pos rate, etc...\n    ax.text(2,0,\n            'False Pos Rate: %.4f'%(fp / (fp+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,1,\n            'True Pos Rate: %.4f'%(tp / (tp+fn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,2,\n            'Accuracy: %.4f'%((tp+tn+0.)/N),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,2,\n            'Neg Pre Val: %.4f'%(1-fn/(fn+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,2,\n            'Pos Pred Val: %.4f'%(tp/(tp+fp+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    plt.tight_layout()\n    plt.show()  y_pred = mod.predict(X)\n\nC = confusion_matrix(y, y_pred)    # see exercise 5.5 for more on the confusion matrix\nshow_confusion_matrix(C, ['Direction_Down', 'Direction_Up'])",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter5/exercise7/#b",
            "text": "mod.fit(X, y)\nprint(mod.intercept_, mod.coef_, (mod.predict(X) == y).mean())  # accuracy\nmod.fit(X.iloc[1:], y.iloc[1:])\nprint(mod.intercept_, mod.coef_, (mod.predict(X) == y).mean())  [ 0.22122405] [[-0.03872222  0.0602483 ]] 0.555555555556\n[ 0.22324305] [[-0.03843317  0.06084763]] 0.556473829201",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter5/exercise7/#c",
            "text": "mod.predict([X.iloc[0]]), y[0]  (array([1]), 0)  This observation was not correctly classified.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter5/exercise7/#d",
            "text": "n = len(X)\nerrors = np.zeros(n)\n\nfor i in range(n):\n    one_out  = ~X.index.isin([i])\n    # i.\n    mod.fit(X[one_out], y[one_out])\n    # ii. iii. iv.\n    if mod.predict([X.iloc[i]]) != y[i]:\n        errors[i] = 1",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter5/exercise7/#e",
            "text": "errors.mean()  0.44995408631772266",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter5/exercise8/",
            "text": "Exercise 5.8\n\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import LeaveOneOut  # To use cross-validation in (c); only available after scikit v0.17.1\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm  # To fit models using least squares\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\nnp.random.seed(1)  # Random numbers generated by Python are different from those generated by R, as mentioned in Exercise 3.11\n\ny = np.random.normal(size=100)  # By default np.random.normal considers a standard normal, so we just have to define the size\nx = np.random.normal(size=100)\nepsilon = np.random.normal(size=100)\n\ny = x - 2 * x**2 + epsilon\n\n\n\n\nn\n = number of observations = 100\n\n\np\n = number of predictors = 2 (X and X^2)\n\n\nModel in equation form:\n\nY = X - 2 X^{2} + \\epsilon\nY = X - 2 X^{2} + \\epsilon\n<span><span class=\"MathJax_Preview\">Y = X - 2 X^{2} + \\epsilon</span><script type=\"math/tex\">Y = X - 2 X^{2} + \\epsilon\n\n\n(b)\n\n\nplt.scatter(x, y);\n\n\n\n\n\n\nComments\n:\n\n\n\n\nQuadratic plot\n\n\nConvex function with negative concavity\n\n\nX from about -2 to 2\n\n\nY from about -10 to 2\n\n\n\n\n(c)\n\n\n# Set new random seed\nnp.random.seed(5)\n\n\n\n\n# Create LOOCV object\nloo = LeaveOneOut()\n\n\n\n\n# Organize data into a dataframe (easier to handle)\ndf = pd.DataFrame({'x':x, 'y':y})\n\n\n\n\n# Initiate variables\nmin_deg = 1  # Minimum degree of the polynomial equations considered\nmax_deg = 4+1  # Maximum degree of the polynomial equations considered\nscores = []\n\n# Compute mean squared error (MSE) for the different polynomial equations.\nfor i in range(min_deg, max_deg):\n    # Leave-one-out cross validation \n    for train, test in loo.split(df):\n        X_train = df['x'][train]\n        y_train = df['y'][train]\n        X_test = df['x'][test]\n        y_test = df['y'][test]\n\n        # Pipeline\n        model = Pipeline([('poly', PolynomialFeatures(degree = i)),\n                      ('linear', LinearRegression())])\n        model.fit(X_train[:,np.newaxis], y_train)\n\n        # MSE\n        score = mean_squared_error(y_test, model.predict(X_test[:,np.newaxis]))\n        scores.append(score)\n    print('Model %i (MSE): %f' % (i,np.mean(scores)))\n    scores = []\n\n\n\n\n/Users/disciplina/anaconda3/envs/islp/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n  warnings.warn(mesg, RuntimeWarning)\n\n\nModel 1 (MSE): 8.292212\nModel 2 (MSE): 1.017096\nModel 3 (MSE): 1.046553\nModel 4 (MSE): 1.057493\n\n\n\n(d)\n\n\n# Set new random seed\nnp.random.seed(10)\n\n\n\n\n# Compute MSE as in (c)\nmin_deg = 1  \nmax_deg = 4+1 \nscores = []\n\nfor i in range(min_deg, max_deg):\n    for train, test in loo.split(df):\n        X_train = df['x'][train]\n        y_train = df['y'][train]\n        X_test = df['x'][test]\n        y_test = df['y'][test]\n\n        model = Pipeline([('poly', PolynomialFeatures(degree = i)),\n                      ('linear', LinearRegression())])\n        model.fit(X_train[:,np.newaxis], y_train)\n\n        score = mean_squared_error(y_test, model.predict(X_test[:,np.newaxis]))\n        scores.append(score)\n    print('Model %i (MSE): %f' % (i,np.mean(scores)))\n    scores = []\n\n\n\n\nModel 1 (MSE): 8.292212\nModel 2 (MSE): 1.017096\nModel 3 (MSE): 1.046553\nModel 4 (MSE): 1.057493\n\n\n\nThe results are \nexactly the same\n because we only remove one observation from the training set. Thus, there is no random effect resulting from the observations used for the test set. LOOCV will always be the same, no matter the random seed.\n\n\n(e)\n\n\nThe model that had the smallest LOOCV error was \nmodel (ii)\n. This was an expected result because model (ii) has the same form as y (second order polynomial).\n\n\nComment:\n If we used \nnp.random.seed(0)\n instead of \nnp.random.seed(1)\n or \nnp.random.seed(10)\n, the answer to the problem would be different. Using \nseed(0)\n, the epsilon values are proportional higher to the remaining values in the y equation. Thus, its influence is greater and the random effect prevails over the second order parameters y. We didn't try what happens for other seed values, but there may be other seed values that produce a similar effect to \nseed(0)\n.\n\n\n(f)\n\n\n# Models with polynomial features\nmin_deg = 1  \nmax_deg = 4+1 \n\nfor i in range(min_deg, max_deg):\n    pol = PolynomialFeatures(degree = i)\n    X_pol = pol.fit_transform(df['x'][:,np.newaxis])\n    y = df['y']\n\n    model = sm.OLS(y, X_pol)\n    results = model.fit()\n\n    print(results.summary())  \n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.088\nModel:                            OLS   Adj. R-squared:                  0.079\nMethod:                 Least Squares   F-statistic:                     9.460\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):            0.00272\nTime:                        17:15:28   Log-Likelihood:                -242.69\nNo. Observations:                 100   AIC:                             489.4\nDf Residuals:                      98   BIC:                             494.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.7609      0.280     -6.278      0.000      -2.317      -1.204\nx1             0.9134      0.297      3.076      0.003       0.324       1.503\n==============================================================================\nOmnibus:                       40.887   Durbin-Watson:                   1.957\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               83.786\nSkew:                          -1.645   Prob(JB):                     6.40e-19\nKurtosis:                       6.048   Cond. No.                         1.19\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.882\nModel:                            OLS   Adj. R-squared:                  0.880\nMethod:                 Least Squares   F-statistic:                     362.9\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):           9.26e-46\nTime:                        17:15:28   Log-Likelihood:                -140.40\nNo. Observations:                 100   AIC:                             286.8\nDf Residuals:                      97   BIC:                             294.6\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0216      0.122     -0.177      0.860      -0.264       0.221\nx1             1.2132      0.108     11.238      0.000       0.999       1.428\nx2            -2.0014      0.078    -25.561      0.000      -2.157      -1.846\n==============================================================================\nOmnibus:                        0.094   Durbin-Watson:                   2.221\nProb(Omnibus):                  0.954   Jarque-Bera (JB):                0.009\nSkew:                          -0.022   Prob(JB):                        0.995\nKurtosis:                       2.987   Cond. No.                         2.26\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.883\nModel:                            OLS   Adj. R-squared:                  0.880\nMethod:                 Least Squares   F-statistic:                     242.1\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):           1.26e-44\nTime:                        17:15:28   Log-Likelihood:                -139.91\nNo. Observations:                 100   AIC:                             287.8\nDf Residuals:                      96   BIC:                             298.2\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0046      0.125      0.037      0.971      -0.244       0.253\nx1             1.0639      0.189      5.636      0.000       0.689       1.439\nx2            -2.0215      0.081    -24.938      0.000      -2.182      -1.861\nx3             0.0550      0.057      0.965      0.337      -0.058       0.168\n==============================================================================\nOmnibus:                        0.034   Durbin-Watson:                   2.253\nProb(Omnibus):                  0.983   Jarque-Bera (JB):                0.050\nSkew:                           0.032   Prob(JB):                        0.975\nKurtosis:                       2.911   Cond. No.                         6.55\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.885\nModel:                            OLS   Adj. R-squared:                  0.880\nMethod:                 Least Squares   F-statistic:                     182.4\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):           1.13e-43\nTime:                        17:15:28   Log-Likelihood:                -139.24\nNo. Observations:                 100   AIC:                             288.5\nDf Residuals:                      95   BIC:                             301.5\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0866      0.144      0.600      0.550      -0.200       0.373\nx1             1.0834      0.189      5.724      0.000       0.708       1.459\nx2            -2.2455      0.214    -10.505      0.000      -2.670      -1.821\nx3             0.0436      0.058      0.755      0.452      -0.071       0.158\nx4             0.0482      0.043      1.132      0.260      -0.036       0.133\n==============================================================================\nOmnibus:                        0.102   Durbin-Watson:                   2.214\nProb(Omnibus):                  0.950   Jarque-Bera (JB):                0.117\nSkew:                           0.069   Prob(JB):                        0.943\nKurtosis:                       2.906   Cond. No.                         17.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nTo answer to the question, we should pay attention to the t-statistic value of each coefficient. \n\n\nAs we can see, when we have a second order polynomial, both x1 and x2 have high t-statistic values. When we have a third order polynomial, x2 has the highest t-statistic, followed by x1 and then by x3. Finally, when we have a fourth order polynomial, x2 is the variable with the highest t-statistic, followed by x1, x4 and x3.\n\n\nWe can conclude that x2 and x1 are variables with relevance for the presented models. These results agree with the conclusions drawn based on the cross-validation results, showing that the first and second order terms are the most significant.\n\n\nReferences\n\n\n\n\nhttps://github.com/scikit-learn/scikit-learn/issues/6161 (scikit.model_selection only available after 0.17.1)\n\n\nhttp://scikit-learn.org/stable/install.html (to install scikit)\n\n\nhttp://stats.stackexchange.com/questions/58739/polynomial-regression-using-scikit-learn (polynomial linear regression)\n\n\nhttp://stackoverflow.com/questions/29241056/the-use-of-python-numpy-newaxis  (use of np.newaxis)\n\n\nhttp://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions  (about the use of pipeline)",
            "title": "5.8"
        },
        {
            "location": "/sols/chapter5/exercise8/#exercise-58",
            "text": "import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import LeaveOneOut  # To use cross-validation in (c); only available after scikit v0.17.1\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm  # To fit models using least squares\n\n%matplotlib inline",
            "title": "Exercise 5.8"
        },
        {
            "location": "/sols/chapter5/exercise8/#a",
            "text": "np.random.seed(1)  # Random numbers generated by Python are different from those generated by R, as mentioned in Exercise 3.11\n\ny = np.random.normal(size=100)  # By default np.random.normal considers a standard normal, so we just have to define the size\nx = np.random.normal(size=100)\nepsilon = np.random.normal(size=100)\n\ny = x - 2 * x**2 + epsilon  n  = number of observations = 100  p  = number of predictors = 2 (X and X^2)  Model in equation form: Y = X - 2 X^{2} + \\epsilon Y = X - 2 X^{2} + \\epsilon <span><span class=\"MathJax_Preview\">Y = X - 2 X^{2} + \\epsilon</span><script type=\"math/tex\">Y = X - 2 X^{2} + \\epsilon",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter5/exercise8/#b",
            "text": "plt.scatter(x, y);   Comments :   Quadratic plot  Convex function with negative concavity  X from about -2 to 2  Y from about -10 to 2",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter5/exercise8/#c",
            "text": "# Set new random seed\nnp.random.seed(5)  # Create LOOCV object\nloo = LeaveOneOut()  # Organize data into a dataframe (easier to handle)\ndf = pd.DataFrame({'x':x, 'y':y})  # Initiate variables\nmin_deg = 1  # Minimum degree of the polynomial equations considered\nmax_deg = 4+1  # Maximum degree of the polynomial equations considered\nscores = []\n\n# Compute mean squared error (MSE) for the different polynomial equations.\nfor i in range(min_deg, max_deg):\n    # Leave-one-out cross validation \n    for train, test in loo.split(df):\n        X_train = df['x'][train]\n        y_train = df['y'][train]\n        X_test = df['x'][test]\n        y_test = df['y'][test]\n\n        # Pipeline\n        model = Pipeline([('poly', PolynomialFeatures(degree = i)),\n                      ('linear', LinearRegression())])\n        model.fit(X_train[:,np.newaxis], y_train)\n\n        # MSE\n        score = mean_squared_error(y_test, model.predict(X_test[:,np.newaxis]))\n        scores.append(score)\n    print('Model %i (MSE): %f' % (i,np.mean(scores)))\n    scores = []  /Users/disciplina/anaconda3/envs/islp/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n  warnings.warn(mesg, RuntimeWarning)\n\n\nModel 1 (MSE): 8.292212\nModel 2 (MSE): 1.017096\nModel 3 (MSE): 1.046553\nModel 4 (MSE): 1.057493",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter5/exercise8/#d",
            "text": "# Set new random seed\nnp.random.seed(10)  # Compute MSE as in (c)\nmin_deg = 1  \nmax_deg = 4+1 \nscores = []\n\nfor i in range(min_deg, max_deg):\n    for train, test in loo.split(df):\n        X_train = df['x'][train]\n        y_train = df['y'][train]\n        X_test = df['x'][test]\n        y_test = df['y'][test]\n\n        model = Pipeline([('poly', PolynomialFeatures(degree = i)),\n                      ('linear', LinearRegression())])\n        model.fit(X_train[:,np.newaxis], y_train)\n\n        score = mean_squared_error(y_test, model.predict(X_test[:,np.newaxis]))\n        scores.append(score)\n    print('Model %i (MSE): %f' % (i,np.mean(scores)))\n    scores = []  Model 1 (MSE): 8.292212\nModel 2 (MSE): 1.017096\nModel 3 (MSE): 1.046553\nModel 4 (MSE): 1.057493  The results are  exactly the same  because we only remove one observation from the training set. Thus, there is no random effect resulting from the observations used for the test set. LOOCV will always be the same, no matter the random seed.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter5/exercise8/#e",
            "text": "The model that had the smallest LOOCV error was  model (ii) . This was an expected result because model (ii) has the same form as y (second order polynomial).  Comment:  If we used  np.random.seed(0)  instead of  np.random.seed(1)  or  np.random.seed(10) , the answer to the problem would be different. Using  seed(0) , the epsilon values are proportional higher to the remaining values in the y equation. Thus, its influence is greater and the random effect prevails over the second order parameters y. We didn't try what happens for other seed values, but there may be other seed values that produce a similar effect to  seed(0) .",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter5/exercise8/#f",
            "text": "# Models with polynomial features\nmin_deg = 1  \nmax_deg = 4+1 \n\nfor i in range(min_deg, max_deg):\n    pol = PolynomialFeatures(degree = i)\n    X_pol = pol.fit_transform(df['x'][:,np.newaxis])\n    y = df['y']\n\n    model = sm.OLS(y, X_pol)\n    results = model.fit()\n\n    print(results.summary())                                OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.088\nModel:                            OLS   Adj. R-squared:                  0.079\nMethod:                 Least Squares   F-statistic:                     9.460\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):            0.00272\nTime:                        17:15:28   Log-Likelihood:                -242.69\nNo. Observations:                 100   AIC:                             489.4\nDf Residuals:                      98   BIC:                             494.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.7609      0.280     -6.278      0.000      -2.317      -1.204\nx1             0.9134      0.297      3.076      0.003       0.324       1.503\n==============================================================================\nOmnibus:                       40.887   Durbin-Watson:                   1.957\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               83.786\nSkew:                          -1.645   Prob(JB):                     6.40e-19\nKurtosis:                       6.048   Cond. No.                         1.19\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.882\nModel:                            OLS   Adj. R-squared:                  0.880\nMethod:                 Least Squares   F-statistic:                     362.9\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):           9.26e-46\nTime:                        17:15:28   Log-Likelihood:                -140.40\nNo. Observations:                 100   AIC:                             286.8\nDf Residuals:                      97   BIC:                             294.6\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0216      0.122     -0.177      0.860      -0.264       0.221\nx1             1.2132      0.108     11.238      0.000       0.999       1.428\nx2            -2.0014      0.078    -25.561      0.000      -2.157      -1.846\n==============================================================================\nOmnibus:                        0.094   Durbin-Watson:                   2.221\nProb(Omnibus):                  0.954   Jarque-Bera (JB):                0.009\nSkew:                          -0.022   Prob(JB):                        0.995\nKurtosis:                       2.987   Cond. No.                         2.26\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.883\nModel:                            OLS   Adj. R-squared:                  0.880\nMethod:                 Least Squares   F-statistic:                     242.1\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):           1.26e-44\nTime:                        17:15:28   Log-Likelihood:                -139.91\nNo. Observations:                 100   AIC:                             287.8\nDf Residuals:                      96   BIC:                             298.2\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0046      0.125      0.037      0.971      -0.244       0.253\nx1             1.0639      0.189      5.636      0.000       0.689       1.439\nx2            -2.0215      0.081    -24.938      0.000      -2.182      -1.861\nx3             0.0550      0.057      0.965      0.337      -0.058       0.168\n==============================================================================\nOmnibus:                        0.034   Durbin-Watson:                   2.253\nProb(Omnibus):                  0.983   Jarque-Bera (JB):                0.050\nSkew:                           0.032   Prob(JB):                        0.975\nKurtosis:                       2.911   Cond. No.                         6.55\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.885\nModel:                            OLS   Adj. R-squared:                  0.880\nMethod:                 Least Squares   F-statistic:                     182.4\nDate:                Fri, 05 Jan 2018   Prob (F-statistic):           1.13e-43\nTime:                        17:15:28   Log-Likelihood:                -139.24\nNo. Observations:                 100   AIC:                             288.5\nDf Residuals:                      95   BIC:                             301.5\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0866      0.144      0.600      0.550      -0.200       0.373\nx1             1.0834      0.189      5.724      0.000       0.708       1.459\nx2            -2.2455      0.214    -10.505      0.000      -2.670      -1.821\nx3             0.0436      0.058      0.755      0.452      -0.071       0.158\nx4             0.0482      0.043      1.132      0.260      -0.036       0.133\n==============================================================================\nOmnibus:                        0.102   Durbin-Watson:                   2.214\nProb(Omnibus):                  0.950   Jarque-Bera (JB):                0.117\nSkew:                           0.069   Prob(JB):                        0.943\nKurtosis:                       2.906   Cond. No.                         17.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  To answer to the question, we should pay attention to the t-statistic value of each coefficient.   As we can see, when we have a second order polynomial, both x1 and x2 have high t-statistic values. When we have a third order polynomial, x2 has the highest t-statistic, followed by x1 and then by x3. Finally, when we have a fourth order polynomial, x2 is the variable with the highest t-statistic, followed by x1, x4 and x3.  We can conclude that x2 and x1 are variables with relevance for the presented models. These results agree with the conclusions drawn based on the cross-validation results, showing that the first and second order terms are the most significant.",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter5/exercise8/#references",
            "text": "https://github.com/scikit-learn/scikit-learn/issues/6161 (scikit.model_selection only available after 0.17.1)  http://scikit-learn.org/stable/install.html (to install scikit)  http://stats.stackexchange.com/questions/58739/polynomial-regression-using-scikit-learn (polynomial linear regression)  http://stackoverflow.com/questions/29241056/the-use-of-python-numpy-newaxis  (use of np.newaxis)  http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions  (about the use of pipeline)",
            "title": "References"
        },
        {
            "location": "/sols/chapter5/exercise9/",
            "text": "Exercise 5.9\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_boston\n\n%matplotlib inline\n\n\n\n\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['MEDV'] = pd.Series(boston.target)\nmedv = df['MEDV']\ndf.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nMEDV\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\nmu = medv.mean()\n\n\n\n\n(b)\n\n\nmedv.std()/np.sqrt(len(df))\n\n\n\n\n0.40886114749753505\n\n\n\n(c)\n\n\nmeans = [medv.sample(n = len(df), replace=True).mean() for _ in range(1000)]\nnp.std(means)\n\n\n\n\n0.4126021332293619\n\n\n\n(d)\n\n\nSE = np.std(means)\nprint(mu - 2*SE, mu + 2*SE)\n\n\n\n\n21.7076020577 23.3580105906\n\n\n\n(e)\n\n\nmedv.median()\n\n\n\n\n21.2\n\n\n\n(f)\n\n\nmedians = [medv.sample(n = len(df), replace=True).median() for _ in range(1000)]\nnp.std(medians)\n\n\n\n\n0.37716367799670181\n\n\n\n(g)\n\n\nmedv.quantile(.1)\n\n\n\n\n12.75\n\n\n\n(h)\n\n\nquantiles = [medv.sample(n = len(df), replace=True).quantile(.1) for _ in range(1000)]\nnp.std(quantiles)x\n\n\n\n\n0.50477123531358237",
            "title": "5.9"
        },
        {
            "location": "/sols/chapter5/exercise9/#exercise-59",
            "text": "import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_boston\n\n%matplotlib inline  boston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['MEDV'] = pd.Series(boston.target)\nmedv = df['MEDV']\ndf.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       MEDV \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2",
            "title": "Exercise 5.9"
        },
        {
            "location": "/sols/chapter5/exercise9/#a",
            "text": "mu = medv.mean()",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter5/exercise9/#b",
            "text": "medv.std()/np.sqrt(len(df))  0.40886114749753505",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter5/exercise9/#c",
            "text": "means = [medv.sample(n = len(df), replace=True).mean() for _ in range(1000)]\nnp.std(means)  0.4126021332293619",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter5/exercise9/#d",
            "text": "SE = np.std(means)\nprint(mu - 2*SE, mu + 2*SE)  21.7076020577 23.3580105906",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter5/exercise9/#e",
            "text": "medv.median()  21.2",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter5/exercise9/#f",
            "text": "medians = [medv.sample(n = len(df), replace=True).median() for _ in range(1000)]\nnp.std(medians)  0.37716367799670181",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter5/exercise9/#g",
            "text": "medv.quantile(.1)  12.75",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter5/exercise9/#h",
            "text": "quantiles = [medv.sample(n = len(df), replace=True).quantile(.1) for _ in range(1000)]\nnp.std(quantiles)x  0.50477123531358237",
            "title": "(h)"
        },
        {
            "location": "/sols/chapter6/exercise1/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.1"
        },
        {
            "location": "/sols/chapter6/exercise2/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.2"
        },
        {
            "location": "/sols/chapter6/exercise3/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.3"
        },
        {
            "location": "/sols/chapter6/exercise4/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.4"
        },
        {
            "location": "/sols/chapter6/exercise5/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.5"
        },
        {
            "location": "/sols/chapter6/exercise6/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.6"
        },
        {
            "location": "/sols/chapter6/exercise7/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.7"
        },
        {
            "location": "/sols/chapter6/exercise8/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.8"
        },
        {
            "location": "/sols/chapter6/exercise9/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.9"
        },
        {
            "location": "/sols/chapter6/exercise10/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.10"
        },
        {
            "location": "/sols/chapter6/exercise11/",
            "text": "We're publishing the solutions to a new chapter at least once a month. \n\n\nAt the latest, Chapter 6 will be out in February 2018.",
            "title": "6.11"
        },
        {
            "location": "/sols/chapter7/exercise1/",
            "text": "Exercise 7.1",
            "title": "7.1"
        },
        {
            "location": "/sols/chapter7/exercise1/#exercise-71",
            "text": "",
            "title": "Exercise 7.1"
        },
        {
            "location": "/sols/chapter7/exercise2/",
            "text": "Exercise 7.2\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\n\n\n\\hat{g} = arg\n\\hat{g} = arg\n \nmin_g(\\sum_{i=1}^{n} (y_i - g(x_i))^2 + \\lambda \\int(g^{(m)}(x))^2 dx)\nmin_g(\\sum_{i=1}^{n} (y_i - g(x_i))^2 + \\lambda \\int(g^{(m)}(x))^2 dx)\n\n\narg\narg\n \nmin_g\nmin_g\n is the value of \ng\n that minimizes the function.\n\n\n(a)\n\n\nWhen \n\\lambda = \\infty\n\\lambda = \\infty\n the first term loses significance and can be ignored. If \ng^{(0)}(x) = g(x) = 0\ng^{(0)}(x) = g(x) = 0\n the function will be minimized, which means that \n\\hat{g}\n\\hat{g}\n must be 0\n. \n\n\n# Example sketch\nx = np.arange(0,10,1)\ny = np.full(10,0,dtype='int')\n\nplt.plot(x,y,'-r');\n\n\n\n\n\n\n(b)\n\n\nWhen \n\\lambda = \\infty\n\\lambda = \\infty\n the first term loses significance and can be ignored. If \ng^{(1)}(x) = g'(x) = c\ng^{(1)}(x) = g'(x) = c\n the function will be minimized because the first derivative of a constant is 0. This means that \n\\hat{g}\n\\hat{g}\n must be an horizontal line\n. \n\n\n# Example sketch\n# We used c=5 but it could have been done with any other c.\nx = np.arange(0,10,1)\ny = np.full(10,5,dtype='int')\n\nplt.plot(x,y,'-r');\n\n\n\n\n\n\n(c)\n\n\nWhen \n\\lambda = \\infty\n\\lambda = \\infty\n the first term loses significance and can be ignored. If \ng^{(2)}(x) = g''(x) = bx + c\ng^{(2)}(x) = g''(x) = bx + c\n the function will be minimized because the second derivative of a linear function is 0. This means that \n\\hat{g}\n\\hat{g}\n must be a linear function\n. \n\n\n# Example sketch\n# We used y=x but it could have been done with any other linear function.\nx = np.arange(0,10,1)\ny = np.arange(0,10,1)\n\nplt.plot(x,y,'-r');\n\n\n\n\n\n\n(d)\n\n\nWhen \n\\lambda = \\infty\n\\lambda = \\infty\n the first term loses significance and can be ignored. If \ng^{(3)}(x) = g'''(x) = ax^2 + bx + c\ng^{(3)}(x) = g'''(x) = ax^2 + bx + c\n the function will be minimized because the third derivative of a quadratic function is 0. This means that \n\\hat{g}\n\\hat{g}\n must be a quadratic function\n. \n\n\n# Example sketch\n# We used y=x^2 but it could have been done with any other quadratic function.\nx = np.arange(0,10,1)\ny = np.arange(0,10,1)**2\n\nplt.plot(x,y,'-r');\n\n\n\n\n\n\n(e)\n\n\nThis situation corresponds to a linear regression least squares fit. If \n\\lambda = 0\n\\lambda = 0\n, the second term loses significance and can be ignored. Therefore, the function will be minimized when \n\\sum_{i=1}^{n} (y_i - g(x_i)^2\n\\sum_{i=1}^{n} (y_i - g(x_i)^2\n is minimum. This means that \n\\hat{g}\n\\hat{g}\n must be such that it interpolates all of the \ny_i\ny_i\n.\n\n\nSince there are many different shapes that can express this situation, we didn't draw any example sketch.",
            "title": "7.2"
        },
        {
            "location": "/sols/chapter7/exercise2/#exercise-72",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline  \\hat{g} = arg \\hat{g} = arg   min_g(\\sum_{i=1}^{n} (y_i - g(x_i))^2 + \\lambda \\int(g^{(m)}(x))^2 dx) min_g(\\sum_{i=1}^{n} (y_i - g(x_i))^2 + \\lambda \\int(g^{(m)}(x))^2 dx)  arg arg   min_g min_g  is the value of  g  that minimizes the function.",
            "title": "Exercise 7.2"
        },
        {
            "location": "/sols/chapter7/exercise2/#a",
            "text": "When  \\lambda = \\infty \\lambda = \\infty  the first term loses significance and can be ignored. If  g^{(0)}(x) = g(x) = 0 g^{(0)}(x) = g(x) = 0  the function will be minimized, which means that  \\hat{g} \\hat{g}  must be 0 .   # Example sketch\nx = np.arange(0,10,1)\ny = np.full(10,0,dtype='int')\n\nplt.plot(x,y,'-r');",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter7/exercise2/#b",
            "text": "When  \\lambda = \\infty \\lambda = \\infty  the first term loses significance and can be ignored. If  g^{(1)}(x) = g'(x) = c g^{(1)}(x) = g'(x) = c  the function will be minimized because the first derivative of a constant is 0. This means that  \\hat{g} \\hat{g}  must be an horizontal line .   # Example sketch\n# We used c=5 but it could have been done with any other c.\nx = np.arange(0,10,1)\ny = np.full(10,5,dtype='int')\n\nplt.plot(x,y,'-r');",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter7/exercise2/#c",
            "text": "When  \\lambda = \\infty \\lambda = \\infty  the first term loses significance and can be ignored. If  g^{(2)}(x) = g''(x) = bx + c g^{(2)}(x) = g''(x) = bx + c  the function will be minimized because the second derivative of a linear function is 0. This means that  \\hat{g} \\hat{g}  must be a linear function .   # Example sketch\n# We used y=x but it could have been done with any other linear function.\nx = np.arange(0,10,1)\ny = np.arange(0,10,1)\n\nplt.plot(x,y,'-r');",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter7/exercise2/#d",
            "text": "When  \\lambda = \\infty \\lambda = \\infty  the first term loses significance and can be ignored. If  g^{(3)}(x) = g'''(x) = ax^2 + bx + c g^{(3)}(x) = g'''(x) = ax^2 + bx + c  the function will be minimized because the third derivative of a quadratic function is 0. This means that  \\hat{g} \\hat{g}  must be a quadratic function .   # Example sketch\n# We used y=x^2 but it could have been done with any other quadratic function.\nx = np.arange(0,10,1)\ny = np.arange(0,10,1)**2\n\nplt.plot(x,y,'-r');",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter7/exercise2/#e",
            "text": "This situation corresponds to a linear regression least squares fit. If  \\lambda = 0 \\lambda = 0 , the second term loses significance and can be ignored. Therefore, the function will be minimized when  \\sum_{i=1}^{n} (y_i - g(x_i)^2 \\sum_{i=1}^{n} (y_i - g(x_i)^2  is minimum. This means that  \\hat{g} \\hat{g}  must be such that it interpolates all of the  y_i y_i .  Since there are many different shapes that can express this situation, we didn't draw any example sketch.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter7/exercise3/",
            "text": "Exercise 7.3",
            "title": "7.3"
        },
        {
            "location": "/sols/chapter7/exercise3/#exercise-73",
            "text": "",
            "title": "Exercise 7.3"
        },
        {
            "location": "/sols/chapter7/exercise4/",
            "text": "Exercise 7.4\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\n\n\nTo solve this exercise, we just have to replace X in the expression. The main expression is:\n\n\n\n\nY =\\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\epsilon \n\n\nY =\\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\epsilon \n\n\n\n\nwhere \n\\hat{\\beta}_0 = 1\n\\hat{\\beta}_0 = 1\n, \n\\hat{\\beta}_1 = 1\n\\hat{\\beta}_1 = 1\n and \n\\hat{\\beta}_2 = 3\n\\hat{\\beta}_2 = 3\n. Accordingly, we get the expression:\n\n\n\n\nY = 1 + b_1(X) + 3 \\times b_2(X) + \\epsilon \n\n\nY = 1 + b_1(X) + 3 \\times b_2(X) + \\epsilon \n\n\n\n\nConsidering that \nI(a \\leq X \\leq b) = 1\nI(a \\leq X \\leq b) = 1\n as long as X is in the interval [a,b] (being zero otherwise), and since we are just estimating the curve between the interval [-2,2], our expression turns into:\n\n\n\n\nY = 1 + b_1(X) + \\epsilon \n\n\nY = 1 + b_1(X) + \\epsilon \n\n\n\n\nAt this point, we can make the necessary computations to solve the exercise:\n\n\n\n\nX=-2 \\implies Y = 1\nX=-2 \\implies Y = 1\n\n\nX=-1 \\implies Y = 1\nX=-1 \\implies Y = 1\n\n\nX=0 \\implies Y = 2\nX=0 \\implies Y = 2\n\n\nX=1 \\implies Y = 2\nX=1 \\implies Y = 2\n\n\nX=2 \\implies Y = 1\nX=2 \\implies Y = 1\n\n\n\n\n# Plot\nx = np.arange(-2,3,1)\ny = [1,1,2,2,1]\n\n\nplt.xlim([-2,2])\nplt.ylim([0,2.5])\nplt.plot(x,y,'-r');\n\n\n\n\n[<matplotlib.lines.Line2D at 0x81007f0>]",
            "title": "7.4"
        },
        {
            "location": "/sols/chapter7/exercise4/#exercise-74",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline  To solve this exercise, we just have to replace X in the expression. The main expression is:   Y =\\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\epsilon   Y =\\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\epsilon    where  \\hat{\\beta}_0 = 1 \\hat{\\beta}_0 = 1 ,  \\hat{\\beta}_1 = 1 \\hat{\\beta}_1 = 1  and  \\hat{\\beta}_2 = 3 \\hat{\\beta}_2 = 3 . Accordingly, we get the expression:   Y = 1 + b_1(X) + 3 \\times b_2(X) + \\epsilon   Y = 1 + b_1(X) + 3 \\times b_2(X) + \\epsilon    Considering that  I(a \\leq X \\leq b) = 1 I(a \\leq X \\leq b) = 1  as long as X is in the interval [a,b] (being zero otherwise), and since we are just estimating the curve between the interval [-2,2], our expression turns into:   Y = 1 + b_1(X) + \\epsilon   Y = 1 + b_1(X) + \\epsilon    At this point, we can make the necessary computations to solve the exercise:   X=-2 \\implies Y = 1 X=-2 \\implies Y = 1  X=-1 \\implies Y = 1 X=-1 \\implies Y = 1  X=0 \\implies Y = 2 X=0 \\implies Y = 2  X=1 \\implies Y = 2 X=1 \\implies Y = 2  X=2 \\implies Y = 1 X=2 \\implies Y = 1   # Plot\nx = np.arange(-2,3,1)\ny = [1,1,2,2,1]\n\n\nplt.xlim([-2,2])\nplt.ylim([0,2.5])\nplt.plot(x,y,'-r');  [<matplotlib.lines.Line2D at 0x81007f0>]",
            "title": "Exercise 7.4"
        },
        {
            "location": "/sols/chapter7/exercise5/",
            "text": "Exercise 7.5",
            "title": "7.5"
        },
        {
            "location": "/sols/chapter7/exercise5/#exercise-75",
            "text": "",
            "title": "Exercise 7.5"
        },
        {
            "location": "/sols/chapter7/exercise6/",
            "text": "Exercise 7.6\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import f_regression\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/Wage.csv', index_col=0)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nyear\n\n      \nage\n\n      \nsex\n\n      \nmaritl\n\n      \nrace\n\n      \neducation\n\n      \nregion\n\n      \njobclass\n\n      \nhealth\n\n      \nhealth_ins\n\n      \nlogwage\n\n      \nwage\n\n    \n\n  \n\n  \n\n    \n\n      \n231655\n\n      \n2006\n\n      \n18\n\n      \n1. Male\n\n      \n1. Never Married\n\n      \n1. White\n\n      \n1. < HS Grad\n\n      \n2. Middle Atlantic\n\n      \n1. Industrial\n\n      \n1. <=Good\n\n      \n2. No\n\n      \n4.318063\n\n      \n75.043154\n\n    \n\n    \n\n      \n86582\n\n      \n2004\n\n      \n24\n\n      \n1. Male\n\n      \n1. Never Married\n\n      \n1. White\n\n      \n4. College Grad\n\n      \n2. Middle Atlantic\n\n      \n2. Information\n\n      \n2. >=Very Good\n\n      \n2. No\n\n      \n4.255273\n\n      \n70.476020\n\n    \n\n    \n\n      \n161300\n\n      \n2003\n\n      \n45\n\n      \n1. Male\n\n      \n2. Married\n\n      \n1. White\n\n      \n3. Some College\n\n      \n2. Middle Atlantic\n\n      \n1. Industrial\n\n      \n1. <=Good\n\n      \n1. Yes\n\n      \n4.875061\n\n      \n130.982177\n\n    \n\n    \n\n      \n155159\n\n      \n2003\n\n      \n43\n\n      \n1. Male\n\n      \n2. Married\n\n      \n3. Asian\n\n      \n4. College Grad\n\n      \n2. Middle Atlantic\n\n      \n2. Information\n\n      \n2. >=Very Good\n\n      \n1. Yes\n\n      \n5.041393\n\n      \n154.685293\n\n    \n\n    \n\n      \n11443\n\n      \n2005\n\n      \n50\n\n      \n1. Male\n\n      \n4. Divorced\n\n      \n1. White\n\n      \n2. HS Grad\n\n      \n2. Middle Atlantic\n\n      \n2. Information\n\n      \n1. <=Good\n\n      \n1. Yes\n\n      \n4.318063\n\n      \n75.043154\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\n# Model variables\ny = df['wage'][:,np.newaxis]\nX = df['age'][:,np.newaxis]\n\n\n\n\n# Compute regression models with different degrees\n# Variable 'scores' saves mean squared errors resulting from the different degrees models.\n# Cross validation is used\nscores = []  \nfor i in range(0,11):\n    model = Pipeline([('poly', PolynomialFeatures(degree=i)), ('linear', LinearRegression())])\n    model.fit(X,y)\n\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    scores.append(np.mean(score))\n\nscores = np.abs(scores)  # Scikit computes negative mean square errors, so we need to turn the values positive.\n\n\n\n\n# Plot errors\nx_plot = np.arange(0,11)\n\nplt.plot(x_plot, scores)\nplt.ylabel('Mean squared error (CV)')\nplt.xlabel('Degrees')\nplt.xlim(0,10)\nplt.show()\n\n\n\n\n\n\n# Print array element correspoding to the minimum mean squared error\n# Element number = polynomial degree for minimum mean squared error\nprint(np.where(scores == np.min(scores)))\n\n\n\n\n(array([4], dtype=int64),)\n\n\n\nOptimal degree \nd\n for the polynomial according to cross-validation: \n4\n\n\nWe will use \nstatsmodels\n to perform the hypothesis testing using ANOVA. \nStatsmodels\n has a built-in function that simplifies our job and we didn't find an equivalent way of solving the problem with \nscikit-learn\n.\n\n\n# Fit polynomial models to use in statsmodels.\nmodels=[]\nfor i in range(0,11):\n    poly = PolynomialFeatures(degree=i)\n    X_pol = poly.fit_transform(X)\n    model = smf.GLS(y, X_pol).fit()\n    models.append(model)\n\n\n\n\n# Hypothesis testing using ANOVA\nsm.stats.anova_lm(models[0], models[1], models[2], models[3], models[4], models[5], models[6], typ=1)\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ndf_resid\n\n      \nssr\n\n      \ndf_diff\n\n      \nss_diff\n\n      \nF\n\n      \nPr(>F)\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n2999.0\n\n      \n5.222086e+06\n\n      \n0.0\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1\n\n      \n2998.0\n\n      \n5.022216e+06\n\n      \n1.0\n\n      \n199869.664970\n\n      \n125.505882\n\n      \n1.444930e-28\n\n    \n\n    \n\n      \n2\n\n      \n2997.0\n\n      \n4.793430e+06\n\n      \n1.0\n\n      \n228786.010128\n\n      \n143.663571\n\n      \n2.285169e-32\n\n    \n\n    \n\n      \n3\n\n      \n2996.0\n\n      \n4.777674e+06\n\n      \n1.0\n\n      \n15755.693664\n\n      \n9.893609\n\n      \n1.674794e-03\n\n    \n\n    \n\n      \n4\n\n      \n2995.0\n\n      \n4.771604e+06\n\n      \n1.0\n\n      \n6070.152124\n\n      \n3.811683\n\n      \n5.098933e-02\n\n    \n\n    \n\n      \n5\n\n      \n2994.0\n\n      \n4.770322e+06\n\n      \n1.0\n\n      \n1282.563017\n\n      \n0.805371\n\n      \n3.695646e-01\n\n    \n\n    \n\n      \n6\n\n      \n2993.0\n\n      \n4.766389e+06\n\n      \n1.0\n\n      \n3932.257136\n\n      \n2.469216\n\n      \n1.162015e-01\n\n    \n\n  \n\n\n\n\n\n\n\nThe lower the values of F, the lower the significance of the coefficient. Degrees higher than 4 don't improve the polynomial regression model significantly. This results is in \nagreement with cross validation results\n.\n\n\n# Save optimal degree\nopt_degree = 4\n\n\n\n\n# Plot polynomial regression\n# Auxiliary variables X_line and y_line are created.\n# These variables allow us to draw the polynomial regression.\n# np.linspace() is used to create an ordered sequence of numbers. Then we can plot the polynomial regression.\nmodel = Pipeline([('poly', PolynomialFeatures(degree = opt_degree)), ('linear', LinearRegression())])\nmodel.fit(X,y)\n\nX_lin = np.linspace(18,80)[:,np.newaxis]\ny_lin = model.predict(X_lin)\n\nplt.scatter(X,y)\nplt.plot(X_lin, y_lin,'-r');\n\n\n\n\n\n\n(b)\n\n\n# Compute cross-validated errors of step function\n'''\nTo define the step function, we need to cut the dataset into parts (pd.cut() does the job)\nand associate a each part to a dummy variable. For example, if we have two parts (age<50\nand age >= 50), we will have one dummy variable that gets value 1 if age<50 and value 0\nif age>50.\n\nOnce we have the dataset in these conditions, we need to fit a linear regression to it.\nThe governing model will be defined by: y = b0 + b1 C1 + b2 C2 + ... + bn Cn, where\n  b stands for the regression coefficient;\n  C stands for the value of a dummy variable.\n\nUsing the same example as above, we have y = b0 + b1 C1, thus ...\n'''\nscores = []\nfor i in range(1,10):\n    age_groups = pd.cut(df['age'], i)\n    df_dummies = pd.get_dummies(age_groups)\n\n    X_cv = df_dummies\n    y_cv = df['wage']\n\n    model.fit(X_cv, y_cv)\n    score = cross_val_score(model, X_cv, y_cv, cv=5, scoring='neg_mean_squared_error')\n    scores.append(score)\n\nscores = np.abs(scores)  # Scikit computes negative mean square errors, so we need to turn the values positive.\n\n\n\n\n# Number of cuts that minimize the error\nmin_scores = []\nfor i in range(0,9):\n    min_score = np.mean(scores[i,:])\n    min_scores.append(min_score)\n\n    print('Number of cuts: %i, error %.3f' % (i+1, min_score))\n\n\n\n\nNumber of cuts: 1, error 1741.335\nNumber of cuts: 2, error 1733.925\nNumber of cuts: 3, error 1687.688\nNumber of cuts: 4, error 1635.756\nNumber of cuts: 5, error 1635.556\nNumber of cuts: 6, error 1627.397\nNumber of cuts: 7, error 1619.168\nNumber of cuts: 8, error 1607.926\nNumber of cuts: 9, error 1616.550\n\n\n\nThe number of cuts that minimize the error is \n8\n.\n\n\n# Plot\n# The following code shows, step by step, how to plot the step function.\n\n\n\n\n# Convert ages to groups of age ranges\nn_groups = 8\nage_groups = pd.cut(df['age'], n_groups)\n\n\n\n\n# Dummy variables\n# Dummy variables is a way to deal with categorical variables in linear regressions.\n# It associates the value 1 to the group to which the variable belongs, and the value 0 to the remaining groups.\n# For example, if age == 20, the (18,25] will have the value 1 while the group (25, 32] will have the value 0.\nage_dummies = pd.get_dummies(age_groups)\n\n\n\n\n# Dataset for step function\n# Add wage to the dummy dataset.\ndf_step = age_dummies.join(df['wage'])\n\ndf_step.head()  # Just to visualize the dataset with the specified number of cuts\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \n(17.938, 25.75]\n\n      \n(25.75, 33.5]\n\n      \n(33.5, 41.25]\n\n      \n(41.25, 49]\n\n      \n(49, 56.75]\n\n      \n(56.75, 64.5]\n\n      \n(64.5, 72.25]\n\n      \n(72.25, 80]\n\n      \nwage\n\n    \n\n  \n\n  \n\n    \n\n      \n231655\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n75.043154\n\n    \n\n    \n\n      \n86582\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n70.476020\n\n    \n\n    \n\n      \n161300\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n130.982177\n\n    \n\n    \n\n      \n155159\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n154.685293\n\n    \n\n    \n\n      \n11443\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n75.043154\n\n    \n\n  \n\n\n\n\n\n\n\n# Variables to fit the step function\n# X == dummy variables; y == wage.\nX_step = df_step.iloc[:,:-1]\ny_step = df_step.iloc[:,-1]\n\n\n\n\n# Fit step function (statsmodels)\nreg = sm.GLM(y_step[:,np.newaxis], X_step).fit()\n\n\n\n\n# Auxiliary data to plot the step function\n# We need to create a comprehensive set of ordered points to draw the figure.\n# These points are based on 'age' values but include also the dummy variables identifiying the group that 'age' belongs.\nX_aux = np.linspace(18,80)\ngroups_aux = pd.cut(X_aux, n_groups)\naux_dummies = pd.get_dummies(groups_aux)\n\n\n\n\n# Plot step function\nX_step_lin = np.linspace(18,80)\ny_lin = reg.predict(aux_dummies)\n\nplt.scatter(X,y)\nplt.plot(X_step_lin, y_lin,'-r');\n\n\n\n\n\n\nReferences\n\n\n\n\nhttp://statsmodels.sourceforge.net/stable/examples/notebooks/generated/glm_formula.html\n\n\nhttp://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html",
            "title": "7.6"
        },
        {
            "location": "/sols/chapter7/exercise6/#exercise-76",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import f_regression\n\n%matplotlib inline  df = pd.read_csv('../data/Wage.csv', index_col=0)  df.head()   \n   \n     \n       \n       year \n       age \n       sex \n       maritl \n       race \n       education \n       region \n       jobclass \n       health \n       health_ins \n       logwage \n       wage \n     \n   \n   \n     \n       231655 \n       2006 \n       18 \n       1. Male \n       1. Never Married \n       1. White \n       1. < HS Grad \n       2. Middle Atlantic \n       1. Industrial \n       1. <=Good \n       2. No \n       4.318063 \n       75.043154 \n     \n     \n       86582 \n       2004 \n       24 \n       1. Male \n       1. Never Married \n       1. White \n       4. College Grad \n       2. Middle Atlantic \n       2. Information \n       2. >=Very Good \n       2. No \n       4.255273 \n       70.476020 \n     \n     \n       161300 \n       2003 \n       45 \n       1. Male \n       2. Married \n       1. White \n       3. Some College \n       2. Middle Atlantic \n       1. Industrial \n       1. <=Good \n       1. Yes \n       4.875061 \n       130.982177 \n     \n     \n       155159 \n       2003 \n       43 \n       1. Male \n       2. Married \n       3. Asian \n       4. College Grad \n       2. Middle Atlantic \n       2. Information \n       2. >=Very Good \n       1. Yes \n       5.041393 \n       154.685293 \n     \n     \n       11443 \n       2005 \n       50 \n       1. Male \n       4. Divorced \n       1. White \n       2. HS Grad \n       2. Middle Atlantic \n       2. Information \n       1. <=Good \n       1. Yes \n       4.318063 \n       75.043154",
            "title": "Exercise 7.6"
        },
        {
            "location": "/sols/chapter7/exercise6/#a",
            "text": "# Model variables\ny = df['wage'][:,np.newaxis]\nX = df['age'][:,np.newaxis]  # Compute regression models with different degrees\n# Variable 'scores' saves mean squared errors resulting from the different degrees models.\n# Cross validation is used\nscores = []  \nfor i in range(0,11):\n    model = Pipeline([('poly', PolynomialFeatures(degree=i)), ('linear', LinearRegression())])\n    model.fit(X,y)\n\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    scores.append(np.mean(score))\n\nscores = np.abs(scores)  # Scikit computes negative mean square errors, so we need to turn the values positive.  # Plot errors\nx_plot = np.arange(0,11)\n\nplt.plot(x_plot, scores)\nplt.ylabel('Mean squared error (CV)')\nplt.xlabel('Degrees')\nplt.xlim(0,10)\nplt.show()   # Print array element correspoding to the minimum mean squared error\n# Element number = polynomial degree for minimum mean squared error\nprint(np.where(scores == np.min(scores)))  (array([4], dtype=int64),)  Optimal degree  d  for the polynomial according to cross-validation:  4  We will use  statsmodels  to perform the hypothesis testing using ANOVA.  Statsmodels  has a built-in function that simplifies our job and we didn't find an equivalent way of solving the problem with  scikit-learn .  # Fit polynomial models to use in statsmodels.\nmodels=[]\nfor i in range(0,11):\n    poly = PolynomialFeatures(degree=i)\n    X_pol = poly.fit_transform(X)\n    model = smf.GLS(y, X_pol).fit()\n    models.append(model)  # Hypothesis testing using ANOVA\nsm.stats.anova_lm(models[0], models[1], models[2], models[3], models[4], models[5], models[6], typ=1)   \n   \n     \n       \n       df_resid \n       ssr \n       df_diff \n       ss_diff \n       F \n       Pr(>F) \n     \n   \n   \n     \n       0 \n       2999.0 \n       5.222086e+06 \n       0.0 \n       NaN \n       NaN \n       NaN \n     \n     \n       1 \n       2998.0 \n       5.022216e+06 \n       1.0 \n       199869.664970 \n       125.505882 \n       1.444930e-28 \n     \n     \n       2 \n       2997.0 \n       4.793430e+06 \n       1.0 \n       228786.010128 \n       143.663571 \n       2.285169e-32 \n     \n     \n       3 \n       2996.0 \n       4.777674e+06 \n       1.0 \n       15755.693664 \n       9.893609 \n       1.674794e-03 \n     \n     \n       4 \n       2995.0 \n       4.771604e+06 \n       1.0 \n       6070.152124 \n       3.811683 \n       5.098933e-02 \n     \n     \n       5 \n       2994.0 \n       4.770322e+06 \n       1.0 \n       1282.563017 \n       0.805371 \n       3.695646e-01 \n     \n     \n       6 \n       2993.0 \n       4.766389e+06 \n       1.0 \n       3932.257136 \n       2.469216 \n       1.162015e-01 \n     \n      The lower the values of F, the lower the significance of the coefficient. Degrees higher than 4 don't improve the polynomial regression model significantly. This results is in  agreement with cross validation results .  # Save optimal degree\nopt_degree = 4  # Plot polynomial regression\n# Auxiliary variables X_line and y_line are created.\n# These variables allow us to draw the polynomial regression.\n# np.linspace() is used to create an ordered sequence of numbers. Then we can plot the polynomial regression.\nmodel = Pipeline([('poly', PolynomialFeatures(degree = opt_degree)), ('linear', LinearRegression())])\nmodel.fit(X,y)\n\nX_lin = np.linspace(18,80)[:,np.newaxis]\ny_lin = model.predict(X_lin)\n\nplt.scatter(X,y)\nplt.plot(X_lin, y_lin,'-r');",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter7/exercise6/#b",
            "text": "# Compute cross-validated errors of step function\n'''\nTo define the step function, we need to cut the dataset into parts (pd.cut() does the job)\nand associate a each part to a dummy variable. For example, if we have two parts (age<50\nand age >= 50), we will have one dummy variable that gets value 1 if age<50 and value 0\nif age>50.\n\nOnce we have the dataset in these conditions, we need to fit a linear regression to it.\nThe governing model will be defined by: y = b0 + b1 C1 + b2 C2 + ... + bn Cn, where\n  b stands for the regression coefficient;\n  C stands for the value of a dummy variable.\n\nUsing the same example as above, we have y = b0 + b1 C1, thus ...\n'''\nscores = []\nfor i in range(1,10):\n    age_groups = pd.cut(df['age'], i)\n    df_dummies = pd.get_dummies(age_groups)\n\n    X_cv = df_dummies\n    y_cv = df['wage']\n\n    model.fit(X_cv, y_cv)\n    score = cross_val_score(model, X_cv, y_cv, cv=5, scoring='neg_mean_squared_error')\n    scores.append(score)\n\nscores = np.abs(scores)  # Scikit computes negative mean square errors, so we need to turn the values positive.  # Number of cuts that minimize the error\nmin_scores = []\nfor i in range(0,9):\n    min_score = np.mean(scores[i,:])\n    min_scores.append(min_score)\n\n    print('Number of cuts: %i, error %.3f' % (i+1, min_score))  Number of cuts: 1, error 1741.335\nNumber of cuts: 2, error 1733.925\nNumber of cuts: 3, error 1687.688\nNumber of cuts: 4, error 1635.756\nNumber of cuts: 5, error 1635.556\nNumber of cuts: 6, error 1627.397\nNumber of cuts: 7, error 1619.168\nNumber of cuts: 8, error 1607.926\nNumber of cuts: 9, error 1616.550  The number of cuts that minimize the error is  8 .  # Plot\n# The following code shows, step by step, how to plot the step function.  # Convert ages to groups of age ranges\nn_groups = 8\nage_groups = pd.cut(df['age'], n_groups)  # Dummy variables\n# Dummy variables is a way to deal with categorical variables in linear regressions.\n# It associates the value 1 to the group to which the variable belongs, and the value 0 to the remaining groups.\n# For example, if age == 20, the (18,25] will have the value 1 while the group (25, 32] will have the value 0.\nage_dummies = pd.get_dummies(age_groups)  # Dataset for step function\n# Add wage to the dummy dataset.\ndf_step = age_dummies.join(df['wage'])\n\ndf_step.head()  # Just to visualize the dataset with the specified number of cuts   \n   \n     \n       \n       (17.938, 25.75] \n       (25.75, 33.5] \n       (33.5, 41.25] \n       (41.25, 49] \n       (49, 56.75] \n       (56.75, 64.5] \n       (64.5, 72.25] \n       (72.25, 80] \n       wage \n     \n   \n   \n     \n       231655 \n       1.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       75.043154 \n     \n     \n       86582 \n       1.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       70.476020 \n     \n     \n       161300 \n       0.0 \n       0.0 \n       0.0 \n       1.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       130.982177 \n     \n     \n       155159 \n       0.0 \n       0.0 \n       0.0 \n       1.0 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       154.685293 \n     \n     \n       11443 \n       0.0 \n       0.0 \n       0.0 \n       0.0 \n       1.0 \n       0.0 \n       0.0 \n       0.0 \n       75.043154 \n     \n      # Variables to fit the step function\n# X == dummy variables; y == wage.\nX_step = df_step.iloc[:,:-1]\ny_step = df_step.iloc[:,-1]  # Fit step function (statsmodels)\nreg = sm.GLM(y_step[:,np.newaxis], X_step).fit()  # Auxiliary data to plot the step function\n# We need to create a comprehensive set of ordered points to draw the figure.\n# These points are based on 'age' values but include also the dummy variables identifiying the group that 'age' belongs.\nX_aux = np.linspace(18,80)\ngroups_aux = pd.cut(X_aux, n_groups)\naux_dummies = pd.get_dummies(groups_aux)  # Plot step function\nX_step_lin = np.linspace(18,80)\ny_lin = reg.predict(aux_dummies)\n\nplt.scatter(X,y)\nplt.plot(X_step_lin, y_lin,'-r');",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter7/exercise6/#references",
            "text": "http://statsmodels.sourceforge.net/stable/examples/notebooks/generated/glm_formula.html  http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html",
            "title": "References"
        },
        {
            "location": "/sols/chapter7/exercise7/",
            "text": "Exercise 7.7",
            "title": "7.7"
        },
        {
            "location": "/sols/chapter7/exercise7/#exercise-77",
            "text": "",
            "title": "Exercise 7.7"
        },
        {
            "location": "/sols/chapter7/exercise8/",
            "text": "Exercise 7.8\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom patsy import dmatrix\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/Auto.csv')\n\n\n\n\n# Visualize dataset\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nmpg\n\n      \ncylinders\n\n      \ndisplacement\n\n      \nhorsepower\n\n      \nweight\n\n      \nacceleration\n\n      \nyear\n\n      \norigin\n\n      \nname\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n18.0\n\n      \n8\n\n      \n307.0\n\n      \n130\n\n      \n3504\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \nchevrolet chevelle malibu\n\n    \n\n    \n\n      \n1\n\n      \n15.0\n\n      \n8\n\n      \n350.0\n\n      \n165\n\n      \n3693\n\n      \n11.5\n\n      \n70\n\n      \n1\n\n      \nbuick skylark 320\n\n    \n\n    \n\n      \n2\n\n      \n18.0\n\n      \n8\n\n      \n318.0\n\n      \n150\n\n      \n3436\n\n      \n11.0\n\n      \n70\n\n      \n1\n\n      \nplymouth satellite\n\n    \n\n    \n\n      \n3\n\n      \n16.0\n\n      \n8\n\n      \n304.0\n\n      \n150\n\n      \n3433\n\n      \n12.0\n\n      \n70\n\n      \n1\n\n      \namc rebel sst\n\n    \n\n    \n\n      \n4\n\n      \n17.0\n\n      \n8\n\n      \n302.0\n\n      \n140\n\n      \n3449\n\n      \n10.5\n\n      \n70\n\n      \n1\n\n      \nford torino\n\n    \n\n  \n\n\n\n\n\n\n\nPre-processing data\n\n\n# 'horsepower' is a string\n# We would like it to be a integer, in order to appear in scatter plots.\ntype(df['horsepower'][0])\n\n\n\n\nstr\n\n\n\n# 'horsepower' is a string because one of its values is '?'\ndf['horsepower'].unique()\n\n\n\n\narray(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)\n\n\n\n# Replace '?' value by zero\ndf['horsepower'] = df['horsepower'].replace(to_replace = '?', value = '0')\ndf['horsepower'].unique()  # Check if it's ok\n\n\n\n\narray(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '0', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)\n\n\n\n# Convert 'horsepower' to int\ndf['horsepower'] = df['horsepower'].astype(int)\ntype(df['horsepower'][0])  # Check if it's ok\n\n\n\n\nnumpy.int32\n\n\n\nRelationships analysis\n\n\n# Scatterplot matrix using Seaborn\nsns.pairplot(df);\n\n\n\n\n\n\nmpg\n seems to have a non-linear relationship with: \ndisplacement\n, \nhorsepower\n and \nweight\n. Since in the book they compare \nmpg\n to \nhorsepower\n, we will only analyse that relationship in this exercise.\n\n\nNon-linear models\n\n\nNon-linear models investigated in Chapter 7:\n\n Polynomial regression\n\n Step function\n\n Regression splines\n\n Smoothing splines\n\n Local regression\n\n Generalized additive models (GAM)\n\n\nWe will develop models for the first three because we didn't find functions for smoothing splines, local regression and GAM.\n\n\n# Dataset\nX = df['horsepower'][:,np.newaxis]\ny = df['mpg']\n\n\n\n\nPolynomial regression\n\n\nfor i in range (1,11):\n    model = Pipeline([('poly', PolynomialFeatures(degree=i)), \n                      ('linear', LinearRegression())])\n    model.fit(X,y)\n\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n\n    print(\"Degree: %i  CV mean squared error: %.3f\" % (i, np.mean(np.abs(score))))\n\n\n\n\nDegree: 1  CV mean squared error: 32.573\nDegree: 2  CV mean squared error: 30.515\nDegree: 3  CV mean squared error: 27.440\nDegree: 4  CV mean squared error: 25.396\nDegree: 5  CV mean squared error: 24.986\nDegree: 6  CV mean squared error: 24.937\nDegree: 7  CV mean squared error: 24.831\nDegree: 8  CV mean squared error: 24.907\nDegree: 9  CV mean squared error: 25.358\nDegree: 10  CV mean squared error: 25.270\n\n\n\nThe polynomial degree that best fits the relationship between \nmpg\n and \nhorsepower\n is \n7\n.\n\n\nStep function\n\n\nfor i in range(1,11):\n    groups = pd.cut(df['horsepower'], i)\n    df_dummies = pd.get_dummies(groups)\n\n    X_step = df_dummies\n    y_step = df['mpg']\n\n    model.fit(X_step, y_step)\n    score = cross_val_score(model, X_step, y_step, cv=5, scoring='neg_mean_squared_error')\n\n    print('Number of cuts: %i  CV mean squared error: %.3f' %(i, np.mean(np.abs(score))))\n\n\n\n\nNumber of cuts: 1  CV mean squared error: 74.402\nNumber of cuts: 2  CV mean squared error: 43.120\nNumber of cuts: 3  CV mean squared error: 36.934\nNumber of cuts: 4  CV mean squared error: 41.236\nNumber of cuts: 5  CV mean squared error: 30.880\nNumber of cuts: 6  CV mean squared error: 29.083\nNumber of cuts: 7  CV mean squared error: 29.269\nNumber of cuts: 8  CV mean squared error: 28.562\nNumber of cuts: 9  CV mean squared error: 27.776\nNumber of cuts: 10  CV mean squared error: 24.407\n\n\n\nThe step function that best fits the relationship between \nmpg\n and \nhorsepower\n is the one with \n10\n steps.\n\n\nRegression splines\n\n\nThere are two types of regression splines: splines and natural splines. The difference between these two groups is that a natural spline is a regression spline with additional boundary constraints: the natural function is required to be linear at the boundary. This additional constraint means that natural splines generally produce more stable estimates at the boundaries.\n\n\nUsually, cubic splines are used. These splines are popular because most human eyes cannot detect the discontinuity at the knots. We use both cubic and natural cubic splines in this exercise.\n\n\nTo generate split basis representation, the package \npatsy\n is used. \npatsy\n is a Python package for describing statistical models (especially linear models, or models that have a linear component). Check the References for more details.\n\n\n Cubic splines\n\n\nfor i in range(3,11):  # The degrees of freedom can't be less than 3 in a cubic spline\n    transformed = dmatrix(\"bs(df.horsepower, df=%i, degree=3)\" % i,\n                         {\"df.horsepower\":df.horsepower},\n                          return_type='dataframe')  # Cubic spline basis representation\n    lin = LinearRegression()\n    lin.fit(transformed, y)\n\n    score = cross_val_score(lin, transformed, y, cv=10, scoring='neg_mean_squared_error')\n\n    print('Number of degrees of freedom: %i  CV mean squared error: %.3f' %(i, np.mean(np.abs(score))))\n\n\n\n\nNumber of degrees of freedom: 3  CV mean squared error: 24.268\nNumber of degrees of freedom: 4  CV mean squared error: 22.195\nNumber of degrees of freedom: 5  CV mean squared error: 22.319\nNumber of degrees of freedom: 6  CV mean squared error: 22.028\nNumber of degrees of freedom: 7  CV mean squared error: 22.099\nNumber of degrees of freedom: 8  CV mean squared error: 22.145\nNumber of degrees of freedom: 9  CV mean squared error: 21.783\nNumber of degrees of freedom: 10  CV mean squared error: 21.965\n\n\n\n Natural cubic splines\n\n\nfor i in range(3,11):  # The degrees of freedom can't be less than 3 in a cubic spline\n    transformed = dmatrix(\"cr(df.horsepower, df=%i)\" % i,\n                         {\"df.horsepower\":df.horsepower},\n                          return_type='dataframe')  # Cubic spline basis representation\n    lin = LinearRegression()\n    lin.fit(transformed, y)\n\n    score = cross_val_score(lin, transformed, y, cv=10, scoring='neg_mean_squared_error')\n\n    print('Number of degrees of freedom: %i  CV mean squared error: %.3f' %(i, np.mean(np.abs(score))))\n\n\n\n\nNumber of degrees of freedom: 3  CV mean squared error: 27.132\nNumber of degrees of freedom: 4  CV mean squared error: 24.297\nNumber of degrees of freedom: 5  CV mean squared error: 22.317\nNumber of degrees of freedom: 6  CV mean squared error: 21.721\nNumber of degrees of freedom: 7  CV mean squared error: 21.904\nNumber of degrees of freedom: 8  CV mean squared error: 22.019\nNumber of degrees of freedom: 9  CV mean squared error: 21.926\nNumber of degrees of freedom: 10  CV mean squared error: 22.201\n\n\n\nThe regression spline that best fits the relationship between \nmpg\n and \nhorsepower\n is a \nnatural cubic spline\n with \n6\n degrees of freedom. This is also the model that gives the best results of the three considered in this exercise.\n\n\nNotes regarding regression splines:\n\n\n In practice it is common to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data. This way, knots are placed in a uniform fashion.\n\n Knowing the degrees of freedom (df), we can know the number of knots (K). If it is a cubic spline, we have \ndf = 4 + K\n. If it is a natural cubic spline, we have \ndf = K\n because there are two additional natural constraints at each boundary to enforce linearity. However, regarding natural cubic splines, different interpretations can be made. For example, it can be said that \ndf = K - 1\n because it includes a constant that is absorbed in the intercept, so we can count only \nK-1\n degrees of freedom.\n* Cross-validation is an objective approach to define how many degrees of freedom our spline should contain.  \n\n\nNotes\n\n\n\n\nSolved exercises use \ndisplacement\n instead of \nhorsepower\n, so I couldn't verify my solutions. I only found one set of solved exercises for this exercise.\n\n\nAccording to https://github.com/JWarmenhoven/ISLR-python/blob/master/Notebooks/Chapter%207.ipynb, Python does not have functions to create smoothing splines, local regression or GAMs.\n\n\n\n\nReferences\n\n\n\n\nIntroduction to Statistical Learning with R, Chapter 3.3\n\n\nhttp://www.science.smith.edu/~jcrouser/SDS293/labs/lab13/Lab%2013%20-%20Splines%20in%20Python.pdf\n\n\nhttps://github.com/JWarmenhoven/ISLR-python/blob/master/Notebooks/Chapter%207.ipynb\n\n\nhttp://patsy.readthedocs.io/en/latest/API-reference.html (search for: 'patsy.cr')",
            "title": "7.8"
        },
        {
            "location": "/sols/chapter7/exercise8/#exercise-78",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom patsy import dmatrix\n\n%matplotlib inline  df = pd.read_csv('../data/Auto.csv')  # Visualize dataset\ndf.head()   \n   \n     \n       \n       mpg \n       cylinders \n       displacement \n       horsepower \n       weight \n       acceleration \n       year \n       origin \n       name \n     \n   \n   \n     \n       0 \n       18.0 \n       8 \n       307.0 \n       130 \n       3504 \n       12.0 \n       70 \n       1 \n       chevrolet chevelle malibu \n     \n     \n       1 \n       15.0 \n       8 \n       350.0 \n       165 \n       3693 \n       11.5 \n       70 \n       1 \n       buick skylark 320 \n     \n     \n       2 \n       18.0 \n       8 \n       318.0 \n       150 \n       3436 \n       11.0 \n       70 \n       1 \n       plymouth satellite \n     \n     \n       3 \n       16.0 \n       8 \n       304.0 \n       150 \n       3433 \n       12.0 \n       70 \n       1 \n       amc rebel sst \n     \n     \n       4 \n       17.0 \n       8 \n       302.0 \n       140 \n       3449 \n       10.5 \n       70 \n       1 \n       ford torino",
            "title": "Exercise 7.8"
        },
        {
            "location": "/sols/chapter7/exercise8/#pre-processing-data",
            "text": "# 'horsepower' is a string\n# We would like it to be a integer, in order to appear in scatter plots.\ntype(df['horsepower'][0])  str  # 'horsepower' is a string because one of its values is '?'\ndf['horsepower'].unique()  array(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)  # Replace '?' value by zero\ndf['horsepower'] = df['horsepower'].replace(to_replace = '?', value = '0')\ndf['horsepower'].unique()  # Check if it's ok  array(['130', '165', '150', '140', '198', '220', '215', '225', '190',\n       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',\n       '200', '210', '193', '0', '100', '105', '175', '153', '180', '110',\n       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',\n       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',\n       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148', '129',\n       '96', '71', '98', '115', '53', '81', '79', '120', '152', '102',\n       '108', '68', '58', '149', '89', '63', '48', '66', '139', '103',\n       '125', '133', '138', '135', '142', '77', '62', '132', '84', '64',\n       '74', '116', '82'], dtype=object)  # Convert 'horsepower' to int\ndf['horsepower'] = df['horsepower'].astype(int)\ntype(df['horsepower'][0])  # Check if it's ok  numpy.int32",
            "title": "Pre-processing data"
        },
        {
            "location": "/sols/chapter7/exercise8/#relationships-analysis",
            "text": "# Scatterplot matrix using Seaborn\nsns.pairplot(df);   mpg  seems to have a non-linear relationship with:  displacement ,  horsepower  and  weight . Since in the book they compare  mpg  to  horsepower , we will only analyse that relationship in this exercise.",
            "title": "Relationships analysis"
        },
        {
            "location": "/sols/chapter7/exercise8/#non-linear-models",
            "text": "Non-linear models investigated in Chapter 7:  Polynomial regression  Step function  Regression splines  Smoothing splines  Local regression  Generalized additive models (GAM)  We will develop models for the first three because we didn't find functions for smoothing splines, local regression and GAM.  # Dataset\nX = df['horsepower'][:,np.newaxis]\ny = df['mpg']",
            "title": "Non-linear models"
        },
        {
            "location": "/sols/chapter7/exercise8/#polynomial-regression",
            "text": "for i in range (1,11):\n    model = Pipeline([('poly', PolynomialFeatures(degree=i)), \n                      ('linear', LinearRegression())])\n    model.fit(X,y)\n\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n\n    print(\"Degree: %i  CV mean squared error: %.3f\" % (i, np.mean(np.abs(score))))  Degree: 1  CV mean squared error: 32.573\nDegree: 2  CV mean squared error: 30.515\nDegree: 3  CV mean squared error: 27.440\nDegree: 4  CV mean squared error: 25.396\nDegree: 5  CV mean squared error: 24.986\nDegree: 6  CV mean squared error: 24.937\nDegree: 7  CV mean squared error: 24.831\nDegree: 8  CV mean squared error: 24.907\nDegree: 9  CV mean squared error: 25.358\nDegree: 10  CV mean squared error: 25.270  The polynomial degree that best fits the relationship between  mpg  and  horsepower  is  7 .",
            "title": "Polynomial regression"
        },
        {
            "location": "/sols/chapter7/exercise8/#step-function",
            "text": "for i in range(1,11):\n    groups = pd.cut(df['horsepower'], i)\n    df_dummies = pd.get_dummies(groups)\n\n    X_step = df_dummies\n    y_step = df['mpg']\n\n    model.fit(X_step, y_step)\n    score = cross_val_score(model, X_step, y_step, cv=5, scoring='neg_mean_squared_error')\n\n    print('Number of cuts: %i  CV mean squared error: %.3f' %(i, np.mean(np.abs(score))))  Number of cuts: 1  CV mean squared error: 74.402\nNumber of cuts: 2  CV mean squared error: 43.120\nNumber of cuts: 3  CV mean squared error: 36.934\nNumber of cuts: 4  CV mean squared error: 41.236\nNumber of cuts: 5  CV mean squared error: 30.880\nNumber of cuts: 6  CV mean squared error: 29.083\nNumber of cuts: 7  CV mean squared error: 29.269\nNumber of cuts: 8  CV mean squared error: 28.562\nNumber of cuts: 9  CV mean squared error: 27.776\nNumber of cuts: 10  CV mean squared error: 24.407  The step function that best fits the relationship between  mpg  and  horsepower  is the one with  10  steps.",
            "title": "Step function"
        },
        {
            "location": "/sols/chapter7/exercise8/#regression-splines",
            "text": "There are two types of regression splines: splines and natural splines. The difference between these two groups is that a natural spline is a regression spline with additional boundary constraints: the natural function is required to be linear at the boundary. This additional constraint means that natural splines generally produce more stable estimates at the boundaries.  Usually, cubic splines are used. These splines are popular because most human eyes cannot detect the discontinuity at the knots. We use both cubic and natural cubic splines in this exercise.  To generate split basis representation, the package  patsy  is used.  patsy  is a Python package for describing statistical models (especially linear models, or models that have a linear component). Check the References for more details.   Cubic splines  for i in range(3,11):  # The degrees of freedom can't be less than 3 in a cubic spline\n    transformed = dmatrix(\"bs(df.horsepower, df=%i, degree=3)\" % i,\n                         {\"df.horsepower\":df.horsepower},\n                          return_type='dataframe')  # Cubic spline basis representation\n    lin = LinearRegression()\n    lin.fit(transformed, y)\n\n    score = cross_val_score(lin, transformed, y, cv=10, scoring='neg_mean_squared_error')\n\n    print('Number of degrees of freedom: %i  CV mean squared error: %.3f' %(i, np.mean(np.abs(score))))  Number of degrees of freedom: 3  CV mean squared error: 24.268\nNumber of degrees of freedom: 4  CV mean squared error: 22.195\nNumber of degrees of freedom: 5  CV mean squared error: 22.319\nNumber of degrees of freedom: 6  CV mean squared error: 22.028\nNumber of degrees of freedom: 7  CV mean squared error: 22.099\nNumber of degrees of freedom: 8  CV mean squared error: 22.145\nNumber of degrees of freedom: 9  CV mean squared error: 21.783\nNumber of degrees of freedom: 10  CV mean squared error: 21.965   Natural cubic splines  for i in range(3,11):  # The degrees of freedom can't be less than 3 in a cubic spline\n    transformed = dmatrix(\"cr(df.horsepower, df=%i)\" % i,\n                         {\"df.horsepower\":df.horsepower},\n                          return_type='dataframe')  # Cubic spline basis representation\n    lin = LinearRegression()\n    lin.fit(transformed, y)\n\n    score = cross_val_score(lin, transformed, y, cv=10, scoring='neg_mean_squared_error')\n\n    print('Number of degrees of freedom: %i  CV mean squared error: %.3f' %(i, np.mean(np.abs(score))))  Number of degrees of freedom: 3  CV mean squared error: 27.132\nNumber of degrees of freedom: 4  CV mean squared error: 24.297\nNumber of degrees of freedom: 5  CV mean squared error: 22.317\nNumber of degrees of freedom: 6  CV mean squared error: 21.721\nNumber of degrees of freedom: 7  CV mean squared error: 21.904\nNumber of degrees of freedom: 8  CV mean squared error: 22.019\nNumber of degrees of freedom: 9  CV mean squared error: 21.926\nNumber of degrees of freedom: 10  CV mean squared error: 22.201  The regression spline that best fits the relationship between  mpg  and  horsepower  is a  natural cubic spline  with  6  degrees of freedom. This is also the model that gives the best results of the three considered in this exercise.  Notes regarding regression splines:   In practice it is common to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data. This way, knots are placed in a uniform fashion.  Knowing the degrees of freedom (df), we can know the number of knots (K). If it is a cubic spline, we have  df = 4 + K . If it is a natural cubic spline, we have  df = K  because there are two additional natural constraints at each boundary to enforce linearity. However, regarding natural cubic splines, different interpretations can be made. For example, it can be said that  df = K - 1  because it includes a constant that is absorbed in the intercept, so we can count only  K-1  degrees of freedom.\n* Cross-validation is an objective approach to define how many degrees of freedom our spline should contain.",
            "title": "Regression splines"
        },
        {
            "location": "/sols/chapter7/exercise8/#notes",
            "text": "Solved exercises use  displacement  instead of  horsepower , so I couldn't verify my solutions. I only found one set of solved exercises for this exercise.  According to https://github.com/JWarmenhoven/ISLR-python/blob/master/Notebooks/Chapter%207.ipynb, Python does not have functions to create smoothing splines, local regression or GAMs.",
            "title": "Notes"
        },
        {
            "location": "/sols/chapter7/exercise8/#references",
            "text": "Introduction to Statistical Learning with R, Chapter 3.3  http://www.science.smith.edu/~jcrouser/SDS293/labs/lab13/Lab%2013%20-%20Splines%20in%20Python.pdf  https://github.com/JWarmenhoven/ISLR-python/blob/master/Notebooks/Chapter%207.ipynb  http://patsy.readthedocs.io/en/latest/API-reference.html (search for: 'patsy.cr')",
            "title": "References"
        },
        {
            "location": "/sols/chapter7/exercise9/",
            "text": "Exercise 7.9",
            "title": "7.9"
        },
        {
            "location": "/sols/chapter7/exercise9/#exercise-79",
            "text": "",
            "title": "Exercise 7.9"
        },
        {
            "location": "/sols/chapter7/exercise10/",
            "text": "Exercise 7.10\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/College.csv', index_col=0)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPrivate\n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n    \n\n  \n\n  \n\n    \n\n      \nAbilene Christian University\n\n      \nYes\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.1\n\n      \n12\n\n      \n7041\n\n      \n60\n\n    \n\n    \n\n      \nAdelphi University\n\n      \nYes\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.2\n\n      \n16\n\n      \n10527\n\n      \n56\n\n    \n\n    \n\n      \nAdrian College\n\n      \nYes\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.9\n\n      \n30\n\n      \n8735\n\n      \n54\n\n    \n\n    \n\n      \nAgnes Scott College\n\n      \nYes\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.7\n\n      \n37\n\n      \n19016\n\n      \n59\n\n    \n\n    \n\n      \nAlaska Pacific University\n\n      \nYes\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.9\n\n      \n2\n\n      \n10922\n\n      \n15\n\n    \n\n  \n\n\n\n\n\n\n\n# Dummy variables\n# The feature 'Private' is categorical. In order to use it in our models, we need to use dummy variables.\ndf = pd.get_dummies(df)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nApps\n\n      \nAccept\n\n      \nEnroll\n\n      \nTop10perc\n\n      \nTop25perc\n\n      \nF.Undergrad\n\n      \nP.Undergrad\n\n      \nOutstate\n\n      \nRoom.Board\n\n      \nBooks\n\n      \nPersonal\n\n      \nPhD\n\n      \nTerminal\n\n      \nS.F.Ratio\n\n      \nperc.alumni\n\n      \nExpend\n\n      \nGrad.Rate\n\n      \nPrivate_No\n\n      \nPrivate_Yes\n\n    \n\n  \n\n  \n\n    \n\n      \nAbilene Christian University\n\n      \n1660\n\n      \n1232\n\n      \n721\n\n      \n23\n\n      \n52\n\n      \n2885\n\n      \n537\n\n      \n7440\n\n      \n3300\n\n      \n450\n\n      \n2200\n\n      \n70\n\n      \n78\n\n      \n18.1\n\n      \n12\n\n      \n7041\n\n      \n60\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \nAdelphi University\n\n      \n2186\n\n      \n1924\n\n      \n512\n\n      \n16\n\n      \n29\n\n      \n2683\n\n      \n1227\n\n      \n12280\n\n      \n6450\n\n      \n750\n\n      \n1500\n\n      \n29\n\n      \n30\n\n      \n12.2\n\n      \n16\n\n      \n10527\n\n      \n56\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \nAdrian College\n\n      \n1428\n\n      \n1097\n\n      \n336\n\n      \n22\n\n      \n50\n\n      \n1036\n\n      \n99\n\n      \n11250\n\n      \n3750\n\n      \n400\n\n      \n1165\n\n      \n53\n\n      \n66\n\n      \n12.9\n\n      \n30\n\n      \n8735\n\n      \n54\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \nAgnes Scott College\n\n      \n417\n\n      \n349\n\n      \n137\n\n      \n60\n\n      \n89\n\n      \n510\n\n      \n63\n\n      \n12960\n\n      \n5450\n\n      \n450\n\n      \n875\n\n      \n92\n\n      \n97\n\n      \n7.7\n\n      \n37\n\n      \n19016\n\n      \n59\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \nAlaska Pacific University\n\n      \n193\n\n      \n146\n\n      \n55\n\n      \n16\n\n      \n44\n\n      \n249\n\n      \n869\n\n      \n7560\n\n      \n4120\n\n      \n800\n\n      \n1500\n\n      \n76\n\n      \n72\n\n      \n11.9\n\n      \n2\n\n      \n10922\n\n      \n15\n\n      \n0.0\n\n      \n1.0\n\n    \n\n  \n\n\n\n\n\n\n\n(a)\n\n\n# Dataset\nX = df.ix[:, df.columns != 'Outstate']\ny = df['Outstate']\n\n\n\n\n# Split into train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)\n\n\n\n\n# Forward stepwise selection\nlr = LinearRegression()\n\nsfs = SFS(lr,\n          k_features = 18,  # We have 18 features\n          forward = True,\n          floating = False,\n          scoring = 'r2',\n          cv = 0)\n\nsfs = sfs.fit(X_train.as_matrix(), y_train)  # as_matrix() to be readable by sfs\n\nfig = plot_sfs(sfs.get_metric_dict())\n\n#plt.title('Sequential forward selection (w. StdDev)')\nplt.title('Sequential forward selection')\nplt.grid()\nplt.show()\n\n\n\n\nc:\\program files\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:82: RuntimeWarning: Degrees of freedom <= 0 for slice\n  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n\n\n\n\n\nWe will choose \n6 features\n. The figure shows that a larger number of features will not increase the performance significantly.\n\n\n# Visualizing the results in dataframes\npd.DataFrame.from_dict(sfs.get_metric_dict()).T\n\n\n\n\nc:\\program files\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:82: RuntimeWarning: Degrees of freedom <= 0 for slice\n  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \navg_score\n\n      \nci_bound\n\n      \ncv_scores\n\n      \nfeature_idx\n\n      \nstd_dev\n\n      \nstd_err\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n0.43622\n\n      \nNaN\n\n      \n[0.436219807936]\n\n      \n(7,)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n2\n\n      \n0.589207\n\n      \nNaN\n\n      \n[0.589206513899]\n\n      \n(13, 7)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n3\n\n      \n0.661242\n\n      \nNaN\n\n      \n[0.661241618445]\n\n      \n(13, 14, 7)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n4\n\n      \n0.718047\n\n      \nNaN\n\n      \n[0.71804725948]\n\n      \n(16, 13, 14, 7)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n5\n\n      \n0.744539\n\n      \nNaN\n\n      \n[0.744538917009]\n\n      \n(16, 10, 13, 14, 7)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n6\n\n      \n0.75298\n\n      \nNaN\n\n      \n[0.752979718057]\n\n      \n(16, 7, 10, 13, 14, 15)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n7\n\n      \n0.755819\n\n      \nNaN\n\n      \n[0.755818691645]\n\n      \n(16, 7, 9, 10, 13, 14, 15)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n8\n\n      \n0.757751\n\n      \nNaN\n\n      \n[0.757750769449]\n\n      \n(16, 4, 7, 9, 10, 13, 14, 15)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n9\n\n      \n0.759088\n\n      \nNaN\n\n      \n[0.759088496118]\n\n      \n(1, 4, 7, 9, 10, 13, 14, 15, 16)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n10\n\n      \n0.763593\n\n      \nNaN\n\n      \n[0.763592832578]\n\n      \n(0, 1, 4, 7, 9, 10, 13, 14, 15, 16)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n11\n\n      \n0.766277\n\n      \nNaN\n\n      \n[0.766276542748]\n\n      \n(0, 1, 2, 4, 7, 9, 10, 13, 14, 15, 16)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n12\n\n      \n0.767782\n\n      \nNaN\n\n      \n[0.767781703908]\n\n      \n(0, 1, 2, 3, 4, 7, 9, 10, 13, 14, 15, 16)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n13\n\n      \n0.768725\n\n      \nNaN\n\n      \n[0.768724891242]\n\n      \n(0, 1, 2, 3, 4, 7, 9, 10, 12, 13, 14, 15, 16)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n14\n\n      \n0.769542\n\n      \nNaN\n\n      \n[0.769541795295]\n\n      \n(0, 1, 2, 3, 4, 7, 9, 10, 11, 12, 13, 14, 15, 16)\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n15\n\n      \n0.77035\n\n      \nNaN\n\n      \n[0.770349525842]\n\n      \n(0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 1...\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n16\n\n      \n0.770451\n\n      \nNaN\n\n      \n[0.770451018385]\n\n      \n(0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n17\n\n      \n0.770452\n\n      \nNaN\n\n      \n[0.770451587]\n\n      \n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n\n      \n0\n\n      \nNaN\n\n    \n\n    \n\n      \n18\n\n      \n0.770452\n\n      \nNaN\n\n      \n[0.770451587]\n\n      \n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n\n      \n0\n\n      \nNaN\n\n    \n\n  \n\n\n\n\n\n\n\n# Variables that we will choose\nprint('Variables: %s, %s, %s, %s, %s, %s' % (X.columns[16], X.columns[7], X.columns[10], X.columns[13], X.columns[14], X.columns[15]))\n\n\n\n\nVariables: Private_No, Room.Board, PhD, perc.alumni, Expend, Grad.Rate\n\n\n\nReferences\n\n\n\n\nhttps://github.com/dswah/pyGAM (GAM in Python)\n\n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html (scikit train and test splits)\n\n\nhttp://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ (mlxtend forward stepwise)\n\n\nhttps://github.com/rasbt/mlxtend/blob/master/mlxtend/plotting/plot_sequential_feature_selection.py (plot_sequential_feature_selection source)\n\n\nhttps://github.com/bsilverthorn/gampy/blob/master/src/python/gampy/backfit.py (GAM package???)",
            "title": "7.10"
        },
        {
            "location": "/sols/chapter7/exercise10/#exercise-710",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\n%matplotlib inline  df = pd.read_csv('../data/College.csv', index_col=0)  df.head()   \n   \n     \n       \n       Private \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n     \n   \n   \n     \n       Abilene Christian University \n       Yes \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.1 \n       12 \n       7041 \n       60 \n     \n     \n       Adelphi University \n       Yes \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.2 \n       16 \n       10527 \n       56 \n     \n     \n       Adrian College \n       Yes \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.9 \n       30 \n       8735 \n       54 \n     \n     \n       Agnes Scott College \n       Yes \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.7 \n       37 \n       19016 \n       59 \n     \n     \n       Alaska Pacific University \n       Yes \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.9 \n       2 \n       10922 \n       15 \n     \n      # Dummy variables\n# The feature 'Private' is categorical. In order to use it in our models, we need to use dummy variables.\ndf = pd.get_dummies(df)  df.head()   \n   \n     \n       \n       Apps \n       Accept \n       Enroll \n       Top10perc \n       Top25perc \n       F.Undergrad \n       P.Undergrad \n       Outstate \n       Room.Board \n       Books \n       Personal \n       PhD \n       Terminal \n       S.F.Ratio \n       perc.alumni \n       Expend \n       Grad.Rate \n       Private_No \n       Private_Yes \n     \n   \n   \n     \n       Abilene Christian University \n       1660 \n       1232 \n       721 \n       23 \n       52 \n       2885 \n       537 \n       7440 \n       3300 \n       450 \n       2200 \n       70 \n       78 \n       18.1 \n       12 \n       7041 \n       60 \n       0.0 \n       1.0 \n     \n     \n       Adelphi University \n       2186 \n       1924 \n       512 \n       16 \n       29 \n       2683 \n       1227 \n       12280 \n       6450 \n       750 \n       1500 \n       29 \n       30 \n       12.2 \n       16 \n       10527 \n       56 \n       0.0 \n       1.0 \n     \n     \n       Adrian College \n       1428 \n       1097 \n       336 \n       22 \n       50 \n       1036 \n       99 \n       11250 \n       3750 \n       400 \n       1165 \n       53 \n       66 \n       12.9 \n       30 \n       8735 \n       54 \n       0.0 \n       1.0 \n     \n     \n       Agnes Scott College \n       417 \n       349 \n       137 \n       60 \n       89 \n       510 \n       63 \n       12960 \n       5450 \n       450 \n       875 \n       92 \n       97 \n       7.7 \n       37 \n       19016 \n       59 \n       0.0 \n       1.0 \n     \n     \n       Alaska Pacific University \n       193 \n       146 \n       55 \n       16 \n       44 \n       249 \n       869 \n       7560 \n       4120 \n       800 \n       1500 \n       76 \n       72 \n       11.9 \n       2 \n       10922 \n       15 \n       0.0 \n       1.0",
            "title": "Exercise 7.10"
        },
        {
            "location": "/sols/chapter7/exercise10/#a",
            "text": "# Dataset\nX = df.ix[:, df.columns != 'Outstate']\ny = df['Outstate']  # Split into train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)  # Forward stepwise selection\nlr = LinearRegression()\n\nsfs = SFS(lr,\n          k_features = 18,  # We have 18 features\n          forward = True,\n          floating = False,\n          scoring = 'r2',\n          cv = 0)\n\nsfs = sfs.fit(X_train.as_matrix(), y_train)  # as_matrix() to be readable by sfs\n\nfig = plot_sfs(sfs.get_metric_dict())\n\n#plt.title('Sequential forward selection (w. StdDev)')\nplt.title('Sequential forward selection')\nplt.grid()\nplt.show()  c:\\program files\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:82: RuntimeWarning: Degrees of freedom <= 0 for slice\n  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)   We will choose  6 features . The figure shows that a larger number of features will not increase the performance significantly.  # Visualizing the results in dataframes\npd.DataFrame.from_dict(sfs.get_metric_dict()).T  c:\\program files\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:82: RuntimeWarning: Degrees of freedom <= 0 for slice\n  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)   \n   \n     \n       \n       avg_score \n       ci_bound \n       cv_scores \n       feature_idx \n       std_dev \n       std_err \n     \n   \n   \n     \n       1 \n       0.43622 \n       NaN \n       [0.436219807936] \n       (7,) \n       0 \n       NaN \n     \n     \n       2 \n       0.589207 \n       NaN \n       [0.589206513899] \n       (13, 7) \n       0 \n       NaN \n     \n     \n       3 \n       0.661242 \n       NaN \n       [0.661241618445] \n       (13, 14, 7) \n       0 \n       NaN \n     \n     \n       4 \n       0.718047 \n       NaN \n       [0.71804725948] \n       (16, 13, 14, 7) \n       0 \n       NaN \n     \n     \n       5 \n       0.744539 \n       NaN \n       [0.744538917009] \n       (16, 10, 13, 14, 7) \n       0 \n       NaN \n     \n     \n       6 \n       0.75298 \n       NaN \n       [0.752979718057] \n       (16, 7, 10, 13, 14, 15) \n       0 \n       NaN \n     \n     \n       7 \n       0.755819 \n       NaN \n       [0.755818691645] \n       (16, 7, 9, 10, 13, 14, 15) \n       0 \n       NaN \n     \n     \n       8 \n       0.757751 \n       NaN \n       [0.757750769449] \n       (16, 4, 7, 9, 10, 13, 14, 15) \n       0 \n       NaN \n     \n     \n       9 \n       0.759088 \n       NaN \n       [0.759088496118] \n       (1, 4, 7, 9, 10, 13, 14, 15, 16) \n       0 \n       NaN \n     \n     \n       10 \n       0.763593 \n       NaN \n       [0.763592832578] \n       (0, 1, 4, 7, 9, 10, 13, 14, 15, 16) \n       0 \n       NaN \n     \n     \n       11 \n       0.766277 \n       NaN \n       [0.766276542748] \n       (0, 1, 2, 4, 7, 9, 10, 13, 14, 15, 16) \n       0 \n       NaN \n     \n     \n       12 \n       0.767782 \n       NaN \n       [0.767781703908] \n       (0, 1, 2, 3, 4, 7, 9, 10, 13, 14, 15, 16) \n       0 \n       NaN \n     \n     \n       13 \n       0.768725 \n       NaN \n       [0.768724891242] \n       (0, 1, 2, 3, 4, 7, 9, 10, 12, 13, 14, 15, 16) \n       0 \n       NaN \n     \n     \n       14 \n       0.769542 \n       NaN \n       [0.769541795295] \n       (0, 1, 2, 3, 4, 7, 9, 10, 11, 12, 13, 14, 15, 16) \n       0 \n       NaN \n     \n     \n       15 \n       0.77035 \n       NaN \n       [0.770349525842] \n       (0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 1... \n       0 \n       NaN \n     \n     \n       16 \n       0.770451 \n       NaN \n       [0.770451018385] \n       (0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14... \n       0 \n       NaN \n     \n     \n       17 \n       0.770452 \n       NaN \n       [0.770451587] \n       (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... \n       0 \n       NaN \n     \n     \n       18 \n       0.770452 \n       NaN \n       [0.770451587] \n       (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... \n       0 \n       NaN \n     \n      # Variables that we will choose\nprint('Variables: %s, %s, %s, %s, %s, %s' % (X.columns[16], X.columns[7], X.columns[10], X.columns[13], X.columns[14], X.columns[15]))  Variables: Private_No, Room.Board, PhD, perc.alumni, Expend, Grad.Rate",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter7/exercise10/#references",
            "text": "https://github.com/dswah/pyGAM (GAM in Python)  http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html (scikit train and test splits)  http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ (mlxtend forward stepwise)  https://github.com/rasbt/mlxtend/blob/master/mlxtend/plotting/plot_sequential_feature_selection.py (plot_sequential_feature_selection source)  https://github.com/bsilverthorn/gampy/blob/master/src/python/gampy/backfit.py (GAM package???)",
            "title": "References"
        },
        {
            "location": "/sols/chapter7/exercise11/",
            "text": "Exercise 7.11",
            "title": "7.11"
        },
        {
            "location": "/sols/chapter7/exercise11/#exercise-711",
            "text": "",
            "title": "Exercise 7.11"
        },
        {
            "location": "/sols/chapter7/exercise12/",
            "text": "Exercise 7.12",
            "title": "7.12"
        },
        {
            "location": "/sols/chapter7/exercise12/#exercise-712",
            "text": "",
            "title": "Exercise 7.12"
        },
        {
            "location": "/sols/chapter8/exercise1/",
            "text": "Exercise 8.3",
            "title": "8.1"
        },
        {
            "location": "/sols/chapter8/exercise1/#exercise-83",
            "text": "",
            "title": "Exercise 8.3"
        },
        {
            "location": "/sols/chapter8/exercise2/",
            "text": "Exercise 8.2",
            "title": "8.2"
        },
        {
            "location": "/sols/chapter8/exercise2/#exercise-82",
            "text": "",
            "title": "Exercise 8.2"
        },
        {
            "location": "/sols/chapter8/exercise3/",
            "text": "Exercise 8.1",
            "title": "8.3"
        },
        {
            "location": "/sols/chapter8/exercise3/#exercise-81",
            "text": "",
            "title": "Exercise 8.1"
        },
        {
            "location": "/sols/chapter8/exercise4/",
            "text": "Exercise 8.4\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\n----------------X1<=1----------------\n\n\n----------------- | -----------------\n\n\n--------X2<=1---------------Y=5------\n\n\n--------- | -------------------------\n\n\n---X1<=0------Y=15-------------------\n\n\n--- | -------------------------------\n\n\nY=3---X2<=0--------------------------\n\n\n------- | ---------------------------\n\n\n----Y=10--Y=0------------------------\n\n\nWe didn't find the beautiful way of doing this, but we believe you'll find and share with us :)\n\n\n(b)\n\n\n# Plot according to the tree presented in the figure\nplt.plot()\nplt.xlim(-1,5)\nplt.ylim(-1,3)\nplt.xticks([1])\nplt.yticks([1,2])\nplt.vlines(1, ymin=-5, ymax=1)\nplt.vlines(0, ymin=1, ymax=2)\nplt.hlines(1, xmin=-5, xmax=5)\nplt.hlines(2, xmin=-5, xmax=5)\nplt.annotate('-1.80', xy=(-.5,0), fontsize=12)\nplt.annotate('-1.06', xy=(-1,1.5), fontsize=12)\nplt.annotate('0.63', xy=(2.5,0), fontsize=12)\nplt.annotate('0.21', xy=(2.5,1.5), fontsize=12)\nplt.annotate('2.49', xy=(1,2.5), fontsize=12)\n\n\n\n\n<matplotlib.text.Annotation at 0xc486e10>",
            "title": "8.4"
        },
        {
            "location": "/sols/chapter8/exercise4/#exercise-84",
            "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline",
            "title": "Exercise 8.4"
        },
        {
            "location": "/sols/chapter8/exercise4/#a",
            "text": "----------------X1<=1----------------  ----------------- | -----------------  --------X2<=1---------------Y=5------  --------- | -------------------------  ---X1<=0------Y=15-------------------  --- | -------------------------------  Y=3---X2<=0--------------------------  ------- | ---------------------------  ----Y=10--Y=0------------------------  We didn't find the beautiful way of doing this, but we believe you'll find and share with us :)",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter8/exercise4/#b",
            "text": "# Plot according to the tree presented in the figure\nplt.plot()\nplt.xlim(-1,5)\nplt.ylim(-1,3)\nplt.xticks([1])\nplt.yticks([1,2])\nplt.vlines(1, ymin=-5, ymax=1)\nplt.vlines(0, ymin=1, ymax=2)\nplt.hlines(1, xmin=-5, xmax=5)\nplt.hlines(2, xmin=-5, xmax=5)\nplt.annotate('-1.80', xy=(-.5,0), fontsize=12)\nplt.annotate('-1.06', xy=(-1,1.5), fontsize=12)\nplt.annotate('0.63', xy=(2.5,0), fontsize=12)\nplt.annotate('0.21', xy=(2.5,1.5), fontsize=12)\nplt.annotate('2.49', xy=(1,2.5), fontsize=12)  <matplotlib.text.Annotation at 0xc486e10>",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter8/exercise5/",
            "text": "Exercise 8.5",
            "title": "8.5"
        },
        {
            "location": "/sols/chapter8/exercise5/#exercise-85",
            "text": "",
            "title": "Exercise 8.5"
        },
        {
            "location": "/sols/chapter8/exercise6/",
            "text": "Exercise 8.6\n\n\nThe algorithm to fit a regression tree is as follows:\n\n\n\n\nUse the recursive binary splitting to obtain a large tree. Consider a minimum number of observations in the terminal nodes as a criterion to stop the recursive binary splitting.\n\n\nApply the cost complexity pruning to get the optimum subtree. The cost complexity pruning is defined as a function of \n\\alpha\n\\alpha\n, which works as a tuning parameter. \n\n\nThe tuning parameter \n\\alpha\n\\alpha\n is determined using the K-fold cross-validation. A training and a test set are then defined. In each fold, the training set is splitten applying the recursive binary splitting. To the resulting tree, the cost complexity pruning is applied. Then, for a different set of specific \n\\alpha\n\\alpha\ns, the mean squared prediction error on the test set is evaluated. The results for the different folds and for the different \n\\alpha\n\\alpha\ns are averaged, being \n\\alpha\n\\alpha\n the value that minimizes the average error.\n\n\nOnce we get the \n\\alpha\n\\alpha\n value, we replace it in Step 2 equation to get the optimum subtree.\n\n\n\n\nSome important definitions:\n\n\n\n\nSubtree\n. A tree that results from pruning a larger tree.\n\n\nRecursive binary splitting.\n Top-down greedy approach to divide the data into distinct and non-overlapping regions. It is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n\n\nCost complexity pruning.\n It is a way to get optimum subtrees. Mathematically, this is done adding a penalty term to the mean squared prediction error expression. This penalty term is governed by a tuning parameter, which controls the trade-off between the subtree\u2019s complexity and its fit to the training data.",
            "title": "8.6"
        },
        {
            "location": "/sols/chapter8/exercise6/#exercise-86",
            "text": "The algorithm to fit a regression tree is as follows:   Use the recursive binary splitting to obtain a large tree. Consider a minimum number of observations in the terminal nodes as a criterion to stop the recursive binary splitting.  Apply the cost complexity pruning to get the optimum subtree. The cost complexity pruning is defined as a function of  \\alpha \\alpha , which works as a tuning parameter.   The tuning parameter  \\alpha \\alpha  is determined using the K-fold cross-validation. A training and a test set are then defined. In each fold, the training set is splitten applying the recursive binary splitting. To the resulting tree, the cost complexity pruning is applied. Then, for a different set of specific  \\alpha \\alpha s, the mean squared prediction error on the test set is evaluated. The results for the different folds and for the different  \\alpha \\alpha s are averaged, being  \\alpha \\alpha  the value that minimizes the average error.  Once we get the  \\alpha \\alpha  value, we replace it in Step 2 equation to get the optimum subtree.   Some important definitions:   Subtree . A tree that results from pruning a larger tree.  Recursive binary splitting.  Top-down greedy approach to divide the data into distinct and non-overlapping regions. It is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.  Cost complexity pruning.  It is a way to get optimum subtrees. Mathematically, this is done adding a penalty term to the mean squared prediction error expression. This penalty term is governed by a tuning parameter, which controls the trade-off between the subtree\u2019s complexity and its fit to the training data.",
            "title": "Exercise 8.6"
        },
        {
            "location": "/sols/chapter8/exercise7/",
            "text": "Exercise 8.7",
            "title": "8.7"
        },
        {
            "location": "/sols/chapter8/exercise7/#exercise-87",
            "text": "",
            "title": "Exercise 8.7"
        },
        {
            "location": "/sols/chapter8/exercise8/",
            "text": "Exercise 8.8\n\n\nimport pandas as pd\nimport numpy as np\nimport pydotplus  # Check the references if you need help to install this module.\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz  # References: download link and instructions to install Graphviz.\nfrom IPython.display import Image  # To plot decision tree.\nfrom sklearn.externals.six import StringIO  # To plot decision tree.\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/Carseats.csv')\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nSales\n\n      \nCompPrice\n\n      \nIncome\n\n      \nAdvertising\n\n      \nPopulation\n\n      \nPrice\n\n      \nShelveLoc\n\n      \nAge\n\n      \nEducation\n\n      \nUrban\n\n      \nUS\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n9.50\n\n      \n138\n\n      \n73\n\n      \n11\n\n      \n276\n\n      \n120\n\n      \nBad\n\n      \n42\n\n      \n17\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n1\n\n      \n11.22\n\n      \n111\n\n      \n48\n\n      \n16\n\n      \n260\n\n      \n83\n\n      \nGood\n\n      \n65\n\n      \n10\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n2\n\n      \n10.06\n\n      \n113\n\n      \n35\n\n      \n10\n\n      \n269\n\n      \n80\n\n      \nMedium\n\n      \n59\n\n      \n12\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n3\n\n      \n7.40\n\n      \n117\n\n      \n100\n\n      \n4\n\n      \n466\n\n      \n97\n\n      \nMedium\n\n      \n55\n\n      \n14\n\n      \nYes\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \n4.15\n\n      \n141\n\n      \n64\n\n      \n3\n\n      \n340\n\n      \n128\n\n      \nBad\n\n      \n38\n\n      \n13\n\n      \nYes\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\n# Dummy variables\n# Transform qualitative variables into quantitative to enable the use of regressors.\ndf = pd.get_dummies(df)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nSales\n\n      \nCompPrice\n\n      \nIncome\n\n      \nAdvertising\n\n      \nPopulation\n\n      \nPrice\n\n      \nAge\n\n      \nEducation\n\n      \nShelveLoc_Bad\n\n      \nShelveLoc_Good\n\n      \nShelveLoc_Medium\n\n      \nUrban_No\n\n      \nUrban_Yes\n\n      \nUS_No\n\n      \nUS_Yes\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n9.50\n\n      \n138\n\n      \n73\n\n      \n11\n\n      \n276\n\n      \n120\n\n      \n42\n\n      \n17\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n1\n\n      \n11.22\n\n      \n111\n\n      \n48\n\n      \n16\n\n      \n260\n\n      \n83\n\n      \n65\n\n      \n10\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n2\n\n      \n10.06\n\n      \n113\n\n      \n35\n\n      \n10\n\n      \n269\n\n      \n80\n\n      \n59\n\n      \n12\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n3\n\n      \n7.40\n\n      \n117\n\n      \n100\n\n      \n4\n\n      \n466\n\n      \n97\n\n      \n55\n\n      \n14\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n4\n\n      \n4.15\n\n      \n141\n\n      \n64\n\n      \n3\n\n      \n340\n\n      \n128\n\n      \n38\n\n      \n13\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n1.0\n\n      \n0.0\n\n    \n\n  \n\n\n\n\n\n\n\n# This function creates images of tree models using pydot\n# Source: http://nbviewer.jupyter.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%208.ipynb\n# The original code used pydot instead of pydotplus. We didn't change anything else.\ndef print_tree(estimator, features, class_names=None, filled=True):\n    tree = estimator\n    names = features\n    color = filled\n    classn = class_names\n\n    dot_data = StringIO()\n    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    return(graph)\n\n\n\n\n(a)\n\n\n# Split data into training and test set\nX = df.ix[:,1:]\ny = df['Sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)\n\n\n\n\n(b)\n\n\n# Fit regression tree\nrgr = DecisionTreeRegressor(max_depth=3)  # We could have chosen another max_depth value. \nrgr.fit(X_train, y_train)\n\n\n\n\nDecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n           max_leaf_nodes=None, min_impurity_split=1e-07,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n           splitter='best')\n\n\n\n# Plot the tree\ngraph = print_tree(rgr, features=list(X_train.columns.values))\nImage(graph.create_png())\n\n\n\n\n\n\nInterpretation\n: According to the tree, ShelveLoc is the most important factor in determining Sales. Shelveloc is an indicator\nof the quality of the shelving location \u2014 that is, the space within a store in which the car seat is displayed \u2014 at each location. Cars with a good shelving location sell more (value = 10.1377) than cars in bad or medium shelving locations (value = 6.8035). Among each of these groups, Price is the second most important factor in determining Sales. For example, for cars with a good shelving location, when the Price is above 135, Sales decrease (6.8218 vs. 10.882). The same analysis logic applies for the remaining branches of the regression tree.\n\n\n# Test MSE\nprint('Test MSE: ', mean_squared_error(y_test, rgr.predict(X_test)))\n\n\n\n\nTest MSE:  4.72516600994\n\n\n\n(c)\n\n\nTo determine the optimal level of\ntree complexity using cross-validation we will use \nGridSearchCV\n.\n\n\n# Build a regressor\nrgr = DecisionTreeRegressor(random_state=1)\n\n\n\n\n# Grid of parameters to hypertune\nparam_grid = {'max_depth':[1,2,3,4,5,6,7,8,9,10]}\n\n\n\n\n# Run grid search\ngrid_search = GridSearchCV(rgr, \n                           param_grid=param_grid, \n                           cv=5)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=5, error_score='raise',\n       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n           max_leaf_nodes=None, min_impurity_split=1e-07,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n           splitter='best'),\n       fit_params={}, iid=True, n_jobs=1,\n       param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n       scoring=None, verbose=0)\n\n\n\n# Find the best estimator\ngrid_search.best_estimator_\n\n\n\n\nDecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n           max_leaf_nodes=None, min_impurity_split=1e-07,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n           splitter='best')\n\n\n\nThe best value for \nmax_depth\n using cross-validation is \n5\n.\n\n\nNote:\n \nPruning\n is currently not supported by scikit-learn, so we didn't solve that part of the exercise.\n\n\n(d)\n\n\n# Fit bagging regressor\nrgr = BaggingRegressor()\nrgr.fit(X_train, y_train)\n\n\n\n\nBaggingRegressor(base_estimator=None, bootstrap=True,\n         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n         verbose=0, warm_start=False)\n\n\n\n# Test MSE\nprint('Test MSE:', mean_squared_error(y_test, rgr.predict(X_test)))\n\n\n\n\nTest MSE: 3.33869740833\n\n\n\nNote:\n We didn't find a way to get variable's importance using scikit-learn, so we didn't solve that part of the exercise.\n\n\n(e)\n\n\n# Fit random forest regressor\nrgr = RandomForestRegressor()\nrgr.fit(X_train, y_train)\n\n\n\n\nRandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_split=1e-07, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n           verbose=0, warm_start=False)\n\n\n\n# Test MSE\nprint('Test MSE:', mean_squared_error(y_test, rgr.predict(X_test)))\n\n\n\n\nTest MSE: 3.23118576667\n\n\n\n# Variable importance\nimportance = pd.DataFrame({'Importance':rgr.feature_importances_*100}, index=X_train.columns)\nimportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r')\nplt.xlabel('Variable importance')\nplt.legend(loc='lower right')\n\n\n\n\n<matplotlib.legend.Legend at 0xec02978>\n\n\n\n\n\nThe test MSE decreased when compared with the previous answers. One possible explanation for this result is the effect of \nm\n. In random forests, the number of variables considered at each split changes. This means that, at each split, only a subset of predictors is taken into account. In some sense, this works as a process to decorrelate the decision trees. For example, in baggin, if there is a strong predictor in the data set, most of the bagged trees wil use this predictor in the top split. As a consequence, the predictions from the bagged trees will tend to be highly correlated. In random forests, this doesn't happen because the method forces each split to consider only a subset of the predictors. Hence, the method leads to models with reducted variance and increased reliability. That's a possible reason for the reduction verified in the test MSE value.\n\n\nReferences\n\n\n\n\nhttps://packaging.python.org/installing/ (help to install Python modules)\n\n\nhttp://www.graphviz.org/Download..php (download Graphviz)\n\n\nhttp://stackoverflow.com/questions/18438997/why-is-pydot-unable-to-find-graphvizs-executables-in-windows-8 (how to install Graphviz)\n\n\nhttp://scikit-learn.org/stable/modules/tree.html  (plotting decision trees with scikit)\n\n\n\n\n2DO\n\n\n\n\nPay attention during the review because the solutions that I've don't match with the existing solutions. I suppose that happens because of the randomness associated to the exercise, but I'm not sure.\n\n\nFind out how to get variable importance when using bagging.",
            "title": "8.8"
        },
        {
            "location": "/sols/chapter8/exercise8/#exercise-88",
            "text": "import pandas as pd\nimport numpy as np\nimport pydotplus  # Check the references if you need help to install this module.\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz  # References: download link and instructions to install Graphviz.\nfrom IPython.display import Image  # To plot decision tree.\nfrom sklearn.externals.six import StringIO  # To plot decision tree.\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n%matplotlib inline  df = pd.read_csv('../data/Carseats.csv')  df.head()   \n   \n     \n       \n       Sales \n       CompPrice \n       Income \n       Advertising \n       Population \n       Price \n       ShelveLoc \n       Age \n       Education \n       Urban \n       US \n     \n   \n   \n     \n       0 \n       9.50 \n       138 \n       73 \n       11 \n       276 \n       120 \n       Bad \n       42 \n       17 \n       Yes \n       Yes \n     \n     \n       1 \n       11.22 \n       111 \n       48 \n       16 \n       260 \n       83 \n       Good \n       65 \n       10 \n       Yes \n       Yes \n     \n     \n       2 \n       10.06 \n       113 \n       35 \n       10 \n       269 \n       80 \n       Medium \n       59 \n       12 \n       Yes \n       Yes \n     \n     \n       3 \n       7.40 \n       117 \n       100 \n       4 \n       466 \n       97 \n       Medium \n       55 \n       14 \n       Yes \n       Yes \n     \n     \n       4 \n       4.15 \n       141 \n       64 \n       3 \n       340 \n       128 \n       Bad \n       38 \n       13 \n       Yes \n       No \n     \n      # Dummy variables\n# Transform qualitative variables into quantitative to enable the use of regressors.\ndf = pd.get_dummies(df)  df.head()   \n   \n     \n       \n       Sales \n       CompPrice \n       Income \n       Advertising \n       Population \n       Price \n       Age \n       Education \n       ShelveLoc_Bad \n       ShelveLoc_Good \n       ShelveLoc_Medium \n       Urban_No \n       Urban_Yes \n       US_No \n       US_Yes \n     \n   \n   \n     \n       0 \n       9.50 \n       138 \n       73 \n       11 \n       276 \n       120 \n       42 \n       17 \n       1.0 \n       0.0 \n       0.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n     \n     \n       1 \n       11.22 \n       111 \n       48 \n       16 \n       260 \n       83 \n       65 \n       10 \n       0.0 \n       1.0 \n       0.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n     \n     \n       2 \n       10.06 \n       113 \n       35 \n       10 \n       269 \n       80 \n       59 \n       12 \n       0.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n     \n     \n       3 \n       7.40 \n       117 \n       100 \n       4 \n       466 \n       97 \n       55 \n       14 \n       0.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n     \n     \n       4 \n       4.15 \n       141 \n       64 \n       3 \n       340 \n       128 \n       38 \n       13 \n       1.0 \n       0.0 \n       0.0 \n       0.0 \n       1.0 \n       1.0 \n       0.0 \n     \n      # This function creates images of tree models using pydot\n# Source: http://nbviewer.jupyter.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%208.ipynb\n# The original code used pydot instead of pydotplus. We didn't change anything else.\ndef print_tree(estimator, features, class_names=None, filled=True):\n    tree = estimator\n    names = features\n    color = filled\n    classn = class_names\n\n    dot_data = StringIO()\n    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    return(graph)",
            "title": "Exercise 8.8"
        },
        {
            "location": "/sols/chapter8/exercise8/#a",
            "text": "# Split data into training and test set\nX = df.ix[:,1:]\ny = df['Sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter8/exercise8/#b",
            "text": "# Fit regression tree\nrgr = DecisionTreeRegressor(max_depth=3)  # We could have chosen another max_depth value. \nrgr.fit(X_train, y_train)  DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n           max_leaf_nodes=None, min_impurity_split=1e-07,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n           splitter='best')  # Plot the tree\ngraph = print_tree(rgr, features=list(X_train.columns.values))\nImage(graph.create_png())   Interpretation : According to the tree, ShelveLoc is the most important factor in determining Sales. Shelveloc is an indicator\nof the quality of the shelving location \u2014 that is, the space within a store in which the car seat is displayed \u2014 at each location. Cars with a good shelving location sell more (value = 10.1377) than cars in bad or medium shelving locations (value = 6.8035). Among each of these groups, Price is the second most important factor in determining Sales. For example, for cars with a good shelving location, when the Price is above 135, Sales decrease (6.8218 vs. 10.882). The same analysis logic applies for the remaining branches of the regression tree.  # Test MSE\nprint('Test MSE: ', mean_squared_error(y_test, rgr.predict(X_test)))  Test MSE:  4.72516600994",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter8/exercise8/#c",
            "text": "To determine the optimal level of\ntree complexity using cross-validation we will use  GridSearchCV .  # Build a regressor\nrgr = DecisionTreeRegressor(random_state=1)  # Grid of parameters to hypertune\nparam_grid = {'max_depth':[1,2,3,4,5,6,7,8,9,10]}  # Run grid search\ngrid_search = GridSearchCV(rgr, \n                           param_grid=param_grid, \n                           cv=5)\ngrid_search.fit(X_train, y_train)  GridSearchCV(cv=5, error_score='raise',\n       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n           max_leaf_nodes=None, min_impurity_split=1e-07,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n           splitter='best'),\n       fit_params={}, iid=True, n_jobs=1,\n       param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n       scoring=None, verbose=0)  # Find the best estimator\ngrid_search.best_estimator_  DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n           max_leaf_nodes=None, min_impurity_split=1e-07,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n           splitter='best')  The best value for  max_depth  using cross-validation is  5 .  Note:   Pruning  is currently not supported by scikit-learn, so we didn't solve that part of the exercise.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter8/exercise8/#d",
            "text": "# Fit bagging regressor\nrgr = BaggingRegressor()\nrgr.fit(X_train, y_train)  BaggingRegressor(base_estimator=None, bootstrap=True,\n         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n         verbose=0, warm_start=False)  # Test MSE\nprint('Test MSE:', mean_squared_error(y_test, rgr.predict(X_test)))  Test MSE: 3.33869740833  Note:  We didn't find a way to get variable's importance using scikit-learn, so we didn't solve that part of the exercise.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter8/exercise8/#e",
            "text": "# Fit random forest regressor\nrgr = RandomForestRegressor()\nrgr.fit(X_train, y_train)  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_split=1e-07, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n           verbose=0, warm_start=False)  # Test MSE\nprint('Test MSE:', mean_squared_error(y_test, rgr.predict(X_test)))  Test MSE: 3.23118576667  # Variable importance\nimportance = pd.DataFrame({'Importance':rgr.feature_importances_*100}, index=X_train.columns)\nimportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r')\nplt.xlabel('Variable importance')\nplt.legend(loc='lower right')  <matplotlib.legend.Legend at 0xec02978>   The test MSE decreased when compared with the previous answers. One possible explanation for this result is the effect of  m . In random forests, the number of variables considered at each split changes. This means that, at each split, only a subset of predictors is taken into account. In some sense, this works as a process to decorrelate the decision trees. For example, in baggin, if there is a strong predictor in the data set, most of the bagged trees wil use this predictor in the top split. As a consequence, the predictions from the bagged trees will tend to be highly correlated. In random forests, this doesn't happen because the method forces each split to consider only a subset of the predictors. Hence, the method leads to models with reducted variance and increased reliability. That's a possible reason for the reduction verified in the test MSE value.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter8/exercise8/#references",
            "text": "https://packaging.python.org/installing/ (help to install Python modules)  http://www.graphviz.org/Download..php (download Graphviz)  http://stackoverflow.com/questions/18438997/why-is-pydot-unable-to-find-graphvizs-executables-in-windows-8 (how to install Graphviz)  http://scikit-learn.org/stable/modules/tree.html  (plotting decision trees with scikit)",
            "title": "References"
        },
        {
            "location": "/sols/chapter8/exercise8/#2do",
            "text": "Pay attention during the review because the solutions that I've don't match with the existing solutions. I suppose that happens because of the randomness associated to the exercise, but I'm not sure.  Find out how to get variable importance when using bagging.",
            "title": "2DO"
        },
        {
            "location": "/sols/chapter8/exercise9/",
            "text": "Exercise 8.9",
            "title": "8.9"
        },
        {
            "location": "/sols/chapter8/exercise9/#exercise-89",
            "text": "",
            "title": "Exercise 8.9"
        },
        {
            "location": "/sols/chapter8/exercise10/",
            "text": "Exercise 8.10\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import BaggingRegressor\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/Hitters.csv', index_col=0)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nAtBat\n\n      \nHits\n\n      \nHmRun\n\n      \nRuns\n\n      \nRBI\n\n      \nWalks\n\n      \nYears\n\n      \nCAtBat\n\n      \nCHits\n\n      \nCHmRun\n\n      \nCRuns\n\n      \nCRBI\n\n      \nCWalks\n\n      \nLeague\n\n      \nDivision\n\n      \nPutOuts\n\n      \nAssists\n\n      \nErrors\n\n      \nSalary\n\n      \nNewLeague\n\n    \n\n  \n\n  \n\n    \n\n      \n-Andy Allanson\n\n      \n293\n\n      \n66\n\n      \n1\n\n      \n30\n\n      \n29\n\n      \n14\n\n      \n1\n\n      \n293\n\n      \n66\n\n      \n1\n\n      \n30\n\n      \n29\n\n      \n14\n\n      \nA\n\n      \nE\n\n      \n446\n\n      \n33\n\n      \n20\n\n      \nNaN\n\n      \nA\n\n    \n\n    \n\n      \n-Alan Ashby\n\n      \n315\n\n      \n81\n\n      \n7\n\n      \n24\n\n      \n38\n\n      \n39\n\n      \n14\n\n      \n3449\n\n      \n835\n\n      \n69\n\n      \n321\n\n      \n414\n\n      \n375\n\n      \nN\n\n      \nW\n\n      \n632\n\n      \n43\n\n      \n10\n\n      \n475.0\n\n      \nN\n\n    \n\n    \n\n      \n-Alvin Davis\n\n      \n479\n\n      \n130\n\n      \n18\n\n      \n66\n\n      \n72\n\n      \n76\n\n      \n3\n\n      \n1624\n\n      \n457\n\n      \n63\n\n      \n224\n\n      \n266\n\n      \n263\n\n      \nA\n\n      \nW\n\n      \n880\n\n      \n82\n\n      \n14\n\n      \n480.0\n\n      \nA\n\n    \n\n    \n\n      \n-Andre Dawson\n\n      \n496\n\n      \n141\n\n      \n20\n\n      \n65\n\n      \n78\n\n      \n37\n\n      \n11\n\n      \n5628\n\n      \n1575\n\n      \n225\n\n      \n828\n\n      \n838\n\n      \n354\n\n      \nN\n\n      \nE\n\n      \n200\n\n      \n11\n\n      \n3\n\n      \n500.0\n\n      \nN\n\n    \n\n    \n\n      \n-Andres Galarraga\n\n      \n321\n\n      \n87\n\n      \n10\n\n      \n39\n\n      \n42\n\n      \n30\n\n      \n2\n\n      \n396\n\n      \n101\n\n      \n12\n\n      \n48\n\n      \n46\n\n      \n33\n\n      \nN\n\n      \nE\n\n      \n805\n\n      \n40\n\n      \n4\n\n      \n91.5\n\n      \nN\n\n    \n\n  \n\n\n\n\n\n\n\ndf = pd.get_dummies(df)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nAtBat\n\n      \nHits\n\n      \nHmRun\n\n      \nRuns\n\n      \nRBI\n\n      \nWalks\n\n      \nYears\n\n      \nCAtBat\n\n      \nCHits\n\n      \nCHmRun\n\n      \n...\n\n      \nPutOuts\n\n      \nAssists\n\n      \nErrors\n\n      \nSalary\n\n      \nLeague_A\n\n      \nLeague_N\n\n      \nDivision_E\n\n      \nDivision_W\n\n      \nNewLeague_A\n\n      \nNewLeague_N\n\n    \n\n  \n\n  \n\n    \n\n      \n-Andy Allanson\n\n      \n293\n\n      \n66\n\n      \n1\n\n      \n30\n\n      \n29\n\n      \n14\n\n      \n1\n\n      \n293\n\n      \n66\n\n      \n1\n\n      \n...\n\n      \n446\n\n      \n33\n\n      \n20\n\n      \nNaN\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n    \n\n    \n\n      \n-Alan Ashby\n\n      \n315\n\n      \n81\n\n      \n7\n\n      \n24\n\n      \n38\n\n      \n39\n\n      \n14\n\n      \n3449\n\n      \n835\n\n      \n69\n\n      \n...\n\n      \n632\n\n      \n43\n\n      \n10\n\n      \n475.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n-Alvin Davis\n\n      \n479\n\n      \n130\n\n      \n18\n\n      \n66\n\n      \n72\n\n      \n76\n\n      \n3\n\n      \n1624\n\n      \n457\n\n      \n63\n\n      \n...\n\n      \n880\n\n      \n82\n\n      \n14\n\n      \n480.0\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n      \n1.0\n\n      \n0.0\n\n    \n\n    \n\n      \n-Andre Dawson\n\n      \n496\n\n      \n141\n\n      \n20\n\n      \n65\n\n      \n78\n\n      \n37\n\n      \n11\n\n      \n5628\n\n      \n1575\n\n      \n225\n\n      \n...\n\n      \n200\n\n      \n11\n\n      \n3\n\n      \n500.0\n\n      \n0.0\n\n      \n1.0\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n-Andres Galarraga\n\n      \n321\n\n      \n87\n\n      \n10\n\n      \n39\n\n      \n42\n\n      \n30\n\n      \n2\n\n      \n396\n\n      \n101\n\n      \n12\n\n      \n...\n\n      \n805\n\n      \n40\n\n      \n4\n\n      \n91.5\n\n      \n0.0\n\n      \n1.0\n\n      \n1.0\n\n      \n0.0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n  \n\n\n\n\n5 rows \u00d7 23 columns\n\n\n\n\n\n(a)\n\n\n# Remove observations for whom the salary information is unknown.\ndf = df.dropna(subset=['Salary'])\n\n\n\n\n# Log transform salaries\ndf['Salary'] = np.log(df['Salary'])\n\n\n\n\n(b)\n\n\n# Create training and testing \n# We don't use the train_test_split because we don't want to split data randomly.\nX = df.drop(['Salary'], axis=1)\ny = df['Salary']\n\nX_train = X.ix[:200,:]\ny_train = y.ix[:200]\nX_test = X.ix[200:,:]\ny_test = y.ix[200:]\n\n\n\n\n(c)\n\n\n# Boosting with different shrinkage values\nshrinkage_values = [.001, .025, .005, .01, .025, .05, .1, .25, .5]\nmses = []\nfor i in shrinkage_values:\n    bst = GradientBoostingRegressor(learning_rate=i, n_estimators=1000, random_state=1)\n    bst.fit(X_train, y_train)\n    mses.append(mean_squared_error(y_train, bst.predict(X_train)))\n\n\n\n\n# Plot training set MSE for different shrinkage values\nplt.scatter(shrinkage_values, mses)\n\n\n\n\n<matplotlib.collections.PathCollection at 0xf98e0f0>\n\n\n\n\n\n(d)\n\n\n# Boosting with different shrinkage values\nshrinkage_values = [.001, .025, .005, .01, .025, .05, .1, .25, .5]\nmses = []\nfor i in shrinkage_values:\n    bst = GradientBoostingRegressor(learning_rate=i, n_estimators=1000, random_state=1)\n    bst.fit(X_train, y_train)\n    mses.append(mean_squared_error(y_test, bst.predict(X_test)))\n\n\n\n\n# Plot training set MSE for different shrinkage values\nplt.scatter(shrinkage_values, mses)\n\n\n\n\n<matplotlib.collections.PathCollection at 0xf9f5240>\n\n\n\n\n\n# Get minimum test MSE value\nprint('Minimum test MSE:', np.min(mses))\n\n\n\n\nMinimum test MSE: 0.208753925111\n\n\n\n# Index of the shrinkage_value that leads to the minimum test MSE\nnp.where(mses == np.min(mses))\n\n\n\n\n(array([2], dtype=int64),)\n\n\n\n(e)\n\n\n# Linear regression\nrgr = LinearRegression()\nrgr.fit(X_train, y_train)\n\nprint('Minimum test MSE:', mean_squared_error(y_test, rgr.predict(X_test)))\n\n\n\n\nMinimum test MSE: 0.491795937545\n\n\n\n# Cross-validated lasso\nlasso = LassoCV(cv=5)\nlasso.fit(X_train, y_train)\n\nprint('Minimum test MSE:', mean_squared_error(y_test, lasso.predict(X_test)))\n\n\n\n\nMinimum test MSE: 0.486586369603\n\n\n\nThe test MSE obtained using boosting is lower than the test MSE obtained using a linear regression or a lasso regularized regression. This means that, according to this error metric, boosting is the model with better predictive capacity.\n\n\n(f)\n\n\n# Plot features importance to understand their importance.\nbst = GradientBoostingRegressor(learning_rate=0.005)  # 0.005 is the learning_rate corresponding to the best test MSE\nbst.fit(X_train, y_train)\n\nfeature_importance = bst.feature_importances_*100\nrel_imp = pd.Series(feature_importance, index=X.columns).sort_values(inplace=False)\nrel_imp.T.plot(kind='barh', color='r')\nplt.xlabel('Variable importance')\n\n\n\n\n<matplotlib.text.Text at 0xd682e10>\n\n\n\n\n\nAccording to the figure, the most important predictors seem to be: CAtBat, AtBat, CRuns, Walks and CRBI.\n\n\n(g)\n\n\n# Fit bagging regressor\nbagging = BaggingRegressor()\nbagging.fit(X_train, y_train)\n\n\n\n\nBaggingRegressor(base_estimator=None, bootstrap=True,\n         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n         verbose=0, warm_start=False)\n\n\n\n# Test MSE\nprint('Test MSE:', mean_squared_error(y_test, bagging.predict(X_test)))\n\n\n\n\nTest MSE: 0.253358375264",
            "title": "8.10"
        },
        {
            "location": "/sols/chapter8/exercise10/#exercise-810",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import BaggingRegressor\n\n%matplotlib inline  df = pd.read_csv('../data/Hitters.csv', index_col=0)  df.head()   \n   \n     \n       \n       AtBat \n       Hits \n       HmRun \n       Runs \n       RBI \n       Walks \n       Years \n       CAtBat \n       CHits \n       CHmRun \n       CRuns \n       CRBI \n       CWalks \n       League \n       Division \n       PutOuts \n       Assists \n       Errors \n       Salary \n       NewLeague \n     \n   \n   \n     \n       -Andy Allanson \n       293 \n       66 \n       1 \n       30 \n       29 \n       14 \n       1 \n       293 \n       66 \n       1 \n       30 \n       29 \n       14 \n       A \n       E \n       446 \n       33 \n       20 \n       NaN \n       A \n     \n     \n       -Alan Ashby \n       315 \n       81 \n       7 \n       24 \n       38 \n       39 \n       14 \n       3449 \n       835 \n       69 \n       321 \n       414 \n       375 \n       N \n       W \n       632 \n       43 \n       10 \n       475.0 \n       N \n     \n     \n       -Alvin Davis \n       479 \n       130 \n       18 \n       66 \n       72 \n       76 \n       3 \n       1624 \n       457 \n       63 \n       224 \n       266 \n       263 \n       A \n       W \n       880 \n       82 \n       14 \n       480.0 \n       A \n     \n     \n       -Andre Dawson \n       496 \n       141 \n       20 \n       65 \n       78 \n       37 \n       11 \n       5628 \n       1575 \n       225 \n       828 \n       838 \n       354 \n       N \n       E \n       200 \n       11 \n       3 \n       500.0 \n       N \n     \n     \n       -Andres Galarraga \n       321 \n       87 \n       10 \n       39 \n       42 \n       30 \n       2 \n       396 \n       101 \n       12 \n       48 \n       46 \n       33 \n       N \n       E \n       805 \n       40 \n       4 \n       91.5 \n       N \n     \n      df = pd.get_dummies(df)  df.head()   \n   \n     \n       \n       AtBat \n       Hits \n       HmRun \n       Runs \n       RBI \n       Walks \n       Years \n       CAtBat \n       CHits \n       CHmRun \n       ... \n       PutOuts \n       Assists \n       Errors \n       Salary \n       League_A \n       League_N \n       Division_E \n       Division_W \n       NewLeague_A \n       NewLeague_N \n     \n   \n   \n     \n       -Andy Allanson \n       293 \n       66 \n       1 \n       30 \n       29 \n       14 \n       1 \n       293 \n       66 \n       1 \n       ... \n       446 \n       33 \n       20 \n       NaN \n       1.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n       0.0 \n     \n     \n       -Alan Ashby \n       315 \n       81 \n       7 \n       24 \n       38 \n       39 \n       14 \n       3449 \n       835 \n       69 \n       ... \n       632 \n       43 \n       10 \n       475.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n       0.0 \n       1.0 \n     \n     \n       -Alvin Davis \n       479 \n       130 \n       18 \n       66 \n       72 \n       76 \n       3 \n       1624 \n       457 \n       63 \n       ... \n       880 \n       82 \n       14 \n       480.0 \n       1.0 \n       0.0 \n       0.0 \n       1.0 \n       1.0 \n       0.0 \n     \n     \n       -Andre Dawson \n       496 \n       141 \n       20 \n       65 \n       78 \n       37 \n       11 \n       5628 \n       1575 \n       225 \n       ... \n       200 \n       11 \n       3 \n       500.0 \n       0.0 \n       1.0 \n       1.0 \n       0.0 \n       0.0 \n       1.0 \n     \n     \n       -Andres Galarraga \n       321 \n       87 \n       10 \n       39 \n       42 \n       30 \n       2 \n       396 \n       101 \n       12 \n       ... \n       805 \n       40 \n       4 \n       91.5 \n       0.0 \n       1.0 \n       1.0 \n       0.0 \n       0.0 \n       1.0 \n     \n     5 rows \u00d7 23 columns",
            "title": "Exercise 8.10"
        },
        {
            "location": "/sols/chapter8/exercise10/#a",
            "text": "# Remove observations for whom the salary information is unknown.\ndf = df.dropna(subset=['Salary'])  # Log transform salaries\ndf['Salary'] = np.log(df['Salary'])",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter8/exercise10/#b",
            "text": "# Create training and testing \n# We don't use the train_test_split because we don't want to split data randomly.\nX = df.drop(['Salary'], axis=1)\ny = df['Salary']\n\nX_train = X.ix[:200,:]\ny_train = y.ix[:200]\nX_test = X.ix[200:,:]\ny_test = y.ix[200:]",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter8/exercise10/#c",
            "text": "# Boosting with different shrinkage values\nshrinkage_values = [.001, .025, .005, .01, .025, .05, .1, .25, .5]\nmses = []\nfor i in shrinkage_values:\n    bst = GradientBoostingRegressor(learning_rate=i, n_estimators=1000, random_state=1)\n    bst.fit(X_train, y_train)\n    mses.append(mean_squared_error(y_train, bst.predict(X_train)))  # Plot training set MSE for different shrinkage values\nplt.scatter(shrinkage_values, mses)  <matplotlib.collections.PathCollection at 0xf98e0f0>",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter8/exercise10/#d",
            "text": "# Boosting with different shrinkage values\nshrinkage_values = [.001, .025, .005, .01, .025, .05, .1, .25, .5]\nmses = []\nfor i in shrinkage_values:\n    bst = GradientBoostingRegressor(learning_rate=i, n_estimators=1000, random_state=1)\n    bst.fit(X_train, y_train)\n    mses.append(mean_squared_error(y_test, bst.predict(X_test)))  # Plot training set MSE for different shrinkage values\nplt.scatter(shrinkage_values, mses)  <matplotlib.collections.PathCollection at 0xf9f5240>   # Get minimum test MSE value\nprint('Minimum test MSE:', np.min(mses))  Minimum test MSE: 0.208753925111  # Index of the shrinkage_value that leads to the minimum test MSE\nnp.where(mses == np.min(mses))  (array([2], dtype=int64),)",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter8/exercise10/#e",
            "text": "# Linear regression\nrgr = LinearRegression()\nrgr.fit(X_train, y_train)\n\nprint('Minimum test MSE:', mean_squared_error(y_test, rgr.predict(X_test)))  Minimum test MSE: 0.491795937545  # Cross-validated lasso\nlasso = LassoCV(cv=5)\nlasso.fit(X_train, y_train)\n\nprint('Minimum test MSE:', mean_squared_error(y_test, lasso.predict(X_test)))  Minimum test MSE: 0.486586369603  The test MSE obtained using boosting is lower than the test MSE obtained using a linear regression or a lasso regularized regression. This means that, according to this error metric, boosting is the model with better predictive capacity.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter8/exercise10/#f",
            "text": "# Plot features importance to understand their importance.\nbst = GradientBoostingRegressor(learning_rate=0.005)  # 0.005 is the learning_rate corresponding to the best test MSE\nbst.fit(X_train, y_train)\n\nfeature_importance = bst.feature_importances_*100\nrel_imp = pd.Series(feature_importance, index=X.columns).sort_values(inplace=False)\nrel_imp.T.plot(kind='barh', color='r')\nplt.xlabel('Variable importance')  <matplotlib.text.Text at 0xd682e10>   According to the figure, the most important predictors seem to be: CAtBat, AtBat, CRuns, Walks and CRBI.",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter8/exercise10/#g",
            "text": "# Fit bagging regressor\nbagging = BaggingRegressor()\nbagging.fit(X_train, y_train)  BaggingRegressor(base_estimator=None, bootstrap=True,\n         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n         verbose=0, warm_start=False)  # Test MSE\nprint('Test MSE:', mean_squared_error(y_test, bagging.predict(X_test)))  Test MSE: 0.253358375264",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter8/exercise11/",
            "text": "Exercise 8.11",
            "title": "8.11"
        },
        {
            "location": "/sols/chapter8/exercise11/#exercise-811",
            "text": "",
            "title": "Exercise 8.11"
        },
        {
            "location": "/sols/chapter8/exercise12/",
            "text": "Exercise 8.12",
            "title": "8.12"
        },
        {
            "location": "/sols/chapter8/exercise12/#exercise-812",
            "text": "",
            "title": "Exercise 8.12"
        },
        {
            "location": "/sols/chapter9/exercise1/",
            "text": "Exercise 9.1",
            "title": "9.1"
        },
        {
            "location": "/sols/chapter9/exercise1/#exercise-91",
            "text": "",
            "title": "Exercise 9.1"
        },
        {
            "location": "/sols/chapter9/exercise2/",
            "text": "Exercise 9.2\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\n(1+X_1)^2+(2-X_2)^2=4\n(1+X_1)^2+(2-X_2)^2=4\n is the equation of a circle. The circle equation is in the format \n(x \u2013 h)^2 + (y \u2013 k)^2 = r^2\n(x \u2013 h)^2 + (y \u2013 k)^2 = r^2\n, where \nh\n and \nk\n are the center of the circle and \nr\n is the radius.\n\n\n# Draw circle\ncircle =plt.Circle((-1,2), 2, color='r', fill=False)\n\nfig, ax = plt.subplots()\n\nax.axis(\"equal\")  # To avoid oval circles. Check the References.\nax.add_artist(circle)\nax.set_xlim((-10,10))\nax.set_ylim((-10,10))\n\n\n\n\n(-10, 10)\n\n\n\n\n\n(b)\n\n\n# Draw circle\ncircle_background =plt.Circle((-1,2), 20, color='b')  # Way to fool matplotlib and have a colored background.\ncircle =plt.Circle((-1,2), 2, color='r')\n\nfig, ax = plt.subplots()\n\nax.axis(\"equal\")  # To avoid oval circles. Check the References.\nax.add_artist(circle_background)\nax.add_artist(circle)\nax.set_xlim((-10,10))\nax.set_ylim((-10,10))\nplt.show()\n\n\n\n\n\n\n\n\n(1+X_1)^2+(2-X_2)^2>4\n(1+X_1)^2+(2-X_2)^2>4\n - Blue region.\n\n\n(1+X_1)^2+(2-X_2)^2 \\leq 4\n(1+X_1)^2+(2-X_2)^2 \\leq 4\n - Red region.\n\n\n\n\n(c)\n\n\n# Draw circle\ncircle_background =plt.Circle((-1,2), 20, color='b')  # Way to fool matplotlib and have a colored background.\ncircle =plt.Circle((-1,2), 2, color='r')\n\nfig, ax = plt.subplots()\nfig.set_size_inches(10, 9)\n\nax.axis(\"equal\")  # To avoid oval circles. Check the References.\nax.add_artist(circle_background)\nax.add_artist(circle)\nax.set_xlim((-10,10))\nax.set_ylim((-1,10))\n\nplt.annotate('X (0,0)', xy=(0,0), xytext=(0,0))\nplt.annotate('X (-1,1)', xy=(-1,1), xytext=(-1,1))\nplt.annotate('X (2,2)', xy=(2,2), xytext=(2,2))\nplt.annotate('X (3,8)', xy=(3,8), xytext=(3,8))\nplt.show()\n\n\n\n\n\n\n\n\nBlue region - (0,0); (2,2); (3,8).\n\n\nRed region - (-1,1).\n\n\n\n\nReferences\n\n\n\n\nhttp://stackoverflow.com/questions/9215658/plot-a-circle-with-pyplot (how to draw a circle)\n\n\nhttp://stackoverflow.com/questions/9230389/why-is-matplotlib-plotting-my-circles-as-ovals/9232513#9232513 (solving matplot oval circles)",
            "title": "9.2"
        },
        {
            "location": "/sols/chapter9/exercise2/#exercise-92",
            "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline",
            "title": "Exercise 9.2"
        },
        {
            "location": "/sols/chapter9/exercise2/#a",
            "text": "(1+X_1)^2+(2-X_2)^2=4 (1+X_1)^2+(2-X_2)^2=4  is the equation of a circle. The circle equation is in the format  (x \u2013 h)^2 + (y \u2013 k)^2 = r^2 (x \u2013 h)^2 + (y \u2013 k)^2 = r^2 , where  h  and  k  are the center of the circle and  r  is the radius.  # Draw circle\ncircle =plt.Circle((-1,2), 2, color='r', fill=False)\n\nfig, ax = plt.subplots()\n\nax.axis(\"equal\")  # To avoid oval circles. Check the References.\nax.add_artist(circle)\nax.set_xlim((-10,10))\nax.set_ylim((-10,10))  (-10, 10)",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter9/exercise2/#b",
            "text": "# Draw circle\ncircle_background =plt.Circle((-1,2), 20, color='b')  # Way to fool matplotlib and have a colored background.\ncircle =plt.Circle((-1,2), 2, color='r')\n\nfig, ax = plt.subplots()\n\nax.axis(\"equal\")  # To avoid oval circles. Check the References.\nax.add_artist(circle_background)\nax.add_artist(circle)\nax.set_xlim((-10,10))\nax.set_ylim((-10,10))\nplt.show()    (1+X_1)^2+(2-X_2)^2>4 (1+X_1)^2+(2-X_2)^2>4  - Blue region.  (1+X_1)^2+(2-X_2)^2 \\leq 4 (1+X_1)^2+(2-X_2)^2 \\leq 4  - Red region.",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter9/exercise2/#c",
            "text": "# Draw circle\ncircle_background =plt.Circle((-1,2), 20, color='b')  # Way to fool matplotlib and have a colored background.\ncircle =plt.Circle((-1,2), 2, color='r')\n\nfig, ax = plt.subplots()\nfig.set_size_inches(10, 9)\n\nax.axis(\"equal\")  # To avoid oval circles. Check the References.\nax.add_artist(circle_background)\nax.add_artist(circle)\nax.set_xlim((-10,10))\nax.set_ylim((-1,10))\n\nplt.annotate('X (0,0)', xy=(0,0), xytext=(0,0))\nplt.annotate('X (-1,1)', xy=(-1,1), xytext=(-1,1))\nplt.annotate('X (2,2)', xy=(2,2), xytext=(2,2))\nplt.annotate('X (3,8)', xy=(3,8), xytext=(3,8))\nplt.show()    Blue region - (0,0); (2,2); (3,8).  Red region - (-1,1).",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter9/exercise2/#references",
            "text": "http://stackoverflow.com/questions/9215658/plot-a-circle-with-pyplot (how to draw a circle)  http://stackoverflow.com/questions/9230389/why-is-matplotlib-plotting-my-circles-as-ovals/9232513#9232513 (solving matplot oval circles)",
            "title": "References"
        },
        {
            "location": "/sols/chapter9/exercise3/",
            "text": "Exercise 9.3",
            "title": "9.3"
        },
        {
            "location": "/sols/chapter9/exercise3/#exercise-93",
            "text": "",
            "title": "Exercise 9.3"
        },
        {
            "location": "/sols/chapter9/exercise4/",
            "text": "Exercise 9.4\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n%matplotlib inline\n\n\n\n\n Function to plot classifiers with support vectors \n\n\n# Plot classifiers\n# Source: https://github.com/JWarmenhoven/ISLR-python/blob/master/Notebooks/Chapter%209.ipynb\ndef plot_svc(svc, X, y, h=0.02, pad=0.25):\n    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n\n    plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired)\n    # Support vectors indicated in plot by vertical lines\n    sv = svc.support_vectors_\n    plt.scatter(sv[:,0], sv[:,1], c='k', marker='|', s=100, linewidths='1')\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.show()\n    print('Number of support vectors: ', svc.support_.size)\n\n\n\n\n Prepare data \n\n\n# Generate data set\ncontrol = 3  # Controls how mixed the classes can be.\nnp.random.seed(5)  # We changed the random.seed value until it gets what we wanted.\nX = np.random.randn(100,2)\ny = np.random.choice([-1,1], 100)\nX[y == 1] = X[y == 1] - control\n\n\n\n\n# Plot\nplt.scatter(X[:,0], X[:,1], s=50, c=y, cmap=plt.cm.Paired)\nplt.xlabel('X1')\nplt.ylabel('X2');\n\n\n\n\n\n\n# Split data into train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n\n\n\n\n Linear SVC \n\n\n# Linear SVC\nsvc = SVC(kernel='linear', random_state=1)\nsvc.fit(X_train,y_train)\n\n\n\n\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\n# Plot\nplot_svc(svc, X, y)\n\n\n\n\n\n\nNumber of support vectors:  9\n\n\n\n# Confusion matrices\nprint('Confusion matrix for train set: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Confusion matrix for test set: ', confusion_matrix(y_test, svc.predict(X_test)))\n\n\n\n\nConfusion matrix for train set:  [[37  0]\n [ 1 42]]\nConfusion matrix for test set:  [[10  0]\n [ 0 10]]\n\n\n\n# Error rate\nprint('Error rate for train set: ', (0+1)/(37+0+1+42))\nprint('Error rate for test set: ', (0+0)/(10+0+0+10))\n\n\n\n\nError rate for train set:  0.0125\nError rate for test set:  0.0\n\n\n\n Polynomial SVC \n\n\n# Polynomial SVC\nsvc = SVC(kernel='poly', random_state=1)\nsvc.fit(X_train,y_train)\n\n\n\n\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\n# Plot\nplot_svc(svc, X, y)\n\n\n\n\n\n\nNumber of support vectors:  4\n\n\n\n# Confusion matrices\nprint('Confusion matrix for train set: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Confusion matrix for test set: ', confusion_matrix(y_test, svc.predict(X_test)))\n\n\n\n\nConfusion matrix for train set:  [[37  0]\n [ 0 43]]\nConfusion matrix for test set:  [[10  0]\n [ 0 10]]\n\n\n\n# Error rate\nprint('Error rate for train set: ', (0+0)/(37+0+0+43))\nprint('Error rate for test set: ', (0+0)/(10+0+0+10))\n\n\n\n\nError rate for train set:  0.0\nError rate for test set:  0.0\n\n\n\n Radial SVC \n\n\n# Radial SVC\nsvc = SVC(kernel='rbf', random_state=1)\nsvc.fit(X_train,y_train)\n\n\n\n\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\n# Plot\nplot_svc(svc, X, y)\n\n\n\n\n\n\nNumber of support vectors:  25\n\n\n\n# Confusion matrices\nprint('Confusion matrix for train set: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Confusion matrix for test set: ', confusion_matrix(y_test, svc.predict(X_test)))\n\n\n\n\nConfusion matrix for train set:  [[37  0]\n [ 0 43]]\nConfusion matrix for test set:  [[10  0]\n [ 0 10]]\n\n\n\n# Error rate\nprint('Error rate for train set: ', (0+0)/(37+0+0+43))\nprint('Error rate for test set: ', (0+0)/(10+0+0+10))\n\n\n\n\nError rate for train set:  0.0\nError rate for test set:  0.0\n\n\n\n Discussion \n\n\nThis simple example shows a situation in which the linear SVC doesn't work as well as the other kernels (polynomial and radial). Considering the dataset, it is not possible to get a perfect prediction with a linear kernel. However, it is possible to get a perfect prediction with both the polynomial and the radial kernels. In this specific dataset, the polynomial and the radial error rates have the same value, but it is possible to notice the differences between these two kernels by the observation of the plots. The boundary line in the polynomial kernel has a shape similar to a polynomial function (!!!), while the boundary line resulting from the radial kernel is more flexible in its shape.",
            "title": "9.4"
        },
        {
            "location": "/sols/chapter9/exercise4/#exercise-94",
            "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n%matplotlib inline   Function to plot classifiers with support vectors   # Plot classifiers\n# Source: https://github.com/JWarmenhoven/ISLR-python/blob/master/Notebooks/Chapter%209.ipynb\ndef plot_svc(svc, X, y, h=0.02, pad=0.25):\n    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n\n    plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired)\n    # Support vectors indicated in plot by vertical lines\n    sv = svc.support_vectors_\n    plt.scatter(sv[:,0], sv[:,1], c='k', marker='|', s=100, linewidths='1')\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.show()\n    print('Number of support vectors: ', svc.support_.size)   Prepare data   # Generate data set\ncontrol = 3  # Controls how mixed the classes can be.\nnp.random.seed(5)  # We changed the random.seed value until it gets what we wanted.\nX = np.random.randn(100,2)\ny = np.random.choice([-1,1], 100)\nX[y == 1] = X[y == 1] - control  # Plot\nplt.scatter(X[:,0], X[:,1], s=50, c=y, cmap=plt.cm.Paired)\nplt.xlabel('X1')\nplt.ylabel('X2');   # Split data into train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)   Linear SVC   # Linear SVC\nsvc = SVC(kernel='linear', random_state=1)\nsvc.fit(X_train,y_train)  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)  # Plot\nplot_svc(svc, X, y)   Number of support vectors:  9  # Confusion matrices\nprint('Confusion matrix for train set: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Confusion matrix for test set: ', confusion_matrix(y_test, svc.predict(X_test)))  Confusion matrix for train set:  [[37  0]\n [ 1 42]]\nConfusion matrix for test set:  [[10  0]\n [ 0 10]]  # Error rate\nprint('Error rate for train set: ', (0+1)/(37+0+1+42))\nprint('Error rate for test set: ', (0+0)/(10+0+0+10))  Error rate for train set:  0.0125\nError rate for test set:  0.0   Polynomial SVC   # Polynomial SVC\nsvc = SVC(kernel='poly', random_state=1)\nsvc.fit(X_train,y_train)  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)  # Plot\nplot_svc(svc, X, y)   Number of support vectors:  4  # Confusion matrices\nprint('Confusion matrix for train set: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Confusion matrix for test set: ', confusion_matrix(y_test, svc.predict(X_test)))  Confusion matrix for train set:  [[37  0]\n [ 0 43]]\nConfusion matrix for test set:  [[10  0]\n [ 0 10]]  # Error rate\nprint('Error rate for train set: ', (0+0)/(37+0+0+43))\nprint('Error rate for test set: ', (0+0)/(10+0+0+10))  Error rate for train set:  0.0\nError rate for test set:  0.0   Radial SVC   # Radial SVC\nsvc = SVC(kernel='rbf', random_state=1)\nsvc.fit(X_train,y_train)  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)  # Plot\nplot_svc(svc, X, y)   Number of support vectors:  25  # Confusion matrices\nprint('Confusion matrix for train set: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Confusion matrix for test set: ', confusion_matrix(y_test, svc.predict(X_test)))  Confusion matrix for train set:  [[37  0]\n [ 0 43]]\nConfusion matrix for test set:  [[10  0]\n [ 0 10]]  # Error rate\nprint('Error rate for train set: ', (0+0)/(37+0+0+43))\nprint('Error rate for test set: ', (0+0)/(10+0+0+10))  Error rate for train set:  0.0\nError rate for test set:  0.0   Discussion   This simple example shows a situation in which the linear SVC doesn't work as well as the other kernels (polynomial and radial). Considering the dataset, it is not possible to get a perfect prediction with a linear kernel. However, it is possible to get a perfect prediction with both the polynomial and the radial kernels. In this specific dataset, the polynomial and the radial error rates have the same value, but it is possible to notice the differences between these two kernels by the observation of the plots. The boundary line in the polynomial kernel has a shape similar to a polynomial function (!!!), while the boundary line resulting from the radial kernel is more flexible in its shape.",
            "title": "Exercise 9.4"
        },
        {
            "location": "/sols/chapter9/exercise5/",
            "text": "Exercise 9.5",
            "title": "9.5"
        },
        {
            "location": "/sols/chapter9/exercise5/#exercise-95",
            "text": "",
            "title": "Exercise 9.5"
        },
        {
            "location": "/sols/chapter9/exercise6/",
            "text": "Exercise 9.6\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\n# Generate dataset\nX, y = make_classification(n_samples=500, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=0)\n\n\n\n\n# Plot\nplt.scatter(X[:,0], X[:,1], s=25, c=y, cmap=plt.cm.Paired)\nplt.xlabel('X1')\nplt.ylabel('X2')\n\n\n\n\n<matplotlib.text.Text at 0x12ea2cf8>\n\n\n\n\n\n(b)\n\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)\n\n\n\n\n# Cost range\ncost_range = [0.01, .1, 1, 10]\n\n\n\n\n# Cross-validation error rates\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni = ', i)\n    print('%.3f' % (np.average(1-cross_val_score(svc, X, y, cv=5, scoring='accuracy'))))\n\n\n\n\ni =  0.01\n0.140\n\ni =  0.1\n0.122\n\ni =  1\n0.126\n\ni =  10\n0.124\n\n\n\n# Confusion matrix for a range of cost values\n# This is a necessary step to get the number of misclassifications.\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni =', i)\n    print('Confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))\n\n\n\n\ni = 0.01\nConfusion matrix:  [[153  23]\n [ 27 147]]\n\ni = 0.1\nConfusion matrix:  [[167   9]\n [ 32 142]]\n\ni = 1\nConfusion matrix:  [[163  13]\n [ 31 143]]\n\ni = 10\nConfusion matrix:  [[162  14]\n [ 31 143]]\n\n\n\n# Misclassifications\nmisclass_1 = 23+27\nmisclass_2 = 9+32\nmisclass_3 = 13+31\nmisclass_4 = 14+31\n\nmisclass = [misclass_1, misclass_2, misclass_3, misclass_4]\n\nfor i in range(0,4):\n    print('Misclassifications, i = %.3f, %i' % (cost_range[i], misclass[i]))\n\n\n\n\nMisclassifications, i = 0.010, 50\nMisclassifications, i = 0.100, 41\nMisclassifications, i = 1.000, 44\nMisclassifications, i = 10.000, 45\n\n\n\nThe cost value with less cross-validation error is C=0.1, which is also the cost value with less misclassifications. This is what we wanted to see because it shows that the training error (evaluated by the number of misclassifications) is in accordance with test error (here evaluated by the cross-validation error). \n\n\n(c)\n\n\n# Generate dataset\nX, y = make_classification(n_samples=500, random_state=1)\n\n\n\n\n# Plot\nplt.scatter(X[:,0], X[:,1], s=25, c=y, cmap=plt.cm.Paired)\nplt.xlabel('X1')\nplt.ylabel('X2')\n\n\n\n\n<matplotlib.text.Text at 0x12f2c3c8>\n\n\n\n\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)\n\n\n\n\n# Test errors\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni: ', i)\n    print('Test error: ', 1-accuracy_score(y_test, svc.predict(X_test)))\n\n\n\n\ni:  0.01\nTest error:  0.0533333333333\n\ni:  0.1\nTest error:  0.06\n\ni:  1\nTest error:  0.08\n\ni:  10\nTest error:  0.0866666666667\n\n\n\n# Training errors\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni: ', i)\n    print('Training error: ', 1-accuracy_score(y_train, svc.predict(X_train)))\n\n\n\n\ni:  0.01\nTraining error:  0.0485714285714\n\ni:  0.1\nTraining error:  0.0571428571429\n\ni:  1\nTraining error:  0.0428571428571\n\ni:  10\nTraining error:  0.0371428571429\n\n\n\n# Cross-validation errors\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni: ', i)\n    print('Cross-validation error: ', np.average(1-cross_val_score(svc, X, y, scoring='accuracy', cv=5)))\n\n\n\n\ni:  0.01\nCross-validation error:  0.0540218021802\n\ni:  0.1\nCross-validation error:  0.05600160016\n\ni:  1\nCross-validation error:  0.0580218021802\n\ni:  10\nCross-validation error:  0.0660420042004\n\n\n\nThe fewest training error is reached when the cost value is 10, while the fewest cross-validation errors occurs when the cost value is 0.01. The fewest cross-validation error is also when the cost value is 0.01. \n\n\nThese results show that high cost values tend to overfit the data. They can give the best results regarding the training error, but when we try to generalize the results they don't work so well. This happens because an high cost value will reduce the number of support vector violating the margin. Thus, the approach gets too sticked to the observations of the training set and will not be able to generalize well, missing the noisy points.",
            "title": "9.6"
        },
        {
            "location": "/sols/chapter9/exercise6/#exercise-96",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\n\n%matplotlib inline",
            "title": "Exercise 9.6"
        },
        {
            "location": "/sols/chapter9/exercise6/#a",
            "text": "# Generate dataset\nX, y = make_classification(n_samples=500, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=0)  # Plot\nplt.scatter(X[:,0], X[:,1], s=25, c=y, cmap=plt.cm.Paired)\nplt.xlabel('X1')\nplt.ylabel('X2')  <matplotlib.text.Text at 0x12ea2cf8>",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter9/exercise6/#b",
            "text": "# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)  # Cost range\ncost_range = [0.01, .1, 1, 10]  # Cross-validation error rates\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni = ', i)\n    print('%.3f' % (np.average(1-cross_val_score(svc, X, y, cv=5, scoring='accuracy'))))  i =  0.01\n0.140\n\ni =  0.1\n0.122\n\ni =  1\n0.126\n\ni =  10\n0.124  # Confusion matrix for a range of cost values\n# This is a necessary step to get the number of misclassifications.\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni =', i)\n    print('Confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))  i = 0.01\nConfusion matrix:  [[153  23]\n [ 27 147]]\n\ni = 0.1\nConfusion matrix:  [[167   9]\n [ 32 142]]\n\ni = 1\nConfusion matrix:  [[163  13]\n [ 31 143]]\n\ni = 10\nConfusion matrix:  [[162  14]\n [ 31 143]]  # Misclassifications\nmisclass_1 = 23+27\nmisclass_2 = 9+32\nmisclass_3 = 13+31\nmisclass_4 = 14+31\n\nmisclass = [misclass_1, misclass_2, misclass_3, misclass_4]\n\nfor i in range(0,4):\n    print('Misclassifications, i = %.3f, %i' % (cost_range[i], misclass[i]))  Misclassifications, i = 0.010, 50\nMisclassifications, i = 0.100, 41\nMisclassifications, i = 1.000, 44\nMisclassifications, i = 10.000, 45  The cost value with less cross-validation error is C=0.1, which is also the cost value with less misclassifications. This is what we wanted to see because it shows that the training error (evaluated by the number of misclassifications) is in accordance with test error (here evaluated by the cross-validation error).",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter9/exercise6/#c",
            "text": "# Generate dataset\nX, y = make_classification(n_samples=500, random_state=1)  # Plot\nplt.scatter(X[:,0], X[:,1], s=25, c=y, cmap=plt.cm.Paired)\nplt.xlabel('X1')\nplt.ylabel('X2')  <matplotlib.text.Text at 0x12f2c3c8>   # Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)  # Test errors\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni: ', i)\n    print('Test error: ', 1-accuracy_score(y_test, svc.predict(X_test)))  i:  0.01\nTest error:  0.0533333333333\n\ni:  0.1\nTest error:  0.06\n\ni:  1\nTest error:  0.08\n\ni:  10\nTest error:  0.0866666666667  # Training errors\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni: ', i)\n    print('Training error: ', 1-accuracy_score(y_train, svc.predict(X_train)))  i:  0.01\nTraining error:  0.0485714285714\n\ni:  0.1\nTraining error:  0.0571428571429\n\ni:  1\nTraining error:  0.0428571428571\n\ni:  10\nTraining error:  0.0371428571429  # Cross-validation errors\nfor i in cost_range:\n    svc = SVC(kernel='linear', C=i, random_state=1)\n    svc.fit(X_train, y_train)\n    print('\\ni: ', i)\n    print('Cross-validation error: ', np.average(1-cross_val_score(svc, X, y, scoring='accuracy', cv=5)))  i:  0.01\nCross-validation error:  0.0540218021802\n\ni:  0.1\nCross-validation error:  0.05600160016\n\ni:  1\nCross-validation error:  0.0580218021802\n\ni:  10\nCross-validation error:  0.0660420042004  The fewest training error is reached when the cost value is 10, while the fewest cross-validation errors occurs when the cost value is 0.01. The fewest cross-validation error is also when the cost value is 0.01.   These results show that high cost values tend to overfit the data. They can give the best results regarding the training error, but when we try to generalize the results they don't work so well. This happens because an high cost value will reduce the number of support vector violating the margin. Thus, the approach gets too sticked to the observations of the training set and will not be able to generalize well, missing the noisy points.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter9/exercise7/",
            "text": "Exercise 9.7",
            "title": "9.7"
        },
        {
            "location": "/sols/chapter9/exercise7/#exercise-97",
            "text": "",
            "title": "Exercise 9.7"
        },
        {
            "location": "/sols/chapter9/exercise8/",
            "text": "Exercise 9.8\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline\n\n\n\n\ndf = pd.read_csv('../data/OJ.csv', index_col=0)\n\n\n\n\n# Data overview\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPurchase\n\n      \nWeekofPurchase\n\n      \nStoreID\n\n      \nPriceCH\n\n      \nPriceMM\n\n      \nDiscCH\n\n      \nDiscMM\n\n      \nSpecialCH\n\n      \nSpecialMM\n\n      \nLoyalCH\n\n      \nSalePriceMM\n\n      \nSalePriceCH\n\n      \nPriceDiff\n\n      \nStore7\n\n      \nPctDiscMM\n\n      \nPctDiscCH\n\n      \nListPriceDiff\n\n      \nSTORE\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \nCH\n\n      \n237\n\n      \n1\n\n      \n1.75\n\n      \n1.99\n\n      \n0.00\n\n      \n0.0\n\n      \n0\n\n      \n0\n\n      \n0.500000\n\n      \n1.99\n\n      \n1.75\n\n      \n0.24\n\n      \nNo\n\n      \n0.000000\n\n      \n0.000000\n\n      \n0.24\n\n      \n1\n\n    \n\n    \n\n      \n2\n\n      \nCH\n\n      \n239\n\n      \n1\n\n      \n1.75\n\n      \n1.99\n\n      \n0.00\n\n      \n0.3\n\n      \n0\n\n      \n1\n\n      \n0.600000\n\n      \n1.69\n\n      \n1.75\n\n      \n-0.06\n\n      \nNo\n\n      \n0.150754\n\n      \n0.000000\n\n      \n0.24\n\n      \n1\n\n    \n\n    \n\n      \n3\n\n      \nCH\n\n      \n245\n\n      \n1\n\n      \n1.86\n\n      \n2.09\n\n      \n0.17\n\n      \n0.0\n\n      \n0\n\n      \n0\n\n      \n0.680000\n\n      \n2.09\n\n      \n1.69\n\n      \n0.40\n\n      \nNo\n\n      \n0.000000\n\n      \n0.091398\n\n      \n0.23\n\n      \n1\n\n    \n\n    \n\n      \n4\n\n      \nMM\n\n      \n227\n\n      \n1\n\n      \n1.69\n\n      \n1.69\n\n      \n0.00\n\n      \n0.0\n\n      \n0\n\n      \n0\n\n      \n0.400000\n\n      \n1.69\n\n      \n1.69\n\n      \n0.00\n\n      \nNo\n\n      \n0.000000\n\n      \n0.000000\n\n      \n0.00\n\n      \n1\n\n    \n\n    \n\n      \n5\n\n      \nCH\n\n      \n228\n\n      \n7\n\n      \n1.69\n\n      \n1.69\n\n      \n0.00\n\n      \n0.0\n\n      \n0\n\n      \n0\n\n      \n0.956535\n\n      \n1.69\n\n      \n1.69\n\n      \n0.00\n\n      \nYes\n\n      \n0.000000\n\n      \n0.000000\n\n      \n0.00\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\n# Define predictors and response \nX = df.drop(axis=1, labels=['Purchase'])\ny = df['Purchase']\n\n\n\n\n# Dummy variables to transform qualitative into quantitative variables\nX = pd.get_dummies(X)\n\n\n\n\n(a)\n\n\n# Split data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=800, random_state=1)\n\n\n\n\n(b)\n\n\n# Fit SVC to data\nsvc = SVC(C=0.01, kernel='linear', random_state=1)\nsvc.fit(X_train, y_train)\n\n\n\n\nSVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\n# Number of support vectors for each class\nsvc.n_support_\n\n\n\n\narray([307, 304])\n\n\n\nIn our dataset, we have 800 observations, 2 classes, and a total of 611 support vectors. From those support vectors, 307 belong to class CH and 304 to class MM.\n\n\n(c)\n\n\nError rate (ERR) is calculated as the number of all incorrect predictions divided by the total number of the dataset. The best error rate is 0.0, whereas the worst is 1.0. The error rate is derived from the confusion matrix.\n\n\nSource: https://classeval.wordpress.com/introduction/basic-evaluation-measures/\n\n\n# Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, svc.predict(X_test)))\n\n\n\n\nTrain confusion matrix:  [[489   7]\n [241  63]]\nTest confusion matrix:  [[150   7]\n [ 90  23]]\n\n\n\nThe count of true negatives is \nC_{0,0}\nC_{0,0}\n, false negatives is \nC_{1,0}\nC_{1,0}\n, true positives is \nC_{1,1}\nC_{1,1}\n and false positives is \nC_{0,1}\nC_{0,1}\n.\n\n\n# Error rate\ntrain_err = (7+241)/(489+7+241+63)\ntest_err = (7+90)/(150+7+90+23)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)\n\n\n\n\nTrain error rate:  0.31\nTest error rate:  0.3592592592592593\n\n\n\n(d)\n\n\nSince the selection of an optimal cost is a hypertuning parameter operation, we will use the GridSearchCV.\n\n\n# Hypertune cost using GridSearchCV\nsvc = SVC(kernel='linear', random_state=1)\n\nparameters = {'C':np.arange(0.01, 10, 2)}\n\nclf = GridSearchCV(svc, parameters)\nclf.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=None, error_score='raise',\n       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False),\n       fit_params={}, iid=True, n_jobs=1,\n       param_grid={'C': array([ 0.01,  2.01,  4.01,  6.01,  8.01])},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n       scoring=None, verbose=0)\n\n\n\n# Best value for cost\nclf.best_params_\n\n\n\n\n{'C': 2.0099999999999998}\n\n\n\n(e)\n\n\n# Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, clf.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, clf.predict(X_test)))\n\n\n\n\nTrain confusion matrix:  [[436  60]\n [ 72 232]]\nTest confusion matrix:  [[143  14]\n [ 30  83]]\n\n\n\n# Error rate\ntrain_err = (59+75)/(437+59+75+229)\ntest_err = (13+35)/(144+13+35+78)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)\n\n\n\n\nTrain error rate:  0.1675\nTest error rate:  0.17777777777777778\n\n\n\n(f)\n\n\n# Fit SVC to data\nsvc = SVC(C=0.01, kernel='rbf', random_state=1)\nsvc.fit(X_train, y_train)\n\n\n\n\nSVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\n# Number of support vectors for each class\nsvc.n_support_\n\n\n\n\narray([321, 304])\n\n\n\n# Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, svc.predict(X_test)))\n\n\n\n\nTrain confusion matrix:  [[496   0]\n [304   0]]\nTest confusion matrix:  [[157   0]\n [113   0]]\n\n\n\n# Error rate\ntrain_err = (0+304)/(496+0+304+0)\ntest_err = (0+113)/(157+0+113+0)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)\n\n\n\n\nTrain error rate:  0.38\nTest error rate:  0.4185185185185185\n\n\n\n# Hypertune cost using GridSearchCV\nsvc = SVC(kernel='rbf', random_state=1)\n\nparameters = {'C':np.arange(0.01, 10, 2)}\n\nclf = GridSearchCV(svc, parameters)\nclf.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=None, error_score='raise',\n       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False),\n       fit_params={}, iid=True, n_jobs=1,\n       param_grid={'C': array([ 0.01,  2.01,  4.01,  6.01,  8.01])},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n       scoring=None, verbose=0)\n\n\n\n# Best value for cost\nclf.best_params_\n\n\n\n\n{'C': 4.0099999999999998}\n\n\n\n# Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, clf.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, clf.predict(X_test)))\n\n\n\n\nTrain confusion matrix:  [[450  46]\n [ 94 210]]\nTest confusion matrix:  [[145  12]\n [ 39  74]]\n\n\n\n# Error rate\ntrain_err = (40+78)/(456+40+78+226)\ntest_err = (11+36)/(146+11+36+77)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)\n\n\n\n\nTrain error rate:  0.1475\nTest error rate:  0.17407407407407408\n\n\n\n(g)\n\n\n# Fit SVC to data\nsvc = SVC(C=0.01, kernel='poly', degree=2, random_state=1)\nsvc.fit(X_train, y_train)\n\n\n\n\nSVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=2, gamma='auto', kernel='poly',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\n# Number of support vectors for each class\nsvc.n_support_\n\n\n\n\narray([164, 164])\n\n\n\n# Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, svc.predict(X_test)))\n\n\n\n\nTrain confusion matrix:  [[435  61]\n [ 70 234]]\nTest confusion matrix:  [[140  17]\n [ 30  83]]\n\n\n\n# Error rate\ntrain_err = (61+70)/(435+61+70+234)\ntest_err = (17+30)/(140+17+30+83)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)\n\n\n\n\nTrain error rate:  0.16375\nTest error rate:  0.17407407407407408\n\n\n\n# Hypertune cost using GridSearchCV\nsvc = SVC(kernel='poly', degree=2, random_state=1)\n\nparameters = {'C':np.arange(0.01, 10, 2)}\n\nclf = GridSearchCV(svc, parameters)\nclf.fit(X_train, y_train)\n\n\n\n\n# Best value for cost\nclf.best_params_\n\n\n\n\n# Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, clf.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, clf.predict(X_test)))\n\n\n\n\n# Error rate\ntrain_err = (61+70)/(456+40+78+226)\ntest_err = (11+36)/(146+11+36+77)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)\n\n\n\n\n(h)\n\n\nOverall, the approach that seems to give the best results on this data is \nxxx\n.",
            "title": "9.8"
        },
        {
            "location": "/sols/chapter9/exercise8/#exercise-98",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline  df = pd.read_csv('../data/OJ.csv', index_col=0)  # Data overview\ndf.head()   \n   \n     \n       \n       Purchase \n       WeekofPurchase \n       StoreID \n       PriceCH \n       PriceMM \n       DiscCH \n       DiscMM \n       SpecialCH \n       SpecialMM \n       LoyalCH \n       SalePriceMM \n       SalePriceCH \n       PriceDiff \n       Store7 \n       PctDiscMM \n       PctDiscCH \n       ListPriceDiff \n       STORE \n     \n   \n   \n     \n       1 \n       CH \n       237 \n       1 \n       1.75 \n       1.99 \n       0.00 \n       0.0 \n       0 \n       0 \n       0.500000 \n       1.99 \n       1.75 \n       0.24 \n       No \n       0.000000 \n       0.000000 \n       0.24 \n       1 \n     \n     \n       2 \n       CH \n       239 \n       1 \n       1.75 \n       1.99 \n       0.00 \n       0.3 \n       0 \n       1 \n       0.600000 \n       1.69 \n       1.75 \n       -0.06 \n       No \n       0.150754 \n       0.000000 \n       0.24 \n       1 \n     \n     \n       3 \n       CH \n       245 \n       1 \n       1.86 \n       2.09 \n       0.17 \n       0.0 \n       0 \n       0 \n       0.680000 \n       2.09 \n       1.69 \n       0.40 \n       No \n       0.000000 \n       0.091398 \n       0.23 \n       1 \n     \n     \n       4 \n       MM \n       227 \n       1 \n       1.69 \n       1.69 \n       0.00 \n       0.0 \n       0 \n       0 \n       0.400000 \n       1.69 \n       1.69 \n       0.00 \n       No \n       0.000000 \n       0.000000 \n       0.00 \n       1 \n     \n     \n       5 \n       CH \n       228 \n       7 \n       1.69 \n       1.69 \n       0.00 \n       0.0 \n       0 \n       0 \n       0.956535 \n       1.69 \n       1.69 \n       0.00 \n       Yes \n       0.000000 \n       0.000000 \n       0.00 \n       0 \n     \n      # Define predictors and response \nX = df.drop(axis=1, labels=['Purchase'])\ny = df['Purchase']  # Dummy variables to transform qualitative into quantitative variables\nX = pd.get_dummies(X)",
            "title": "Exercise 9.8"
        },
        {
            "location": "/sols/chapter9/exercise8/#a",
            "text": "# Split data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=800, random_state=1)",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter9/exercise8/#b",
            "text": "# Fit SVC to data\nsvc = SVC(C=0.01, kernel='linear', random_state=1)\nsvc.fit(X_train, y_train)  SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)  # Number of support vectors for each class\nsvc.n_support_  array([307, 304])  In our dataset, we have 800 observations, 2 classes, and a total of 611 support vectors. From those support vectors, 307 belong to class CH and 304 to class MM.",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter9/exercise8/#c",
            "text": "Error rate (ERR) is calculated as the number of all incorrect predictions divided by the total number of the dataset. The best error rate is 0.0, whereas the worst is 1.0. The error rate is derived from the confusion matrix.  Source: https://classeval.wordpress.com/introduction/basic-evaluation-measures/  # Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, svc.predict(X_test)))  Train confusion matrix:  [[489   7]\n [241  63]]\nTest confusion matrix:  [[150   7]\n [ 90  23]]  The count of true negatives is  C_{0,0} C_{0,0} , false negatives is  C_{1,0} C_{1,0} , true positives is  C_{1,1} C_{1,1}  and false positives is  C_{0,1} C_{0,1} .  # Error rate\ntrain_err = (7+241)/(489+7+241+63)\ntest_err = (7+90)/(150+7+90+23)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)  Train error rate:  0.31\nTest error rate:  0.3592592592592593",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter9/exercise8/#d",
            "text": "Since the selection of an optimal cost is a hypertuning parameter operation, we will use the GridSearchCV.  # Hypertune cost using GridSearchCV\nsvc = SVC(kernel='linear', random_state=1)\n\nparameters = {'C':np.arange(0.01, 10, 2)}\n\nclf = GridSearchCV(svc, parameters)\nclf.fit(X_train, y_train)  GridSearchCV(cv=None, error_score='raise',\n       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False),\n       fit_params={}, iid=True, n_jobs=1,\n       param_grid={'C': array([ 0.01,  2.01,  4.01,  6.01,  8.01])},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n       scoring=None, verbose=0)  # Best value for cost\nclf.best_params_  {'C': 2.0099999999999998}",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter9/exercise8/#e",
            "text": "# Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, clf.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, clf.predict(X_test)))  Train confusion matrix:  [[436  60]\n [ 72 232]]\nTest confusion matrix:  [[143  14]\n [ 30  83]]  # Error rate\ntrain_err = (59+75)/(437+59+75+229)\ntest_err = (13+35)/(144+13+35+78)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)  Train error rate:  0.1675\nTest error rate:  0.17777777777777778",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter9/exercise8/#f",
            "text": "# Fit SVC to data\nsvc = SVC(C=0.01, kernel='rbf', random_state=1)\nsvc.fit(X_train, y_train)  SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)  # Number of support vectors for each class\nsvc.n_support_  array([321, 304])  # Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, svc.predict(X_test)))  Train confusion matrix:  [[496   0]\n [304   0]]\nTest confusion matrix:  [[157   0]\n [113   0]]  # Error rate\ntrain_err = (0+304)/(496+0+304+0)\ntest_err = (0+113)/(157+0+113+0)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)  Train error rate:  0.38\nTest error rate:  0.4185185185185185  # Hypertune cost using GridSearchCV\nsvc = SVC(kernel='rbf', random_state=1)\n\nparameters = {'C':np.arange(0.01, 10, 2)}\n\nclf = GridSearchCV(svc, parameters)\nclf.fit(X_train, y_train)  GridSearchCV(cv=None, error_score='raise',\n       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False),\n       fit_params={}, iid=True, n_jobs=1,\n       param_grid={'C': array([ 0.01,  2.01,  4.01,  6.01,  8.01])},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n       scoring=None, verbose=0)  # Best value for cost\nclf.best_params_  {'C': 4.0099999999999998}  # Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, clf.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, clf.predict(X_test)))  Train confusion matrix:  [[450  46]\n [ 94 210]]\nTest confusion matrix:  [[145  12]\n [ 39  74]]  # Error rate\ntrain_err = (40+78)/(456+40+78+226)\ntest_err = (11+36)/(146+11+36+77)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)  Train error rate:  0.1475\nTest error rate:  0.17407407407407408",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter9/exercise8/#g",
            "text": "# Fit SVC to data\nsvc = SVC(C=0.01, kernel='poly', degree=2, random_state=1)\nsvc.fit(X_train, y_train)  SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=2, gamma='auto', kernel='poly',\n  max_iter=-1, probability=False, random_state=1, shrinking=True,\n  tol=0.001, verbose=False)  # Number of support vectors for each class\nsvc.n_support_  array([164, 164])  # Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, svc.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, svc.predict(X_test)))  Train confusion matrix:  [[435  61]\n [ 70 234]]\nTest confusion matrix:  [[140  17]\n [ 30  83]]  # Error rate\ntrain_err = (61+70)/(435+61+70+234)\ntest_err = (17+30)/(140+17+30+83)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)  Train error rate:  0.16375\nTest error rate:  0.17407407407407408  # Hypertune cost using GridSearchCV\nsvc = SVC(kernel='poly', degree=2, random_state=1)\n\nparameters = {'C':np.arange(0.01, 10, 2)}\n\nclf = GridSearchCV(svc, parameters)\nclf.fit(X_train, y_train)  # Best value for cost\nclf.best_params_  # Confusion matrix\nprint('Train confusion matrix: ', confusion_matrix(y_train, clf.predict(X_train)))\nprint('Test confusion matrix: ', confusion_matrix(y_test, clf.predict(X_test)))  # Error rate\ntrain_err = (61+70)/(456+40+78+226)\ntest_err = (11+36)/(146+11+36+77)\n\nprint('Train error rate: ', train_err)\nprint('Test error rate: ', test_err)",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter9/exercise8/#h",
            "text": "Overall, the approach that seems to give the best results on this data is  xxx .",
            "title": "(h)"
        },
        {
            "location": "/sols/chapter10/exercise1/",
            "text": "Exercise 10.1",
            "title": "10.1"
        },
        {
            "location": "/sols/chapter10/exercise1/#exercise-101",
            "text": "",
            "title": "Exercise 10.1"
        },
        {
            "location": "/sols/chapter10/exercise2/",
            "text": "Exercise 10.2\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\n# Dissimilarity matrix\n'''\nThe Dissimilarity matrix is a matrix that express the similarity pair to pair between to sets. It's square, and symmetric. The diagonal\nmembers are defined as zero, meaning that zero is the measure of dissimilarity between an element and itself. Thus, the information the \nmatrix holds can be seen as a triangular matrix.\n\nSource: https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Dissimilarity_Matrix_Calculation\n'''\ndisMat = np.array([[0, 0.3, 0.4, 0.7],\n                   [0.3, 0, 0.5, 0.8],\n                   [0.4, 0.5, 0, 0.45],\n                   [0.7, 0.8, 0.45, 0]])\n\n\n\n\n# Linkage matrix\n'''\nTo solve this problem, we can use SciPy, in particular the function 'scipy.cluster.hierarchy.linkage'. However, in contrast to R's 'hclust',\n'linkage' is not able to deal directly with dissimilarity matrices. It's inputs must be a 1d compressed distance matrix or a 2d array of\nobservation vectors.\nSince we don't have a 2d array of observations (samples x features), we must use the 1d compressed distance matrix. The 1d compressed matrix\nhas the distances between all the possible combinations of observations ((0,1), (0,2), (0,3), (1,2), (1,3), and (2,3)). Distances can be \nseen as a dissimilarity measure, as they express how different the observations are. Accordingly, to solve the exercise we just need to \ntransform the dissimilarity matrix into a 1d compressed distance matrix and use it in the 'linkage' function. This can be done using the\n'triu_indices' function, which returns the indices for the upper-triangle of an (n, m) array (remember that the dissimilarity matrix is a \ntriangluar matrix).\n'''\nh, w = disMat.shape\nz = linkage(disMat[np.triu_indices(h,1)], method='complete')\n\n\n\n\n# Plot dendrogram\nplt.figure(figsize=(10,5))\nplt.title('Hierarchical clustering dendrogram')\nplt.xlabel('Observations')\nplt.ylabel('Distance')\n\ndendrogram(z, leaf_rotation=90, leaf_font_size=8)\nplt.show()\n\n\n\n\n\n\n(b)\n\n\n# Linkage matrix\nz = linkage(disMat[np.triu_indices(h,1)], method='single')\n\n\n\n\n# Plot dendrogram\nplt.figure(figsize=(10,5))\nplt.title('Hierarchical clustering dendrogram')\nplt.xlabel('Observations')\nplt.ylabel('Distance')\n\ndendrogram(z, leaf_rotation=90, leaf_font_size=8)\nplt.show()\n\n\n\n\n\n\n(c)\n\n\nIn \nCluster A\n we would have observations \n0\n and \n1\n, while in \nCluster B\n we would have observations \n2\n and \n3\n.\n\n\n(d)\n\n\nIn \nCluster A\n we would have observation \n3\n, while in \nCluster B\n we would have observations \n2\n, \n0\n and \n1\n.\n\n\n(e)\n\n\nThis question is just about graphical manipulation. Although in R it is easy to do this manipulation, we didn't find an equivalent way to do it in Python. Since the answer to this question doesn't add significant value in terms of clusters knowledge, we didn't decided not to solve it. \n\n\nReferences\n\n\n\n\nhttps://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ (nice tutorial about using SciPy for hierarchical clustering and dendograms)\n\n\nhttps://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Dissimilarity_Matrix_Calculation (Dissimilarity matrix definition)\n\n\nhttp://stackoverflow.com/questions/13079563/how-does-condensed-distance-matrix-work-pdist (condensed distance matrix explanation)",
            "title": "10.2"
        },
        {
            "location": "/sols/chapter10/exercise2/#exercise-102",
            "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n%matplotlib inline",
            "title": "Exercise 10.2"
        },
        {
            "location": "/sols/chapter10/exercise2/#a",
            "text": "# Dissimilarity matrix\n'''\nThe Dissimilarity matrix is a matrix that express the similarity pair to pair between to sets. It's square, and symmetric. The diagonal\nmembers are defined as zero, meaning that zero is the measure of dissimilarity between an element and itself. Thus, the information the \nmatrix holds can be seen as a triangular matrix.\n\nSource: https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Dissimilarity_Matrix_Calculation\n'''\ndisMat = np.array([[0, 0.3, 0.4, 0.7],\n                   [0.3, 0, 0.5, 0.8],\n                   [0.4, 0.5, 0, 0.45],\n                   [0.7, 0.8, 0.45, 0]])  # Linkage matrix\n'''\nTo solve this problem, we can use SciPy, in particular the function 'scipy.cluster.hierarchy.linkage'. However, in contrast to R's 'hclust',\n'linkage' is not able to deal directly with dissimilarity matrices. It's inputs must be a 1d compressed distance matrix or a 2d array of\nobservation vectors.\nSince we don't have a 2d array of observations (samples x features), we must use the 1d compressed distance matrix. The 1d compressed matrix\nhas the distances between all the possible combinations of observations ((0,1), (0,2), (0,3), (1,2), (1,3), and (2,3)). Distances can be \nseen as a dissimilarity measure, as they express how different the observations are. Accordingly, to solve the exercise we just need to \ntransform the dissimilarity matrix into a 1d compressed distance matrix and use it in the 'linkage' function. This can be done using the\n'triu_indices' function, which returns the indices for the upper-triangle of an (n, m) array (remember that the dissimilarity matrix is a \ntriangluar matrix).\n'''\nh, w = disMat.shape\nz = linkage(disMat[np.triu_indices(h,1)], method='complete')  # Plot dendrogram\nplt.figure(figsize=(10,5))\nplt.title('Hierarchical clustering dendrogram')\nplt.xlabel('Observations')\nplt.ylabel('Distance')\n\ndendrogram(z, leaf_rotation=90, leaf_font_size=8)\nplt.show()",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter10/exercise2/#b",
            "text": "# Linkage matrix\nz = linkage(disMat[np.triu_indices(h,1)], method='single')  # Plot dendrogram\nplt.figure(figsize=(10,5))\nplt.title('Hierarchical clustering dendrogram')\nplt.xlabel('Observations')\nplt.ylabel('Distance')\n\ndendrogram(z, leaf_rotation=90, leaf_font_size=8)\nplt.show()",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter10/exercise2/#c",
            "text": "In  Cluster A  we would have observations  0  and  1 , while in  Cluster B  we would have observations  2  and  3 .",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter10/exercise2/#d",
            "text": "In  Cluster A  we would have observation  3 , while in  Cluster B  we would have observations  2 ,  0  and  1 .",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter10/exercise2/#e",
            "text": "This question is just about graphical manipulation. Although in R it is easy to do this manipulation, we didn't find an equivalent way to do it in Python. Since the answer to this question doesn't add significant value in terms of clusters knowledge, we didn't decided not to solve it.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter10/exercise2/#references",
            "text": "https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ (nice tutorial about using SciPy for hierarchical clustering and dendograms)  https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Dissimilarity_Matrix_Calculation (Dissimilarity matrix definition)  http://stackoverflow.com/questions/13079563/how-does-condensed-distance-matrix-work-pdist (condensed distance matrix explanation)",
            "title": "References"
        },
        {
            "location": "/sols/chapter10/exercise3/",
            "text": "Exercise 10.3",
            "title": "10.3"
        },
        {
            "location": "/sols/chapter10/exercise3/#exercise-103",
            "text": "",
            "title": "Exercise 10.3"
        },
        {
            "location": "/sols/chapter10/exercise4/",
            "text": "Exercise 10.4\n\n\nTo solve this exercise, there are some definitions that we should take into account:\n\n\n\n\nLinkage\n - measures the dissimilarity between two groups of observations (cluster).\n\n\nSingle linkage\n - linkage is given by the smallest pairwise distance between the observations in cluster A and the observations in cluster B.  \n\n\nComplete linkage\n - linkage is given by the largest pairwise distance between the observations in cluster A and the observations in cluster B.\n\n\n\n\n(a)\n\n\nWe don't have enough information to tell\n. If all observations in cluster {1,2,3} and cluster {4,5} have the same pairwise distance, the fusion between these clusters occur at the same height on the tree. An example would be \nd(1,4)=d(1,5)=d(2,4)=d(2,5)=d(3,4)=d(2,5)=1\n, where \nd(x,y)\n denotes the distance between observation \nx\n and observation \ny\n.\n\n\nIn contrast, if observations between clusters have different distances, the fusion will occur higher on the complete linkage. For example, if \nd(1,4)=1\n, \nd(1,5)=2\n, \nd(2,4)=3\n, \nd(2,5)=4\n, \nd(3,4)=5\n, and \nd(3,5)=6\n, single linkage would fuse at height 1 and complete linkage at height 6.\n\n\n(b)\n\n\nThey fuse at the same height\n. The distance between two observations is unique. Thus, the smallest and largest pairwise distance are the same and the will fuse at the same height.",
            "title": "10.4"
        },
        {
            "location": "/sols/chapter10/exercise4/#exercise-104",
            "text": "To solve this exercise, there are some definitions that we should take into account:   Linkage  - measures the dissimilarity between two groups of observations (cluster).  Single linkage  - linkage is given by the smallest pairwise distance between the observations in cluster A and the observations in cluster B.    Complete linkage  - linkage is given by the largest pairwise distance between the observations in cluster A and the observations in cluster B.",
            "title": "Exercise 10.4"
        },
        {
            "location": "/sols/chapter10/exercise4/#a",
            "text": "We don't have enough information to tell . If all observations in cluster {1,2,3} and cluster {4,5} have the same pairwise distance, the fusion between these clusters occur at the same height on the tree. An example would be  d(1,4)=d(1,5)=d(2,4)=d(2,5)=d(3,4)=d(2,5)=1 , where  d(x,y)  denotes the distance between observation  x  and observation  y .  In contrast, if observations between clusters have different distances, the fusion will occur higher on the complete linkage. For example, if  d(1,4)=1 ,  d(1,5)=2 ,  d(2,4)=3 ,  d(2,5)=4 ,  d(3,4)=5 , and  d(3,5)=6 , single linkage would fuse at height 1 and complete linkage at height 6.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter10/exercise4/#b",
            "text": "They fuse at the same height . The distance between two observations is unique. Thus, the smallest and largest pairwise distance are the same and the will fuse at the same height.",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter10/exercise5/",
            "text": "Exercise 10.5",
            "title": "10.5"
        },
        {
            "location": "/sols/chapter10/exercise5/#exercise-105",
            "text": "",
            "title": "Exercise 10.5"
        },
        {
            "location": "/sols/chapter10/exercise6/",
            "text": "Exercise 10.6\n\n\n(a)\n\n\nWhen we say that a variable 'explains 10% of the variation', we are talking about the variable's capacity to distinguish different observations.\n\n\nImagine that you want to move out of your parents house and you're trying to buy a new house. You go to the real estate agency and they show you a set of houses that you could like. Since we live in a capitalist world, the first thing that you'll try to do is to determine the price of each house. Imagine that each house is described by it's \nlocation\n, \ncondition\n and \narea\n. For example, house H1 is in location A, its condition is 'Good', and it has 50 m2 (H1=(A,'Good',50)). The remaining ones can be described like this: H2=(A,'Bad',500) and H3=(A,'Good',5000).\n\n\nIn this case, \nlocation\n would be a variable that doesn't allow us to distinguish the houses. All houses have the same exact location, so that it's not possibile to distinguish, for example, H1 from H2 just based on the location. Thus, the location can't explain why prices are different. This means that its explanatory power is reduced (it doesn't explains the variation that we have between the different observations).\n\n\nIn contrast, since all the houses have different areas, \narea\n could be a good explanatory variable. If someone asks you which house has 50 m2, you can confidently say 'H1!'. This means that area gives you enough information to distinguish the observations. Accordingly, we can say that it is able to explain the variation. \n\n\nTo conclude, just remember that the first principal component considered in this exercise explains 10% of the variation. There are still 90% of the variation to be explained or, by other words, 90% of the information is missing if we just consider the first principal component.\n\n\nReferences\n\n\n\n\nhttp://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579  (nice storytelling and figures to understand PCA)",
            "title": "10.6"
        },
        {
            "location": "/sols/chapter10/exercise6/#exercise-106",
            "text": "",
            "title": "Exercise 10.6"
        },
        {
            "location": "/sols/chapter10/exercise6/#a",
            "text": "When we say that a variable 'explains 10% of the variation', we are talking about the variable's capacity to distinguish different observations.  Imagine that you want to move out of your parents house and you're trying to buy a new house. You go to the real estate agency and they show you a set of houses that you could like. Since we live in a capitalist world, the first thing that you'll try to do is to determine the price of each house. Imagine that each house is described by it's  location ,  condition  and  area . For example, house H1 is in location A, its condition is 'Good', and it has 50 m2 (H1=(A,'Good',50)). The remaining ones can be described like this: H2=(A,'Bad',500) and H3=(A,'Good',5000).  In this case,  location  would be a variable that doesn't allow us to distinguish the houses. All houses have the same exact location, so that it's not possibile to distinguish, for example, H1 from H2 just based on the location. Thus, the location can't explain why prices are different. This means that its explanatory power is reduced (it doesn't explains the variation that we have between the different observations).  In contrast, since all the houses have different areas,  area  could be a good explanatory variable. If someone asks you which house has 50 m2, you can confidently say 'H1!'. This means that area gives you enough information to distinguish the observations. Accordingly, we can say that it is able to explain the variation.   To conclude, just remember that the first principal component considered in this exercise explains 10% of the variation. There are still 90% of the variation to be explained or, by other words, 90% of the information is missing if we just consider the first principal component.",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter10/exercise6/#references",
            "text": "http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579  (nice storytelling and figures to understand PCA)",
            "title": "References"
        },
        {
            "location": "/sols/chapter10/exercise7/",
            "text": "Exercise 10.7",
            "title": "10.7"
        },
        {
            "location": "/sols/chapter10/exercise7/#exercise-107",
            "text": "",
            "title": "Exercise 10.7"
        },
        {
            "location": "/sols/chapter10/exercise8/",
            "text": "Exercise 10.8\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline\n\n\n\n\n# Import dataset\ndf = pd.read_csv('../data/USArrests.csv', index_col=0)\n\n\n\n\n# Data overview\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nMurder\n\n      \nAssault\n\n      \nUrbanPop\n\n      \nRape\n\n    \n\n  \n\n  \n\n    \n\n      \nAlabama\n\n      \n13.2\n\n      \n236\n\n      \n58\n\n      \n21.2\n\n    \n\n    \n\n      \nAlaska\n\n      \n10.0\n\n      \n263\n\n      \n48\n\n      \n44.5\n\n    \n\n    \n\n      \nArizona\n\n      \n8.1\n\n      \n294\n\n      \n80\n\n      \n31.0\n\n    \n\n    \n\n      \nArkansas\n\n      \n8.8\n\n      \n190\n\n      \n50\n\n      \n19.5\n\n    \n\n    \n\n      \nCalifornia\n\n      \n9.0\n\n      \n276\n\n      \n91\n\n      \n40.6\n\n    \n\n  \n\n\n\n\n\n\n\n# Scale data (standardize)\nscl = StandardScaler()\ndf_scl = scl.fit_transform(df)\n\n\n\n\nNote:\n We should standardize the data because scale is an issue in this exercise. The variance of the variable \nAssault\n is significantly larger than the variance of the remaining variables. Thus, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for \nAssault\n. This would take us to a misleading solution.\n\n\n(a)\n\n\n# PCA\npca = PCA()\npca.fit(df_scl)\n\n\n\n\nPCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)\n\n\n\n# Proportion of variance explained\n# This is equivalent to do sdev output of the prcomp() function in R.\npca.explained_variance_ratio_\n\n\n\n\narray([ 0.62006039,  0.24744129,  0.0891408 ,  0.04335752])\n\n\n\n(b)\n\n\n# Loadings of the principal components\n# Rows are the loading vectors (see References)\npca.components_\n\n\n\n\narray([[ 0.53589947,  0.58318363,  0.27819087,  0.54343209],\n       [ 0.41818087,  0.1879856 , -0.87280619, -0.16731864],\n       [-0.34123273, -0.26814843, -0.37801579,  0.81777791],\n       [ 0.6492278 , -0.74340748,  0.13387773,  0.08902432]])\n\n\n\n# Centered and scaled variables\n# In (a) we used centered ans scaled variables, so we should use the same data here.\ndf_scl\n\n\n\n\narray([[ 1.25517927,  0.79078716, -0.52619514, -0.00345116],\n       [ 0.51301858,  1.11805959, -1.22406668,  2.50942392],\n       [ 0.07236067,  1.49381682,  1.00912225,  1.05346626],\n       [ 0.23470832,  0.23321191, -1.08449238, -0.18679398],\n       [ 0.28109336,  1.2756352 ,  1.77678094,  2.08881393],\n       [ 0.02597562,  0.40290872,  0.86954794,  1.88390137],\n       [-1.04088037, -0.73648418,  0.79976079, -1.09272319],\n       [-0.43787481,  0.81502956,  0.45082502, -0.58583422],\n       [ 1.76541475,  1.99078607,  1.00912225,  1.1505301 ],\n       [ 2.22926518,  0.48775713, -0.38662083,  0.49265293],\n       [-0.57702994, -1.51224105,  1.21848371, -0.11129987],\n       [-1.20322802, -0.61527217, -0.80534376, -0.75839217],\n       [ 0.60578867,  0.94836277,  1.21848371,  0.29852525],\n       [-0.13637203, -0.70012057, -0.03768506, -0.0250209 ],\n       [-1.29599811, -1.39102904, -0.5959823 , -1.07115345],\n       [-0.41468229, -0.67587817,  0.03210209, -0.34856705],\n       [ 0.44344101, -0.74860538, -0.94491807, -0.53190987],\n       [ 1.76541475,  0.94836277,  0.03210209,  0.10439756],\n       [-1.31919063, -1.06375661, -1.01470522, -1.44862395],\n       [ 0.81452136,  1.56654403,  0.10188925,  0.70835037],\n       [-0.78576263, -0.26375734,  1.35805802, -0.53190987],\n       [ 1.00006153,  1.02108998,  0.59039932,  1.49564599],\n       [-1.1800355 , -1.19708982,  0.03210209, -0.68289807],\n       [ 1.9277624 ,  1.06957478, -1.5032153 , -0.44563089],\n       [ 0.28109336,  0.0877575 ,  0.31125071,  0.75148985],\n       [-0.41468229, -0.74860538, -0.87513091, -0.521125  ],\n       [-0.80895515, -0.83345379, -0.24704653, -0.51034012],\n       [ 1.02325405,  0.98472638,  1.0789094 ,  2.671197  ],\n       [-1.31919063, -1.37890783, -0.66576945, -1.26528114],\n       [-0.08998698, -0.14254532,  1.63720664, -0.26228808],\n       [ 0.83771388,  1.38472601,  0.31125071,  1.17209984],\n       [ 0.76813632,  1.00896878,  1.42784517,  0.52500755],\n       [ 1.20879423,  2.01502847, -1.43342815, -0.55347961],\n       [-1.62069341, -1.52436225, -1.5032153 , -1.50254831],\n       [-0.11317951, -0.61527217,  0.66018648,  0.01811858],\n       [-0.27552716, -0.23951493,  0.1716764 , -0.13286962],\n       [-0.66980002, -0.14254532,  0.10188925,  0.87012344],\n       [-0.34510472, -0.78496898,  0.45082502, -0.68289807],\n       [-1.01768785,  0.03927269,  1.49763233, -1.39469959],\n       [ 1.53348953,  1.3119988 , -1.22406668,  0.13675217],\n       [-0.92491776, -1.027393  , -1.43342815, -0.90938037],\n       [ 1.25517927,  0.20896951, -0.45640799,  0.61128652],\n       [ 1.13921666,  0.36654512,  1.00912225,  0.46029832],\n       [-1.06407289, -0.61527217,  1.00912225,  0.17989166],\n       [-1.29599811, -1.48799864, -2.34066115, -1.08193832],\n       [ 0.16513075, -0.17890893, -0.17725937, -0.05737552],\n       [-0.87853272, -0.31224214,  0.52061217,  0.53579242],\n       [-0.48425985, -1.08799901, -1.85215107, -1.28685088],\n       [-1.20322802, -1.42739264,  0.03210209, -1.1250778 ],\n       [-0.22914211, -0.11830292, -0.38662083, -0.60740397]])\n\n\n\n# Application of Equation 10.8\n\nfor k in range(0,np.shape(pca.components_)[1]):\n    # Numerator\n    accum = 0\n    num = 0\n    for i in range(0, np.shape(df_scl)[0]):\n        for j in range(0, np.shape(df_scl)[1]):\n            accum += pca.components_[k][j] * df_scl[i][j]\n        num += accum**2\n        accum = 0\n\n    # Denominator\n    accum = 0\n    den = 0\n    for j in range(0, np.shape(df_scl)[1]):\n        for i in range(0, np.shape(df_scl)[0]):\n            accum += df_scl[i][j]**2\n        den += accum\n        accum = 0\n\n    # Result\n    print('principal component number:', k+1)\n    print(num/den)\n\n\n\n\nprincipal component number: 1\n0.620060394787\nprincipal component number: 2\n0.247441288135\nprincipal component number: 3\n0.0891407951452\nprincipal component number: 4\n0.0433575219325\n\n\n\nReferences\n\n\n\n\nhttp://stackoverflow.com/questions/21217710/factor-loadings-using-sklearn (loading vectors)",
            "title": "10.8"
        },
        {
            "location": "/sols/chapter10/exercise8/#exercise-108",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline  # Import dataset\ndf = pd.read_csv('../data/USArrests.csv', index_col=0)  # Data overview\ndf.head()   \n   \n     \n       \n       Murder \n       Assault \n       UrbanPop \n       Rape \n     \n   \n   \n     \n       Alabama \n       13.2 \n       236 \n       58 \n       21.2 \n     \n     \n       Alaska \n       10.0 \n       263 \n       48 \n       44.5 \n     \n     \n       Arizona \n       8.1 \n       294 \n       80 \n       31.0 \n     \n     \n       Arkansas \n       8.8 \n       190 \n       50 \n       19.5 \n     \n     \n       California \n       9.0 \n       276 \n       91 \n       40.6 \n     \n      # Scale data (standardize)\nscl = StandardScaler()\ndf_scl = scl.fit_transform(df)  Note:  We should standardize the data because scale is an issue in this exercise. The variance of the variable  Assault  is significantly larger than the variance of the remaining variables. Thus, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for  Assault . This would take us to a misleading solution.",
            "title": "Exercise 10.8"
        },
        {
            "location": "/sols/chapter10/exercise8/#a",
            "text": "# PCA\npca = PCA()\npca.fit(df_scl)  PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)  # Proportion of variance explained\n# This is equivalent to do sdev output of the prcomp() function in R.\npca.explained_variance_ratio_  array([ 0.62006039,  0.24744129,  0.0891408 ,  0.04335752])",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter10/exercise8/#b",
            "text": "# Loadings of the principal components\n# Rows are the loading vectors (see References)\npca.components_  array([[ 0.53589947,  0.58318363,  0.27819087,  0.54343209],\n       [ 0.41818087,  0.1879856 , -0.87280619, -0.16731864],\n       [-0.34123273, -0.26814843, -0.37801579,  0.81777791],\n       [ 0.6492278 , -0.74340748,  0.13387773,  0.08902432]])  # Centered and scaled variables\n# In (a) we used centered ans scaled variables, so we should use the same data here.\ndf_scl  array([[ 1.25517927,  0.79078716, -0.52619514, -0.00345116],\n       [ 0.51301858,  1.11805959, -1.22406668,  2.50942392],\n       [ 0.07236067,  1.49381682,  1.00912225,  1.05346626],\n       [ 0.23470832,  0.23321191, -1.08449238, -0.18679398],\n       [ 0.28109336,  1.2756352 ,  1.77678094,  2.08881393],\n       [ 0.02597562,  0.40290872,  0.86954794,  1.88390137],\n       [-1.04088037, -0.73648418,  0.79976079, -1.09272319],\n       [-0.43787481,  0.81502956,  0.45082502, -0.58583422],\n       [ 1.76541475,  1.99078607,  1.00912225,  1.1505301 ],\n       [ 2.22926518,  0.48775713, -0.38662083,  0.49265293],\n       [-0.57702994, -1.51224105,  1.21848371, -0.11129987],\n       [-1.20322802, -0.61527217, -0.80534376, -0.75839217],\n       [ 0.60578867,  0.94836277,  1.21848371,  0.29852525],\n       [-0.13637203, -0.70012057, -0.03768506, -0.0250209 ],\n       [-1.29599811, -1.39102904, -0.5959823 , -1.07115345],\n       [-0.41468229, -0.67587817,  0.03210209, -0.34856705],\n       [ 0.44344101, -0.74860538, -0.94491807, -0.53190987],\n       [ 1.76541475,  0.94836277,  0.03210209,  0.10439756],\n       [-1.31919063, -1.06375661, -1.01470522, -1.44862395],\n       [ 0.81452136,  1.56654403,  0.10188925,  0.70835037],\n       [-0.78576263, -0.26375734,  1.35805802, -0.53190987],\n       [ 1.00006153,  1.02108998,  0.59039932,  1.49564599],\n       [-1.1800355 , -1.19708982,  0.03210209, -0.68289807],\n       [ 1.9277624 ,  1.06957478, -1.5032153 , -0.44563089],\n       [ 0.28109336,  0.0877575 ,  0.31125071,  0.75148985],\n       [-0.41468229, -0.74860538, -0.87513091, -0.521125  ],\n       [-0.80895515, -0.83345379, -0.24704653, -0.51034012],\n       [ 1.02325405,  0.98472638,  1.0789094 ,  2.671197  ],\n       [-1.31919063, -1.37890783, -0.66576945, -1.26528114],\n       [-0.08998698, -0.14254532,  1.63720664, -0.26228808],\n       [ 0.83771388,  1.38472601,  0.31125071,  1.17209984],\n       [ 0.76813632,  1.00896878,  1.42784517,  0.52500755],\n       [ 1.20879423,  2.01502847, -1.43342815, -0.55347961],\n       [-1.62069341, -1.52436225, -1.5032153 , -1.50254831],\n       [-0.11317951, -0.61527217,  0.66018648,  0.01811858],\n       [-0.27552716, -0.23951493,  0.1716764 , -0.13286962],\n       [-0.66980002, -0.14254532,  0.10188925,  0.87012344],\n       [-0.34510472, -0.78496898,  0.45082502, -0.68289807],\n       [-1.01768785,  0.03927269,  1.49763233, -1.39469959],\n       [ 1.53348953,  1.3119988 , -1.22406668,  0.13675217],\n       [-0.92491776, -1.027393  , -1.43342815, -0.90938037],\n       [ 1.25517927,  0.20896951, -0.45640799,  0.61128652],\n       [ 1.13921666,  0.36654512,  1.00912225,  0.46029832],\n       [-1.06407289, -0.61527217,  1.00912225,  0.17989166],\n       [-1.29599811, -1.48799864, -2.34066115, -1.08193832],\n       [ 0.16513075, -0.17890893, -0.17725937, -0.05737552],\n       [-0.87853272, -0.31224214,  0.52061217,  0.53579242],\n       [-0.48425985, -1.08799901, -1.85215107, -1.28685088],\n       [-1.20322802, -1.42739264,  0.03210209, -1.1250778 ],\n       [-0.22914211, -0.11830292, -0.38662083, -0.60740397]])  # Application of Equation 10.8\n\nfor k in range(0,np.shape(pca.components_)[1]):\n    # Numerator\n    accum = 0\n    num = 0\n    for i in range(0, np.shape(df_scl)[0]):\n        for j in range(0, np.shape(df_scl)[1]):\n            accum += pca.components_[k][j] * df_scl[i][j]\n        num += accum**2\n        accum = 0\n\n    # Denominator\n    accum = 0\n    den = 0\n    for j in range(0, np.shape(df_scl)[1]):\n        for i in range(0, np.shape(df_scl)[0]):\n            accum += df_scl[i][j]**2\n        den += accum\n        accum = 0\n\n    # Result\n    print('principal component number:', k+1)\n    print(num/den)  principal component number: 1\n0.620060394787\nprincipal component number: 2\n0.247441288135\nprincipal component number: 3\n0.0891407951452\nprincipal component number: 4\n0.0433575219325",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter10/exercise8/#references",
            "text": "http://stackoverflow.com/questions/21217710/factor-loadings-using-sklearn (loading vectors)",
            "title": "References"
        },
        {
            "location": "/sols/chapter10/exercise9/",
            "text": "Exercise 10.9",
            "title": "10.9"
        },
        {
            "location": "/sols/chapter10/exercise9/#exercise-109",
            "text": "",
            "title": "Exercise 10.9"
        },
        {
            "location": "/sols/chapter10/exercise10/",
            "text": "Exercise 10.10\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n\n%matplotlib inline\n\n\n\n\n(a)\n\n\n# Generate dataset\n# scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\n# make_blobs provides greater control regarding the centers and standard deviations of clusters, and is used to demonstrate clustering.\nX, y = make_blobs(n_samples=60, n_features=50, centers=3, cluster_std=5, random_state=1)\n\n\n\n\n(b)\n\n\n# PCA \n# We are looking for the first two principal components, so n_components=2.\npca = PCA(n_components=2, random_state=1)\nX_r = pca.fit_transform(X)\n\n\n\n\n# Plot\nplt.figure()\ncolors = ['blue', 'red', 'green']\n\nfor color, i in zip(colors, [0,1,2]):\n    plt.scatter(X_r[y==i, 0], X_r[y==i, 1], color=color)\nplt.title('Principal component score vectors')\nplt.xlabel('First principal component')\nplt.ylabel('Second principal component')\n\n\n\n\n<matplotlib.text.Text at 0x10941eb8>\n\n\n\n\n\n(c)\n\n\n# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=3)\nkm3.fit(X)\nkm3.labels_\n\n\n\n\narray([2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 0, 1,\n       2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 2,\n       2, 2, 0, 2, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2])\n\n\n\n# Get true class labels\ny\n\n\n\n\narray([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])\n\n\n\nThe results show an agreement between true class labels and clustering labels. K-means clustering will arbitrarily number the clusters, so we cannot simply check whether the true class labels and clustering labels are the same. Here, the arbitrary attribution of numbers to the clusters led to the following correspondences: \n\n\n\n\nClustering label 0 --> true class label 1.\n\n\nClustering label 1 --> true class label 2. \n\n\nClustering label 2 --> true class label 0. \n\n\n\n\nConsidering these correspondences, we can see that both labels match perfectly. \n\n\n(d)\n\n\n# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=2)\nkm3.fit(X)\nkm3.labels_\n\n\n\n\narray([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n\n\n\n# Get true class labels\ny\n\n\n\n\narray([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])\n\n\n\nIn this case, we only have two clustering labels. The results above show that when:\n\n\n\n\nClustering label is 0 --> true class label can be 0 or 1.\n\n\nClustering label is 1 --> true class label is 2;\n\n\n\n\nThis means that when K-means is equal to 2, we are merging the labels 0 and 1. Thus, we can say that label 2 corresponds to a finer set of observations.\n\n\n(e)\n\n\n# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=4)\nkm3.fit(X)\nkm3.labels_\n\n\n\n\narray([2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 3, 2, 3, 3, 2, 0, 2, 0, 3,\n       2, 2, 3, 1, 1, 2, 0, 3, 0, 2, 3, 0, 1, 1, 3, 1, 0, 3, 3, 2, 0, 0, 2,\n       2, 2, 0, 2, 0, 0, 1, 3, 2, 1, 2, 0, 1, 2])\n\n\n\n# Get true class labels\ny\n\n\n\n\narray([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])\n\n\n\nThe results show the following correspondences between clustering and true labels:\n\n\n\n\nClustering label 0 --> True label 2.\n\n\nClustering label 1 --> True label 1.\n\n\nClustering label 2 --> True label 0.\n\n\nClustering label 3 --> True label 1.\n\n\n\n\nWe can conclude that the original cluster 1 (true label = 1), was split into two different clusters (clustering labels 1 and 3). The remaining clusters didn't have any significant change. This is an expected result, since the only coherent way to create a new cluster would be to split one of the original clusters, while remaining the other two unchanged.\n\n\n(f)\n\n\n# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=3)\nkm3.fit(X_r)\nkm3.labels_\n\n\n\n\narray([2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 0, 1,\n       2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 2,\n       2, 2, 0, 2, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2])\n\n\n\n# Get true class labels\ny\n\n\n\n\narray([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])\n\n\n\nThe results show that, apart from the correspondences, true and clustering labels match perfectly. This means that the two principal components were able to reduce dimensionsal space without loss of information.\n\n\n(g)\n\n\n# Scale variables\n# Data will be scaled to unit variance but it will not be centered (mean will not be removed).\nX_scaler = StandardScaler(with_mean=False)\nX_scl = X_scaler.fit_transform(X)\n\n\n\n\n# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=3)\nkm3.fit(X_scl)\nkm3.labels_\n\n\n\n\narray([0, 2, 1, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 0, 1, 1, 0, 2, 0, 2, 1,\n       0, 0, 1, 1, 1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 2, 0,\n       0, 0, 2, 0, 2, 2, 1, 1, 0, 1, 0, 2, 1, 0])\n\n\n\n# Get true class labels\ny\n\n\n\n\narray([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])\n\n\n\nAs in the previous cases, apart from the correspondence between clustering and true class, the labels match perfectly. However, it should be noted that this is not an obvious result. The k-means algorithm is sensitive to the scale of the variables. Standardizing the observations can have a strong impact on the results obtained. Accordingly, it could have happened that the clustering and the true class didn't match.\n\n\nThe decision about standardizing or not the variables depends on the data. In a real case, we should look for different choices and go for the one that give us the most interpretable solution or the most useful solution.",
            "title": "10.10"
        },
        {
            "location": "/sols/chapter10/exercise10/#exercise-1010",
            "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n\n%matplotlib inline",
            "title": "Exercise 10.10"
        },
        {
            "location": "/sols/chapter10/exercise10/#a",
            "text": "# Generate dataset\n# scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\n# make_blobs provides greater control regarding the centers and standard deviations of clusters, and is used to demonstrate clustering.\nX, y = make_blobs(n_samples=60, n_features=50, centers=3, cluster_std=5, random_state=1)",
            "title": "(a)"
        },
        {
            "location": "/sols/chapter10/exercise10/#b",
            "text": "# PCA \n# We are looking for the first two principal components, so n_components=2.\npca = PCA(n_components=2, random_state=1)\nX_r = pca.fit_transform(X)  # Plot\nplt.figure()\ncolors = ['blue', 'red', 'green']\n\nfor color, i in zip(colors, [0,1,2]):\n    plt.scatter(X_r[y==i, 0], X_r[y==i, 1], color=color)\nplt.title('Principal component score vectors')\nplt.xlabel('First principal component')\nplt.ylabel('Second principal component')  <matplotlib.text.Text at 0x10941eb8>",
            "title": "(b)"
        },
        {
            "location": "/sols/chapter10/exercise10/#c",
            "text": "# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=3)\nkm3.fit(X)\nkm3.labels_  array([2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 0, 1,\n       2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 2,\n       2, 2, 0, 2, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2])  # Get true class labels\ny  array([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])  The results show an agreement between true class labels and clustering labels. K-means clustering will arbitrarily number the clusters, so we cannot simply check whether the true class labels and clustering labels are the same. Here, the arbitrary attribution of numbers to the clusters led to the following correspondences:    Clustering label 0 --> true class label 1.  Clustering label 1 --> true class label 2.   Clustering label 2 --> true class label 0.    Considering these correspondences, we can see that both labels match perfectly.",
            "title": "(c)"
        },
        {
            "location": "/sols/chapter10/exercise10/#d",
            "text": "# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=2)\nkm3.fit(X)\nkm3.labels_  array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0])  # Get true class labels\ny  array([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])  In this case, we only have two clustering labels. The results above show that when:   Clustering label is 0 --> true class label can be 0 or 1.  Clustering label is 1 --> true class label is 2;   This means that when K-means is equal to 2, we are merging the labels 0 and 1. Thus, we can say that label 2 corresponds to a finer set of observations.",
            "title": "(d)"
        },
        {
            "location": "/sols/chapter10/exercise10/#e",
            "text": "# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=4)\nkm3.fit(X)\nkm3.labels_  array([2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 3, 2, 3, 3, 2, 0, 2, 0, 3,\n       2, 2, 3, 1, 1, 2, 0, 3, 0, 2, 3, 0, 1, 1, 3, 1, 0, 3, 3, 2, 0, 0, 2,\n       2, 2, 0, 2, 0, 0, 1, 3, 2, 1, 2, 0, 1, 2])  # Get true class labels\ny  array([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])  The results show the following correspondences between clustering and true labels:   Clustering label 0 --> True label 2.  Clustering label 1 --> True label 1.  Clustering label 2 --> True label 0.  Clustering label 3 --> True label 1.   We can conclude that the original cluster 1 (true label = 1), was split into two different clusters (clustering labels 1 and 3). The remaining clusters didn't have any significant change. This is an expected result, since the only coherent way to create a new cluster would be to split one of the original clusters, while remaining the other two unchanged.",
            "title": "(e)"
        },
        {
            "location": "/sols/chapter10/exercise10/#f",
            "text": "# Get clustering labels using K-means\nkm3 = KMeans(n_clusters=3)\nkm3.fit(X_r)\nkm3.labels_  array([2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 0, 1,\n       2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 2,\n       2, 2, 0, 2, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2])  # Get true class labels\ny  array([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])  The results show that, apart from the correspondences, true and clustering labels match perfectly. This means that the two principal components were able to reduce dimensionsal space without loss of information.",
            "title": "(f)"
        },
        {
            "location": "/sols/chapter10/exercise10/#g",
            "text": "# Scale variables\n# Data will be scaled to unit variance but it will not be centered (mean will not be removed).\nX_scaler = StandardScaler(with_mean=False)\nX_scl = X_scaler.fit_transform(X)  # Get clustering labels using K-means\nkm3 = KMeans(n_clusters=3)\nkm3.fit(X_scl)\nkm3.labels_  array([0, 2, 1, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 0, 1, 1, 0, 2, 0, 2, 1,\n       0, 0, 1, 1, 1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 2, 0,\n       0, 0, 2, 0, 2, 2, 1, 1, 0, 1, 0, 2, 1, 0])  # Get true class labels\ny  array([0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n       0, 0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0])  As in the previous cases, apart from the correspondence between clustering and true class, the labels match perfectly. However, it should be noted that this is not an obvious result. The k-means algorithm is sensitive to the scale of the variables. Standardizing the observations can have a strong impact on the results obtained. Accordingly, it could have happened that the clustering and the true class didn't match.  The decision about standardizing or not the variables depends on the data. In a real case, we should look for different choices and go for the one that give us the most interpretable solution or the most useful solution.",
            "title": "(g)"
        },
        {
            "location": "/sols/chapter10/exercise11/",
            "text": "Exercise 10.11",
            "title": "10.11"
        },
        {
            "location": "/sols/chapter10/exercise11/#exercise-1011",
            "text": "",
            "title": "Exercise 10.11"
        },
        {
            "location": "/about/",
            "text": "This is a project by \nbotlnec\n, a group of friends working in different areas of data science. \n\n\nThis project was developed by two members of the Botlnec team and is included in a set of activities that this group of friends has been developping.\n\n\nPedro Marcelino\n\n\nPedro works as a researcher at \nLNEC\n and he is also a PhD candidate at \nIST\n.\nCurrently, he is developing a new approach to the maintenance management of transport infrastructures using machine learning.\nIn his spare time, he likes to work on TreeTree2, read books and play sports. \n\n\nJo\u00e3o Rico\n\n\nJo\u00e3o is a researcher at LNEC interested in machine and human learning.",
            "title": "About"
        },
        {
            "location": "/about/#pedro-marcelino",
            "text": "Pedro works as a researcher at  LNEC  and he is also a PhD candidate at  IST .\nCurrently, he is developing a new approach to the maintenance management of transport infrastructures using machine learning.\nIn his spare time, he likes to work on TreeTree2, read books and play sports.",
            "title": "Pedro Marcelino"
        },
        {
            "location": "/about/#joao-rico",
            "text": "Jo\u00e3o is a researcher at LNEC interested in machine and human learning.",
            "title": "Jo\u00e3o Rico"
        }
    ]
}